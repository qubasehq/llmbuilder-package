{
  "model": {
    "vocab_size": 16000,
    "embedding_dim": 512,
    "num_layers": 8,
    "num_heads": 8,
    "max_seq_length": 1024,
    "dropout": 0.0
  },
  "tokenizer": {
    "vocab_size": 16000,
    "model_type": "bpe",
    "character_coverage": 0.9995
  },
  "training": {
    "batch_size": 1,
    "learning_rate": 1e-5,
    "num_epochs": 1,
    "weight_decay": 0.0,
    "gradient_clip_norm": 1.0
  },
  "data": {
    "max_length": 1024,
    "min_length": 10,
    "clean_text": true,
    "validation_split": 0.0,
    "test_split": 0.0,
    "ingestion": {
      "supported_formats": ["txt"],
      "batch_size": 1,
      "num_workers": 1,
      "output_format": "txt",
      "enable_ocr": false
    },
    "deduplication": {
      "enable_exact_deduplication": false,
      "enable_semantic_deduplication": false,
      "similarity_threshold": 0.85,
      "batch_size": 100,
      "use_gpu_for_embeddings": false
    }
  },
  "tokenizer_training": {
    "vocab_size": 16000,
    "algorithm": "bpe",
    "min_frequency": 1,
    "special_tokens": ["<pad>", "<unk>", "<s>", "</s>"],
    "character_coverage": 0.9995,
    "num_threads": 1,
    "max_training_time": 600
  },
  "gguf_conversion": {
    "quantization_level": "Q4_0",
    "validate_output": true,
    "conversion_timeout": 1800,
    "preferred_script": "auto",
    "output_naming": "quantization_suffix"
  },
  "inference": {
    "max_new_tokens": 256,
    "temperature": 0.7,
    "top_k": 40,
    "top_p": 0.9,
    "repetition_penalty": 1.1,
    "do_sample": true
  },
  "system": {
    "device": "auto",
    "mixed_precision": false,
    "compile_model": true,
    "num_workers": 1,
    "pin_memory": false
  },
  "paths": {
    "data_dir": "data",
    "model_dir": "models",
    "checkpoint_dir": "checkpoints",
    "tokenizer_dir": "tokenizers",
    "output_dir": "output"
  }
}
