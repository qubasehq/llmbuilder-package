# ü§ñ LLMBuilder

[![Documentation](https://img.shields.io/badge/docs-mkdocs-blue)](https://qubasehq.github.io/llmbuilder-package/)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A comprehensive toolkit for building, training, fine-tuning, and deploying GPT-style language models with CPU-friendly defaults.

## About LLMBuilder Framework

**LLMBuilder** is a production-ready framework for training and fine-tuning Large Language Models (LLMs) ‚Äî not a model itself. Designed for developers, researchers, and AI engineers, LLMBuilder provides a full pipeline to go from raw text data to deployable, optimized LLMs, all running locally on CPUs or GPUs.

### Complete Framework Structure

The full LLMBuilder framework includes:

```
LLMBuilder/
‚îú‚îÄ‚îÄ data/                   # Data directories
‚îÇ   ‚îú‚îÄ‚îÄ raw/               # Raw input files (.txt, .pdf, .docx)
‚îÇ   ‚îú‚îÄ‚îÄ cleaned/           # Processed text files
‚îÇ   ‚îî‚îÄ‚îÄ tokens/            # Tokenized datasets
‚îÇ   ‚îú‚îÄ‚îÄ download_data.py   # Script to download datasets
‚îÇ   ‚îî‚îÄ‚îÄ SOURCES.md         # Data sources documentation
‚îÇ
‚îú‚îÄ‚îÄ debug_scripts/         # Debugging utilities
‚îÇ   ‚îú‚îÄ‚îÄ debug_loader.py    # Data loading debugger
‚îÇ   ‚îú‚îÄ‚îÄ debug_training.py  # Training process debugger
‚îÇ   ‚îî‚îÄ‚îÄ debug_timestamps.py # Timing analysis
‚îÇ
‚îú‚îÄ‚îÄ eval/                  # Model evaluation
‚îÇ   ‚îî‚îÄ‚îÄ eval.py           # Evaluation scripts
‚îÇ
‚îú‚îÄ‚îÄ exports/               # Output directories
‚îÇ   ‚îú‚îÄ‚îÄ checkpoints/      # Training checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ gguf/             # GGUF model exports
‚îÇ   ‚îú‚îÄ‚îÄ onnx/             # ONNX model exports
‚îÇ   ‚îî‚îÄ‚îÄ tokenizer/        # Saved tokenizer files
‚îÇ
‚îú‚îÄ‚îÄ finetune/             # Fine-tuning scripts
‚îÇ   ‚îú‚îÄ‚îÄ finetune.py      # Fine-tuning implementation
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py      # Package initialization
‚îÇ
‚îú‚îÄ‚îÄ logs/                 # Training and evaluation logs
‚îÇ
‚îú‚îÄ‚îÄ model/                # Model architecture
‚îÇ   ‚îî‚îÄ‚îÄ gpt_model.py     # GPT model implementation
‚îÇ
‚îú‚îÄ‚îÄ tools/                # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ analyze_data.ps1  # PowerShell data analysis
‚îÇ   ‚îú‚îÄ‚îÄ analyze_data.sh   # Bash data analysis
‚îÇ   ‚îú‚îÄ‚îÄ download_hf_model.py # HuggingFace model downloader
‚îÇ   ‚îî‚îÄ‚îÄ export_gguf.py    # GGUF export utility
‚îÇ
‚îú‚îÄ‚îÄ training/             # Training pipeline
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py       # Dataset handling
‚îÇ   ‚îú‚îÄ‚îÄ preprocess.py    # Data preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ quantization.py  # Model quantization
‚îÇ   ‚îú‚îÄ‚îÄ train.py         # Main training script
‚îÇ   ‚îú‚îÄ‚îÄ train_tokenizer.py # Tokenizer training
‚îÇ   ‚îî‚îÄ‚îÄ utils.py         # Training utilities
‚îÇ
‚îú‚îÄ‚îÄ .gitignore           # Git ignore rules
‚îú‚îÄ‚îÄ config.json          # Main configuration
‚îú‚îÄ‚îÄ config_cpu_small.json # Small CPU config
‚îú‚îÄ‚îÄ config_gpu.json      # GPU configuration
‚îú‚îÄ‚îÄ inference.py         # Inference script
‚îú‚îÄ‚îÄ quantize_model.py    # Model quantization
‚îú‚îÄ‚îÄ README.md           # Documentation
‚îú‚îÄ‚îÄ requirements.txt    # Python dependencies
‚îú‚îÄ‚îÄ run.ps1            # PowerShell runner
‚îî‚îÄ‚îÄ run.sh             # Bash runner
```

üîó **Full Framework Repository**: [https://github.com/Qubasehq/llmbuilder](https://github.com/Qubasehq/llmbuilder)

> [!NOTE]
> **This is a separate framework** - The complete LLMBuilder framework shown above is **not related to this package**. It's a standalone, comprehensive framework available at the GitHub repository. This package (`llmbuilder_package`) provides the core modular toolkit, while the complete framework offers additional utilities, debugging tools, and production-ready scripts for comprehensive LLM development workflows.

## üöÄ Quick Start

### Installation

```bash
pip install llmbuilder
```

### 5-Minute Example

```python
import llmbuilder as lb

# Load configuration and build model
cfg = lb.load_config(preset="cpu_small")
model = lb.build_model(cfg.model)

# Train the model
from llmbuilder.data import TextDataset
dataset = TextDataset("./data/clean.txt", block_size=cfg.model.max_seq_length)
results = lb.train_model(model, dataset, cfg.training)

# Generate text
text = lb.generate_text(
    model_path="./checkpoints/model.pt",
    tokenizer_path="./tokenizers",
    prompt="The future of AI is",
    max_new_tokens=50
)
print(text)
```

## üìö Documentation

**Complete documentation is available at: [https://qubasehq.github.io/llmbuilder-package/](https://qubasehq.github.io/llmbuilder-package/)**

The documentation includes:
- üìñ **Getting Started Guide** - From installation to your first model
- üéØ **User Guides** - Comprehensive guides for all features  
- üñ•Ô∏è **CLI Reference** - Complete command-line interface documentation
- üêç **Python API** - Full API reference with examples
- üìã **Examples** - Working code examples for common tasks
- ‚ùì **FAQ** - Answers to frequently asked questions

## CLI Quickstart

- Show help:
  - `llmbuilder --help`
- Preprocess data from mixed files into a text corpus:
  - `llmbuilder data preprocess -i ./data/raw -o ./data/clean.txt`
- Train tokenizer (BPE via SentencePiece):
  - `llmbuilder data tokenizer -i ./data/clean.txt -o ./tokenizers --vocab-size 16000`
- Train model (tiny CPU-friendly settings):
  - `llmbuilder train model -d ./data/clean.txt -t ./tokenizers -o ./checkpoints`
- Generate text (interactive):
  - `llmbuilder generate text --setup`

## Python API Quickstart

```python
import llmbuilder as lb

# Load a preset config and build a model
cfg = lb.load_config(preset="cpu_small")
model = lb.build_model(cfg.model)

# Train (example; see examples/train_tiny.py for a runnable script)
from llmbuilder.data import TextDataset
dataset = TextDataset("./data/clean.txt", block_size=cfg.model.max_seq_length)
results = lb.train_model(model, dataset, cfg.training)

# Generate text
text = lb.generate_text(
    model_path="./checkpoints/model.pt",
    tokenizer_path="./tokenizers",
    prompt="Hello world",
    max_new_tokens=50,
)
print(text)
```

## More
- Examples: see the `examples/` folder
  - `examples/generate_text.py`
  - `examples/train_tiny.py`
- Migration from older scripts: see `MIGRATION.md`

## For Developers and Advanced Users
- Python API quickstart:
  ```python
  import llmbuilder as lb
  cfg = lb.load_config(preset="cpu_small")
  model = lb.build_model(cfg.model)
  from llmbuilder.data import TextDataset
  dataset = TextDataset("./data/clean.txt", block_size=cfg.model.max_seq_length)
  results = lb.train_model(model, dataset, cfg.training)
  text = lb.generate_text(
      model_path="./checkpoints/model.pt",
      tokenizer_path="./tokenizers",
      prompt="Hello",
      max_new_tokens=64,
      temperature=0.8,
      top_k=50,
      top_p=0.9,
  )
  print(text)
  ```
- Config presets and legacy keys:
  - Use `lb.load_config(preset="cpu_small")` or `path="config.yaml"`.
  - Legacy flat keys like `n_layer`, `n_head`, `n_embd` are accepted and mapped internally.
- Useful CLI flags:
  - Training: `--epochs`, `--batch-size`, `--lr`, `--eval-interval`, `--save-interval` (see `llmbuilder train model --help`).
  - Generation: `--max-new-tokens`, `--temperature`, `--top-k`, `--top-p`, `--device` (see `llmbuilder generate text --help`).
- Environment knobs:
  - Enable slow tests: `set RUN_SLOW=1`
  - Enable performance tests: `set RUN_PERF=1`
- Performance tips:
  - Prefer CPU wheels for broad compatibility; use smaller seq length and batch size.
  - Checkpoints are saved under `checkpoints/`; consider periodic eval to monitor perplexity.

## Testing (developers)
- Fast tests: `python -m pytest -q tests`
- Slow tests: `set RUN_SLOW=1 && python -m pytest -q tests`
- Performance tests: `set RUN_PERF=1 && python -m pytest -q tests\performance`

## License
Apache-2.0 (or project license).
