# llmbuilder

A modular toolkit for building, training, fine-tuning, generating, and exporting GPT-style language models with CPU-friendly defaults.

## About LLMBuilder Framework

**LLMBuilder** is a production-ready framework for training and fine-tuning Large Language Models (LLMs) â€” not a model itself. Designed for developers, researchers, and AI engineers, LLMBuilder provides a full pipeline to go from raw text data to deployable, optimized LLMs, all running locally on CPUs or GPUs.

### Complete Framework Structure

The full LLMBuilder framework includes:

```
LLMBuilder/
â”œâ”€â”€ data/                   # Data directories
â”‚   â”œâ”€â”€ raw/               # Raw input files (.txt, .pdf, .docx)
â”‚   â”œâ”€â”€ cleaned/           # Processed text files
â”‚   â””â”€â”€ tokens/            # Tokenized datasets
â”‚   â”œâ”€â”€ download_data.py   # Script to download datasets
â”‚   â””â”€â”€ SOURCES.md         # Data sources documentation
â”‚
â”œâ”€â”€ debug_scripts/         # Debugging utilities
â”‚   â”œâ”€â”€ debug_loader.py    # Data loading debugger
â”‚   â”œâ”€â”€ debug_training.py  # Training process debugger
â”‚   â””â”€â”€ debug_timestamps.py # Timing analysis
â”‚
â”œâ”€â”€ eval/                  # Model evaluation
â”‚   â””â”€â”€ eval.py           # Evaluation scripts
â”‚
â”œâ”€â”€ exports/               # Output directories
â”‚   â”œâ”€â”€ checkpoints/      # Training checkpoints
â”‚   â”œâ”€â”€ gguf/             # GGUF model exports
â”‚   â”œâ”€â”€ onnx/             # ONNX model exports
â”‚   â””â”€â”€ tokenizer/        # Saved tokenizer files
â”‚
â”œâ”€â”€ finetune/             # Fine-tuning scripts
â”‚   â”œâ”€â”€ finetune.py      # Fine-tuning implementation
â”‚   â””â”€â”€ __init__.py      # Package initialization
â”‚
â”œâ”€â”€ logs/                 # Training and evaluation logs
â”‚
â”œâ”€â”€ model/                # Model architecture
â”‚   â””â”€â”€ gpt_model.py     # GPT model implementation
â”‚
â”œâ”€â”€ tools/                # Utility scripts
â”‚   â”œâ”€â”€ analyze_data.ps1  # PowerShell data analysis
â”‚   â”œâ”€â”€ analyze_data.sh   # Bash data analysis
â”‚   â”œâ”€â”€ download_hf_model.py # HuggingFace model downloader
â”‚   â””â”€â”€ export_gguf.py    # GGUF export utility
â”‚
â”œâ”€â”€ training/             # Training pipeline
â”‚   â”œâ”€â”€ dataset.py       # Dataset handling
â”‚   â”œâ”€â”€ preprocess.py    # Data preprocessing
â”‚   â”œâ”€â”€ quantization.py  # Model quantization
â”‚   â”œâ”€â”€ train.py         # Main training script
â”‚   â”œâ”€â”€ train_tokenizer.py # Tokenizer training
â”‚   â””â”€â”€ utils.py         # Training utilities
â”‚
â”œâ”€â”€ .gitignore           # Git ignore rules
â”œâ”€â”€ config.json          # Main configuration
â”œâ”€â”€ config_cpu_small.json # Small CPU config
â”œâ”€â”€ config_gpu.json      # GPU configuration
â”œâ”€â”€ inference.py         # Inference script
â”œâ”€â”€ quantize_model.py    # Model quantization
â”œâ”€â”€ README.md           # Documentation
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ run.ps1            # PowerShell runner
â””â”€â”€ run.sh             # Bash runner
```

ðŸ”— **Full Framework Repository**: [https://github.com/qubasehq/llmbuilder](https://github.com/qubasehq/llmbuilder)

> [!NOTE]
> **This is a separate framework** - The complete LLMBuilder framework shown above is **not related to this package**. It's a standalone, comprehensive framework available at the GitHub repository. This package (`llmbuilder_package`) provides the core modular toolkit, while the complete framework offers additional utilities, debugging tools, and production-ready scripts for comprehensive LLM development workflows.

## Installation

Python 3.9+ recommended.

- CPU-only PyTorch:
  - `pip install torch --index-url https://download.pytorch.org/whl/cpu`
- Core dependencies:
  - `pip install -e .`

Optional:
- Performance tests: `pip install pytest-benchmark`

## CLI Quickstart

- Show help:
  - `llmbuilder --help`
- Preprocess data from mixed files into a text corpus:
  - `llmbuilder data preprocess -i ./data/raw -o ./data/clean.txt`
- Train tokenizer (BPE via SentencePiece):
  - `llmbuilder data tokenizer -i ./data/clean.txt -o ./tokenizers --vocab-size 16000`
- Train model (tiny CPU-friendly settings):
  - `llmbuilder train model -d ./data/clean.txt -t ./tokenizers -o ./checkpoints`
- Generate text (interactive):
  - `llmbuilder generate text --setup`

## Python API Quickstart

```python
import llmbuilder as lb

# Load a preset config and build a model
cfg = lb.load_config(preset="cpu_small")
model = lb.build_model(cfg.model)

# Train (example; see examples/train_tiny.py for a runnable script)
from llmbuilder.data import TextDataset
dataset = TextDataset("./data/clean.txt", block_size=cfg.model.max_seq_length)
results = lb.train_model(model, dataset, cfg.training)

# Generate text
text = lb.generate_text(
    model_path="./checkpoints/model.pt",
    tokenizer_path="./tokenizers",
    prompt="Hello world",
    max_new_tokens=50,
)
print(text)
```

## More
- Examples: see the `examples/` folder
  - `examples/generate_text.py`
  - `examples/train_tiny.py`
- Migration from older scripts: see `MIGRATION.md`

## For Developers and Advanced Users
- Python API quickstart:
  ```python
  import llmbuilder as lb
  cfg = lb.load_config(preset="cpu_small")
  model = lb.build_model(cfg.model)
  from llmbuilder.data import TextDataset
  dataset = TextDataset("./data/clean.txt", block_size=cfg.model.max_seq_length)
  results = lb.train_model(model, dataset, cfg.training)
  text = lb.generate_text(
      model_path="./checkpoints/model.pt",
      tokenizer_path="./tokenizers",
      prompt="Hello",
      max_new_tokens=64,
      temperature=0.8,
      top_k=50,
      top_p=0.9,
  )
  print(text)
  ```
- Config presets and legacy keys:
  - Use `lb.load_config(preset="cpu_small")` or `path="config.yaml"`.
  - Legacy flat keys like `n_layer`, `n_head`, `n_embd` are accepted and mapped internally.
- Useful CLI flags:
  - Training: `--epochs`, `--batch-size`, `--lr`, `--eval-interval`, `--save-interval` (see `llmbuilder train model --help`).
  - Generation: `--max-new-tokens`, `--temperature`, `--top-k`, `--top-p`, `--device` (see `llmbuilder generate text --help`).
- Environment knobs:
  - Enable slow tests: `set RUN_SLOW=1`
  - Enable performance tests: `set RUN_PERF=1`
- Performance tips:
  - Prefer CPU wheels for broad compatibility; use smaller seq length and batch size.
  - Checkpoints are saved under `checkpoints/`; consider periodic eval to monitor perplexity.

## Testing (developers)
- Fast tests: `python -m pytest -q tests`
- Slow tests: `set RUN_SLOW=1 && python -m pytest -q tests`
- Performance tests: `set RUN_PERF=1 && python -m pytest -q tests\performance`

## License
Apache-2.0 (or project license).
