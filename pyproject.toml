[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "llmbuilder"
dynamic = ["version"]
description = "A comprehensive toolkit for building, training, and deploying language models"
readme = "README.md"
license = {text = "MIT"}
authors = [
    {name = "Qubase", email = "contact@qubase.in"}
]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers", 
    "Intended Audience :: Science/Research",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9", 
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Software Development :: Libraries :: Python Modules",
]
requires-python = ">=3.8"
keywords = ["llm", "language model", "transformer", "training", "fine-tuning", "nlp", "ai", "machine learning"]

# Keep base dependencies light to reduce install friction and import time.
# Torch is intentionally NOT listed to avoid pulling GPU wheels by default; instruct users in README.
dependencies = [
  "numpy>=1.21",
  "tqdm>=4.64",
  "loguru>=0.6.0",
  "click>=8.0",
]

[project.urls]
Homepage = "https://github.com/qubasehq/llmbuilder-package"
"Bug Reports" = "https://github.com/qubasehq/llmbuilder-package/issues"
Source = "https://github.com/qubasehq/llmbuilder-package"
Documentation = "https://github.com/qubasehq/llmbuilder-package/wiki"

[project.scripts]
llmbuilder = "llmbuilder.cli:main"

[tool.setuptools.dynamic]
version = {attr = "llmbuilder.__version__"}

[tool.setuptools.packages.find]
where = ["."]
include = ["llmbuilder*"]

[tool.setuptools.package-data]
llmbuilder = ["config/defaults/*.json", "templates/*", "README.md"]

[project.optional-dependencies]
test = [
    "pytest>=7.0.0",
    "pytest-benchmark>=4.0.0",
]
dev = [
  "black>=23.0",
  "ruff>=0.1.0",
  "mypy>=1.2.0",
]
tokenizer = [
  "sentencepiece>=0.1.99",
  "tokenizers>=0.13.0",
]
data = [
  "pandas>=1.3",
  "pymupdf>=1.21",
  "docx2txt>=0.8",
  "python-pptx>=0.6.21",
  "markdown>=3.3",
  "beautifulsoup4>=4.10",
]
inference = [
  # If using external transformer pipelines; optional.
  "transformers>=4.30",
]