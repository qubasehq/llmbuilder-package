{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"LLMBuilder Documentation\ud83e\udd16 LLMBuilder","text":"<p>A comprehensive toolkit for building, training, and deploying language models</p>    [![PyPI version](https://badge.fury.io/py/llmbuilder.svg)](https://badge.fury.io/py/llmbuilder)   [![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)   [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)   [![GitHub stars](https://img.shields.io/github/stars/Qubasehq/llmbuilder-package.svg)](https://github.com/Qubasehq/llmbuilder-package/stargazers)"},{"location":"index.html#what-is-llmbuilder","title":"What is LLMBuilder?","text":"<p>LLMBuilder is a production-ready framework for training and fine-tuning Large Language Models (LLMs) \u2014 not a model itself. Designed for developers, researchers, and AI engineers, LLMBuilder provides a complete pipeline to go from raw text data to deployable, optimized LLMs, all running locally on CPUs or GPUs.</p>"},{"location":"index.html#key-features","title":"\ud83c\udfaf Key Features","text":"\ud83d\ude80 Easy to Use\ud83d\udd27 Comprehensive\u26a1 Performance\ud83d\udee0\ufe0f Developer Friendly <ul> <li>One-line training: <code>llmbuilder train model --data data.txt --output model/</code></li> <li>Interactive CLI: Guided setup with <code>llmbuilder welcome</code></li> <li>Python API: Simple <code>import llmbuilder as lb</code> interface</li> <li>CPU-friendly: Optimized for local development</li> </ul> <ul> <li>Data Processing: PDF, DOCX, TXT, and more formats</li> <li>Tokenization: BPE, Unigram, Word, Character models</li> <li>Training: Full GPT-style transformer training</li> <li>Fine-tuning: LoRA and full parameter fine-tuning</li> <li>Export: GGUF, ONNX, quantized formats</li> </ul> <ul> <li>Memory efficient: Gradient checkpointing and mixed precision</li> <li>Scalable: Single GPU to multi-GPU training</li> <li>Fast inference: Optimized text generation</li> <li>Quantization: 8-bit and 16-bit model compression</li> </ul> <ul> <li>Modular design: Use only what you need</li> <li>Extensive docs: Complete API reference and examples</li> <li>Testing: Comprehensive test suite</li> <li>Migration: Easy upgrade from legacy scripts</li> </ul>"},{"location":"index.html#quick-example","title":"Quick Example","text":"<pre><code>import llmbuilder as lb\n\n# Load configuration and build model\ncfg = lb.load_config(preset=\"cpu_small\")\nmodel = lb.build_model(cfg.model)\n\n# Train the model\nfrom llmbuilder.data import TextDataset\ndataset = TextDataset(\"./data/clean.txt\", block_size=cfg.model.max_seq_length)\nresults = lb.train_model(model, dataset, cfg.training)\n\n# Generate text\ntext = lb.generate_text(\n    model_path=\"./checkpoints/model.pt\",\n    tokenizer_path=\"./tokenizers\",\n    prompt=\"The future of AI is\",\n    max_new_tokens=50\n)\nprint(text)\n</code></pre>"},{"location":"index.html#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TB\n    A[Raw Data&lt;br/&gt;PDF, DOCX, TXT] --&gt; B[Data Loader]\n    B --&gt; C[Text Cleaner]\n    C --&gt; D[Tokenizer Training]\n    D --&gt; E[Dataset Creation]\n    E --&gt; F[Model Training]\n    F --&gt; G[Checkpoints]\n    G --&gt; H[Text Generation]\n    G --&gt; I[Model Export&lt;br/&gt;GGUF, ONNX]\n\n    style A fill:#e1f5fe\n    style H fill:#e8f5e8\n    style I fill:#fff3e0</code></pre>"},{"location":"index.html#getting-started","title":"Getting Started","text":"<p>Choose your path to get started with LLMBuilder:</p> <ul> <li> <p> Quick Start</p> <p>Get up and running in 5 minutes with our quick start guide.</p> <p> Quick Start</p> </li> <li> <p> Installation</p> <p>Install LLMBuilder and set up your environment.</p> <p> Installation</p> </li> <li> <p> First Model</p> <p>Train your first language model step by step.</p> <p> First Model</p> </li> <li> <p> User Guide</p> <p>Comprehensive guides for all features and capabilities.</p> <p> User Guide</p> </li> </ul>"},{"location":"index.html#use-cases","title":"Use Cases","text":"<p>Research &amp; Experimentation</p> <p>Perfect for researchers who need to quickly prototype and experiment with different model architectures, training strategies, and datasets.</p> <p>Educational Projects</p> <p>Ideal for students and educators learning about transformer models, with clear examples and comprehensive documentation.</p> <p>Production Deployment</p> <p>Ready for production use with model export, quantization, and optimization features for deployment at scale.</p> <p>Domain-Specific Models</p> <p>Fine-tune models on your specific domain data for improved performance on specialized tasks.</p>"},{"location":"index.html#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub: Qubasehq/llmbuilder-package</li> <li>Issues: Report bugs and request features</li> <li>Discussions: Community discussions</li> <li>Website: qubase.in</li> </ul> <p>Built with \u2764\ufe0f by Qub\u25b3se</p> <p>Empowering developers to create amazing AI applications</p>"},{"location":"api/core.html","title":"Core API","text":"<p>The core LLMBuilder API provides high-level functions for common tasks. These functions are designed to be simple to use while providing access to the full power of the framework.</p>"},{"location":"api/core.html#overview","title":"\ud83c\udfaf Overview","text":"<p>The core API is accessible through the main <code>llmbuilder</code> module:</p> <pre><code>import llmbuilder as lb\n\n# High-level functions\nconfig = lb.load_config(preset=\"cpu_small\")\nmodel = lb.build_model(config.model)\ntext = lb.generate_text(model_path, tokenizer_path, prompt)\n</code></pre>"},{"location":"api/core.html#core-functions","title":"\ud83d\udccb Core Functions","text":""},{"location":"api/core.html#configuration-functions","title":"Configuration Functions","text":""},{"location":"api/core.html#llmbuilder.load_config","title":"<code>llmbuilder.load_config(path=None, preset=None)</code>","text":"<p>Load configuration from file or use preset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[str]</code> <p>Optional path to a JSON/YAML configuration file.</p> <code>None</code> <code>preset</code> <code>Optional[str]</code> <p>Optional name of a built-in preset.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A configuration object suitable for model/training builders.</p> Source code in <code>llmbuilder\\__init__.py</code> <pre><code>def load_config(path: Optional[str] = None, preset: Optional[str] = None) -&gt; Any:\n    \"\"\"Load configuration from file or use preset.\n\n    Args:\n        path: Optional path to a JSON/YAML configuration file.\n        preset: Optional name of a built-in preset.\n\n    Returns:\n        A configuration object suitable for model/training builders.\n    \"\"\"\n    from .config import load_config as _load_config\n    return _load_config(path, preset)\n</code></pre>"},{"location":"api/core.html#model-functions","title":"Model Functions","text":""},{"location":"api/core.html#llmbuilder.build_model","title":"<code>llmbuilder.build_model(config)</code>","text":"<p>Build a model from configuration.</p> Source code in <code>llmbuilder\\__init__.py</code> <pre><code>def build_model(config: Any) -&gt; Any:\n    \"\"\"Build a model from configuration.\"\"\"\n    from .model import build_model as _build_model\n    return _build_model(config)\n</code></pre>"},{"location":"api/core.html#training-functions","title":"Training Functions","text":""},{"location":"api/core.html#llmbuilder.train_model","title":"<code>llmbuilder.train_model(model, dataset, config)</code>","text":"<p>Train a model with the given dataset and configuration.</p> Source code in <code>llmbuilder\\__init__.py</code> <pre><code>def train_model(model: Any, dataset: Any, config: Any) -&gt; Any:\n    \"\"\"Train a model with the given dataset and configuration.\"\"\"\n    from .training import train_model as _train_model\n    return _train_model(model, dataset, config)\n</code></pre>"},{"location":"api/core.html#llmbuilder.train","title":"<code>llmbuilder.train(data_path, output_dir, config=None, clean=False)</code>","text":"<p>High-level training function that handles the complete training pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Union[str, Path, List[Union[str, Path]]]</code> <p>Path to input data file(s) or directory</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Directory to save outputs (tokenizer, checkpoints, etc.)</p> required <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Optional configuration dictionary</p> <code>None</code> <code>clean</code> <code>bool</code> <p>If True, clean up previous outputs before starting</p> <code>False</code> <p>Returns:</p> Name Type Description <code>TrainingPipeline</code> <code>TrainingPipeline</code> <p>The trained pipeline instance</p> Example <p>import llmbuilder</p> Source code in <code>llmbuilder\\__init__.py</code> <pre><code>def train(\n    data_path: Union[str, Path, List[Union[str, Path]]],\n    output_dir: Union[str, Path],\n    config: Optional[Dict[str, Any]] = None,\n    clean: bool = False\n) -&gt; 'TrainingPipeline':\n    \"\"\"\n    High-level training function that handles the complete training pipeline.\n\n    Args:\n        data_path: Path to input data file(s) or directory\n        output_dir: Directory to save outputs (tokenizer, checkpoints, etc.)\n        config: Optional configuration dictionary\n        clean: If True, clean up previous outputs before starting\n\n    Returns:\n        TrainingPipeline: The trained pipeline instance\n\n    Example:\n        &gt;&gt;&gt; import llmbuilder\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Train with default settings\n        &gt;&gt;&gt; pipeline = llmbuilder.train(\n        ...     data_path=\"./my_data/\",\n        ...     output_dir=\"./output/\"\n        ... )\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # Generate text after training\n        &gt;&gt;&gt; text = pipeline.generate(\"The future of AI is\")\n    \"\"\"\n    from .pipeline import train as _train\n    return _train(data_path, output_dir, config or {}, clean)\n</code></pre>"},{"location":"api/core.html#llmbuilder.train--train-with-default-settings","title":"Train with default settings","text":"<p>pipeline = llmbuilder.train( ...     data_path=\"./my_data/\", ...     output_dir=\"./output/\" ... )</p>"},{"location":"api/core.html#llmbuilder.train--generate-text-after-training","title":"Generate text after training","text":"<p>text = pipeline.generate(\"The future of AI is\")</p>"},{"location":"api/core.html#generation-functions","title":"Generation Functions","text":""},{"location":"api/core.html#llmbuilder.generate_text","title":"<code>llmbuilder.generate_text(model_path, tokenizer_path, prompt, **kwargs)</code>","text":"<p>Generate text using a trained model.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to trained model checkpoint</p> required <code>tokenizer_path</code> <code>str</code> <p>Path to tokenizer directory</p> required <code>prompt</code> <code>str</code> <p>Input text prompt</p> required <code>**kwargs</code> <code>Any</code> <p>Additional generation parameters (temperature, top_k, top_p, etc.)</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Generated text string</p> Example <p>import llmbuilder</p> <p>text = llmbuilder.generate_text( ...     model_path=\"./output/checkpoints/model.pt\", ...     tokenizer_path=\"./output/tokenizer/\", ...     prompt=\"The future of AI is\", ...     max_new_tokens=100, ...     temperature=0.8 ... )</p> Source code in <code>llmbuilder\\__init__.py</code> <pre><code>def generate_text(model_path: str, tokenizer_path: str, prompt: str, **kwargs: Any) -&gt; str:\n    \"\"\"\n    Generate text using a trained model.\n\n    Args:\n        model_path: Path to trained model checkpoint\n        tokenizer_path: Path to tokenizer directory\n        prompt: Input text prompt\n        **kwargs: Additional generation parameters (temperature, top_k, top_p, etc.)\n\n    Returns:\n        Generated text string\n\n    Example:\n        &gt;&gt;&gt; import llmbuilder\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; text = llmbuilder.generate_text(\n        ...     model_path=\"./output/checkpoints/model.pt\",\n        ...     tokenizer_path=\"./output/tokenizer/\",\n        ...     prompt=\"The future of AI is\",\n        ...     max_new_tokens=100,\n        ...     temperature=0.8\n        ... )\n    \"\"\"\n    from .inference import generate_text as _generate_text\n    return _generate_text(model_path, tokenizer_path, prompt, **kwargs)\n</code></pre>"},{"location":"api/core.html#llmbuilder.interactive_cli","title":"<code>llmbuilder.interactive_cli(model_path, tokenizer_path, **kwargs)</code>","text":"<p>Start an interactive CLI for text generation.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to trained model checkpoint</p> required <code>tokenizer_path</code> <code>str</code> <p>Path to tokenizer directory</p> required <code>**kwargs</code> <code>Any</code> <p>Additional configuration parameters</p> <code>{}</code> Example <p>import llmbuilder</p> <p>llmbuilder.interactive_cli( ...     model_path=\"./output/checkpoints/model.pt\", ...     tokenizer_path=\"./output/tokenizer/\", ...     temperature=0.8 ... )</p> Source code in <code>llmbuilder\\__init__.py</code> <pre><code>def interactive_cli(model_path: str, tokenizer_path: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Start an interactive CLI for text generation.\n\n    Args:\n        model_path: Path to trained model checkpoint\n        tokenizer_path: Path to tokenizer directory\n        **kwargs: Additional configuration parameters\n\n    Example:\n        &gt;&gt;&gt; import llmbuilder\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; llmbuilder.interactive_cli(\n        ...     model_path=\"./output/checkpoints/model.pt\",\n        ...     tokenizer_path=\"./output/tokenizer/\",\n        ...     temperature=0.8\n        ... )\n    \"\"\"\n    from .inference import interactive_cli as _interactive_cli\n    _interactive_cli(model_path, tokenizer_path, **kwargs)\n</code></pre>"},{"location":"api/core.html#fine-tuning-functions","title":"Fine-tuning Functions","text":""},{"location":"api/core.html#llmbuilder.finetune_model","title":"<code>llmbuilder.finetune_model(model, dataset, config, **kwargs)</code>","text":"<p>Fine-tune a model with the given dataset and configuration.</p> Source code in <code>llmbuilder\\__init__.py</code> <pre><code>def finetune_model(model: Any, dataset: Any, config: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Fine-tune a model with the given dataset and configuration.\"\"\"\n    from .finetune import finetune_model as _finetune_model\n    return _finetune_model(model, dataset, config, **kwargs)\n</code></pre>"},{"location":"api/core.html#quick-examples","title":"\ud83d\ude80 Quick Examples","text":""},{"location":"api/core.html#basic-training-pipeline","title":"Basic Training Pipeline","text":"<pre><code>import llmbuilder as lb\n\n# 1. Load configuration\nconfig = lb.load_config(preset=\"cpu_small\")\n\n# 2. Build model\nmodel = lb.build_model(config.model)\n\n# 3. Prepare dataset\nfrom llmbuilder.data import TextDataset\ndataset = TextDataset(\"training_data.txt\", block_size=config.model.max_seq_length)\n\n# 4. Train model\nresults = lb.train_model(model, dataset, config.training)\n\n# 5. Generate text\ntext = lb.generate_text(\n    model_path=\"./checkpoints/model.pt\",\n    tokenizer_path=\"./tokenizers\",\n    prompt=\"Hello world\",\n    max_new_tokens=50\n)\n</code></pre>"},{"location":"api/core.html#high-level-training","title":"High-Level Training","text":"<pre><code>import llmbuilder as lb\n\n# Complete training pipeline in one function\npipeline = lb.train(\n    data_path=\"./my_data/\",\n    output_dir=\"./output/\",\n    config={\n        \"model\": {\"num_layers\": 8, \"embedding_dim\": 512},\n        \"training\": {\"num_epochs\": 10, \"batch_size\": 16}\n    }\n)\n\n# Generate text after training\ntext = pipeline.generate(\"The future of AI is\")\n</code></pre>"},{"location":"api/core.html#interactive-generation","title":"Interactive Generation","text":"<pre><code>import llmbuilder as lb\n\n# Start interactive text generation\nlb.interactive_cli(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer/\",\n    temperature=0.8,\n    max_new_tokens=100\n)\n</code></pre>"},{"location":"api/core.html#advanced-usage","title":"\ud83d\udd27 Advanced Usage","text":""},{"location":"api/core.html#custom-configuration","title":"Custom Configuration","text":"<pre><code>import llmbuilder as lb\nfrom llmbuilder.config import Config, ModelConfig, TrainingConfig\n\n# Create custom configuration\nconfig = Config(\n    model=ModelConfig(\n        vocab_size=32000,\n        num_layers=24,\n        num_heads=16,\n        embedding_dim=1024\n    ),\n    training=TrainingConfig(\n        batch_size=8,\n        learning_rate=1e-4,\n        num_epochs=20\n    )\n)\n\n# Use with core functions\nmodel = lb.build_model(config.model)\n</code></pre>"},{"location":"api/core.html#error-handling","title":"Error Handling","text":"<pre><code>import llmbuilder as lb\nfrom llmbuilder.utils import ModelError, DataError\n\ntry:\n    config = lb.load_config(\"config.json\")\n    model = lb.build_model(config.model)\nexcept ModelError as e:\n    print(f\"Model error: {e}\")\nexcept DataError as e:\n    print(f\"Data error: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"api/core.html#return-types","title":"\ud83d\udcca Return Types","text":""},{"location":"api/core.html#training-results","title":"Training Results","text":"<pre><code>results = lb.train_model(model, dataset, config)\n\n# Access training metrics\nprint(f\"Final loss: {results.final_loss}\")\nprint(f\"Training time: {results.training_time}\")\nprint(f\"Best validation loss: {results.best_val_loss}\")\nprint(f\"Model path: {results.model_path}\")\n</code></pre>"},{"location":"api/core.html#generation-results","title":"Generation Results","text":"<pre><code># Simple string return\ntext = lb.generate_text(model_path, tokenizer_path, prompt)\n\n# With detailed results\nfrom llmbuilder.inference import generate_with_details\n\nresult = generate_with_details(\n    model_path=model_path,\n    tokenizer_path=tokenizer_path,\n    prompt=prompt,\n    return_details=True\n)\n\nprint(f\"Generated text: {result.text}\")\nprint(f\"Generation time: {result.generation_time}\")\nprint(f\"Tokens per second: {result.tokens_per_second}\")\n</code></pre>"},{"location":"api/core.html#best-practices","title":"\ud83c\udfaf Best Practices","text":""},{"location":"api/core.html#1-configuration-management","title":"1. Configuration Management","text":"<pre><code># Use presets as starting points\nconfig = lb.load_config(preset=\"gpu_medium\")\n\n# Modify specific settings\nconfig.model.num_layers = 16\nconfig.training.learning_rate = 5e-5\n\n# Save for reuse\nconfig.save(\"my_config.json\")\n</code></pre>"},{"location":"api/core.html#2-resource-management","title":"2. Resource Management","text":"<pre><code># Check available resources\nfrom llmbuilder.utils import get_device_info\n\ndevice_info = get_device_info()\nif device_info.has_cuda:\n    config = lb.load_config(preset=\"gpu_medium\")\nelse:\n    config = lb.load_config(preset=\"cpu_small\")\n</code></pre>"},{"location":"api/core.html#3-error-recovery","title":"3. Error Recovery","text":"<pre><code># Implement checkpointing\ntry:\n    results = lb.train_model(model, dataset, config)\nexcept KeyboardInterrupt:\n    print(\"Training interrupted, saving checkpoint...\")\n    # Checkpoint is automatically saved\nexcept Exception as e:\n    print(f\"Training failed: {e}\")\n    # Resume from last checkpoint if available\n</code></pre> <p>Core API Tips</p> <ul> <li>Start with high-level functions and move to lower-level APIs as needed</li> <li>Use configuration presets as starting points</li> <li>Always handle exceptions appropriately</li> <li>Take advantage of automatic checkpointing for long training runs</li> </ul>"},{"location":"cli/overview.html","title":"CLI Overview","text":"<p>LLMBuilder provides a comprehensive command-line interface (CLI) that makes it easy to train, fine-tune, and deploy language models without writing code. This guide covers all CLI commands and their usage.</p>"},{"location":"cli/overview.html#getting-started","title":"\ud83d\ude80 Getting Started","text":""},{"location":"cli/overview.html#installation-verification","title":"Installation Verification","text":"<p>First, verify that LLMBuilder is properly installed:</p> <pre><code>llmbuilder --version\nllmbuilder --help\n</code></pre>"},{"location":"cli/overview.html#welcome-command","title":"Welcome Command","text":"<p>For first-time users, start with the welcome command:</p> <pre><code>llmbuilder welcome\n</code></pre> <p>This interactive command guides you through: - Learning about LLMBuilder - Creating configuration files - Processing data - Training models - Generating text</p>"},{"location":"cli/overview.html#command-structure","title":"\ud83d\udccb Command Structure","text":"<p>LLMBuilder CLI follows a hierarchical command structure:</p> <pre><code>llmbuilder [GLOBAL_OPTIONS] COMMAND [COMMAND_OPTIONS] [ARGS]\n</code></pre>"},{"location":"cli/overview.html#global-options","title":"Global Options","text":"Option Description <code>--version</code> Show version and exit <code>--verbose</code>, <code>-v</code> Enable verbose output <code>--help</code> Show help message"},{"location":"cli/overview.html#main-commands","title":"Main Commands","text":"Command Description <code>welcome</code> Interactive getting started guide <code>info</code> Display package information <code>config</code> Configuration management <code>data</code> Data processing and loading <code>train</code> Model training <code>finetune</code> Model fine-tuning <code>generate</code> Text generation <code>model</code> Model management <code>export</code> Model export utilities"},{"location":"cli/overview.html#command-categories","title":"\ud83c\udfaf Command Categories","text":""},{"location":"cli/overview.html#information-commands","title":"Information Commands","text":""},{"location":"cli/overview.html#welcome","title":"<code>welcome</code>","text":"<p>Interactive getting started experience:</p> <pre><code>llmbuilder welcome\n</code></pre> <p>Features: - Guided setup process - Learn about LLMBuilder capabilities - Quick access to common tasks - Beginner-friendly explanations</p>"},{"location":"cli/overview.html#info","title":"<code>info</code>","text":"<p>Display package information and credits:</p> <pre><code>llmbuilder info\n</code></pre> <p>Shows: - Package version and description - Available modules and their purposes - Quick command examples - Links to documentation and support</p>"},{"location":"cli/overview.html#configuration-commands","title":"Configuration Commands","text":""},{"location":"cli/overview.html#config-create","title":"<code>config create</code>","text":"<p>Create configuration files with presets:</p> <pre><code># Interactive configuration creation\nllmbuilder config create --interactive\n\n# Create from preset\nllmbuilder config create --preset cpu_small --output config.json\n\n# Available presets: cpu_small, gpu_medium, gpu_large, inference\n</code></pre>"},{"location":"cli/overview.html#config-validate","title":"<code>config validate</code>","text":"<p>Validate configuration files:</p> <pre><code>llmbuilder config validate config.json\n</code></pre>"},{"location":"cli/overview.html#config-list","title":"<code>config list</code>","text":"<p>List available configuration presets:</p> <pre><code>llmbuilder config list\n</code></pre>"},{"location":"cli/overview.html#data-processing-commands","title":"Data Processing Commands","text":""},{"location":"cli/overview.html#data-load","title":"<code>data load</code>","text":"<p>Load and preprocess text data from various formats:</p> <pre><code># Interactive data loading\nllmbuilder data load --interactive\n\n# Process specific directory\nllmbuilder data load \\\n  --input ./documents \\\n  --output clean_text.txt \\\n  --format all \\\n  --clean \\\n  --min-length 100\n</code></pre>"},{"location":"cli/overview.html#data-tokenizer","title":"<code>data tokenizer</code>","text":"<p>Train tokenizers on text data:</p> <pre><code>llmbuilder data tokenizer \\\n  --input training_data.txt \\\n  --output ./tokenizer \\\n  --vocab-size 16000 \\\n  --model-type bpe\n</code></pre>"},{"location":"cli/overview.html#training-commands","title":"Training Commands","text":""},{"location":"cli/overview.html#train-model","title":"<code>train model</code>","text":"<p>Train language models from scratch:</p> <pre><code># Interactive training setup\nllmbuilder train model --interactive\n\n# Direct training\nllmbuilder train model \\\n  --config config.json \\\n  --data training_data.txt \\\n  --tokenizer ./tokenizer \\\n  --output ./model \\\n  --epochs 10 \\\n  --batch-size 16\n</code></pre>"},{"location":"cli/overview.html#train-resume","title":"<code>train resume</code>","text":"<p>Resume training from checkpoints:</p> <pre><code>llmbuilder train resume \\\n  --checkpoint ./model/checkpoint_1000.pt \\\n  --data training_data.txt \\\n  --output ./continued_model\n</code></pre>"},{"location":"cli/overview.html#fine-tuning-commands","title":"Fine-tuning Commands","text":""},{"location":"cli/overview.html#finetune-model","title":"<code>finetune model</code>","text":"<p>Fine-tune pre-trained models:</p> <pre><code>llmbuilder finetune model \\\n  --model ./pretrained_model/model.pt \\\n  --dataset domain_data.txt \\\n  --output ./finetuned_model \\\n  --epochs 5 \\\n  --lr 5e-5 \\\n  --use-lora\n</code></pre>"},{"location":"cli/overview.html#generation-commands","title":"Generation Commands","text":""},{"location":"cli/overview.html#generate-text","title":"<code>generate text</code>","text":"<p>Generate text with trained models:</p> <pre><code># Interactive generation\nllmbuilder generate text --setup\n\n# Direct generation\nllmbuilder generate text \\\n  --model ./model/model.pt \\\n  --tokenizer ./tokenizer \\\n  --prompt \"The future of AI is\" \\\n  --max-tokens 100 \\\n  --temperature 0.8\n\n# Interactive chat mode\nllmbuilder generate text \\\n  --model ./model/model.pt \\\n  --tokenizer ./tokenizer \\\n  --interactive\n</code></pre>"},{"location":"cli/overview.html#model-management-commands","title":"Model Management Commands","text":""},{"location":"cli/overview.html#model-create","title":"<code>model create</code>","text":"<p>Create new model architectures:</p> <pre><code>llmbuilder model create \\\n  --vocab-size 16000 \\\n  --layers 12 \\\n  --heads 12 \\\n  --dim 768 \\\n  --output ./new_model\n</code></pre>"},{"location":"cli/overview.html#model-info","title":"<code>model info</code>","text":"<p>Display model information:</p> <pre><code>llmbuilder model info ./model/model.pt\n</code></pre>"},{"location":"cli/overview.html#model-evaluate","title":"<code>model evaluate</code>","text":"<p>Evaluate model performance:</p> <pre><code>llmbuilder model evaluate \\\n  ./model/model.pt \\\n  --dataset test_data.txt \\\n  --batch-size 32\n</code></pre>"},{"location":"cli/overview.html#export-commands","title":"Export Commands","text":""},{"location":"cli/overview.html#export-gguf","title":"<code>export gguf</code>","text":"<p>Export models to GGUF format:</p> <pre><code>llmbuilder export gguf \\\n  ./model/model.pt \\\n  --output model.gguf \\\n  --quantization q4_0\n</code></pre>"},{"location":"cli/overview.html#export-onnx","title":"<code>export onnx</code>","text":"<p>Export models to ONNX format:</p> <pre><code>llmbuilder export onnx \\\n  ./model/model.pt \\\n  --output model.onnx \\\n  --opset 11\n</code></pre>"},{"location":"cli/overview.html#export-quantize","title":"<code>export quantize</code>","text":"<p>Quantize models for deployment:</p> <pre><code>llmbuilder export quantize \\\n  ./model/model.pt \\\n  --output quantized_model.pt \\\n  --method dynamic \\\n  --bits 8\n</code></pre>"},{"location":"cli/overview.html#interactive-features","title":"\ud83c\udfa8 Interactive Features","text":""},{"location":"cli/overview.html#guided-setup","title":"Guided Setup","text":"<p>Many commands support <code>--interactive</code> or <code>--setup</code> flags for guided experiences:</p> <pre><code># Interactive data loading\nllmbuilder data load --interactive\n\n# Interactive model training\nllmbuilder train model --interactive\n\n# Interactive text generation setup\nllmbuilder generate text --setup\n</code></pre>"},{"location":"cli/overview.html#progress-indicators","title":"Progress Indicators","text":"<p>LLMBuilder provides rich progress indicators:</p> <pre><code># Training progress with real-time metrics\nllmbuilder train model --data data.txt --output model/ --verbose\n\n# Data processing with progress bars\nllmbuilder data load --input docs/ --output data.txt --verbose\n</code></pre>"},{"location":"cli/overview.html#colorful-output","title":"Colorful Output","text":"<p>The CLI uses colors and emojis for better user experience:</p> <ul> <li>\ud83d\udfe2 Green: Success messages</li> <li>\ud83d\udd35 Blue: Information and headers</li> <li>\ud83d\udfe1 Yellow: Warnings and prompts</li> <li>\ud83d\udd34 Red: Errors</li> <li>\ud83c\udfaf Emojis: Visual indicators for different operations</li> </ul>"},{"location":"cli/overview.html#advanced-usage","title":"\ud83d\udd27 Advanced Usage","text":""},{"location":"cli/overview.html#configuration-files","title":"Configuration Files","text":"<p>Use configuration files for complex setups:</p> <pre><code># Create configuration\nllmbuilder config create --preset gpu_medium --output training_config.json\n\n# Use configuration in training\nllmbuilder train model --config training_config.json --data data.txt --output model/\n</code></pre>"},{"location":"cli/overview.html#environment-variables","title":"Environment Variables","text":"<p>Set environment variables for default behavior:</p> <pre><code># Set default device\nexport LLMBUILDER_DEVICE=cuda\n\n# Set cache directory\nexport LLMBUILDER_CACHE_DIR=/path/to/cache\n\n# Enable debug logging\nexport LLMBUILDER_LOG_LEVEL=DEBUG\n</code></pre>"},{"location":"cli/overview.html#batch-processing","title":"Batch Processing","text":"<p>Process multiple files or configurations:</p> <pre><code># Process multiple data directories\nllmbuilder data load \\\n  --input \"dir1,dir2,dir3\" \\\n  --output combined_data.txt\n\n# Train multiple model variants\nfor preset in cpu_small gpu_medium gpu_large; do\n  llmbuilder config create --preset $preset --output ${preset}_config.json\n  llmbuilder train model --config ${preset}_config.json --data data.txt --output ${preset}_model/\ndone\n</code></pre>"},{"location":"cli/overview.html#pipeline-automation","title":"Pipeline Automation","text":"<p>Chain commands for complete workflows:</p> <pre><code>#!/bin/bash\n# Complete training pipeline\n\n# 1. Process data\nllmbuilder data load \\\n  --input ./raw_documents \\\n  --output training_data.txt \\\n  --clean --min-length 100\n\n# 2. Train tokenizer\nllmbuilder data tokenizer \\\n  --input training_data.txt \\\n  --output ./tokenizer \\\n  --vocab-size 16000\n\n# 3. Create configuration\nllmbuilder config create \\\n  --preset gpu_medium \\\n  --output model_config.json\n\n# 4. Train model\nllmbuilder train model \\\n  --config model_config.json \\\n  --data training_data.txt \\\n  --tokenizer ./tokenizer \\\n  --output ./trained_model\n\n# 5. Test generation\nllmbuilder generate text \\\n  --model ./trained_model/model.pt \\\n  --tokenizer ./tokenizer \\\n  --prompt \"Test generation\" \\\n  --max-tokens 50\n\necho \"Training pipeline completed!\"\n</code></pre>"},{"location":"cli/overview.html#error-handling","title":"\ud83d\udea8 Error Handling","text":""},{"location":"cli/overview.html#common-error-messages","title":"Common Error Messages","text":""},{"location":"cli/overview.html#configuration-errors","title":"Configuration Errors","text":"<pre><code>\u274c Configuration validation failed: num_heads (8) must divide embedding_dim (512)\n\ud83d\udca1 Try: Set num_heads to 4, 8, or 16\n</code></pre>"},{"location":"cli/overview.html#data-errors","title":"Data Errors","text":"<pre><code>\u274c No supported files found in directory: ./documents\n\ud83d\udca1 Supported formats: .txt, .pdf, .docx, .html, .md\n</code></pre>"},{"location":"cli/overview.html#memory-errors","title":"Memory Errors","text":"<pre><code>\u274c CUDA out of memory\n\ud83d\udca1 Try: Reduce batch size with --batch-size 4 or use CPU with --device cpu\n</code></pre>"},{"location":"cli/overview.html#model-errors","title":"Model Errors","text":"<pre><code>\u274c Model file not found: ./model/model.pt\n\ud83d\udca1 Check the model path or train a model first with: llmbuilder train model\n</code></pre>"},{"location":"cli/overview.html#debugging-tips","title":"Debugging Tips","text":"<p>Enable verbose output for detailed information:</p> <pre><code>llmbuilder --verbose train model --data data.txt --output model/\n</code></pre> <p>Check system information:</p> <pre><code>llmbuilder info --system\n</code></pre> <p>Validate configurations before use:</p> <pre><code>llmbuilder config validate config.json --strict\n</code></pre>"},{"location":"cli/overview.html#output-and-logging","title":"\ud83d\udcca Output and Logging","text":""},{"location":"cli/overview.html#standard-output","title":"Standard Output","text":"<p>LLMBuilder provides structured output:</p> <pre><code>\ud83d\ude80 Starting model training...\n\ud83d\udcca Dataset: 10,000 samples\n\ud83e\udde0 Model: 12.5M parameters\n\ud83d\udcc8 Training progress:\n  Epoch 1/10: loss=3.45, lr=0.0003, time=2m 15s\n  Epoch 2/10: loss=2.87, lr=0.0003, time=2m 12s\n  ...\n\u2705 Training completed successfully!\n\ud83d\udcbe Model saved to: ./model/model.pt\n</code></pre>"},{"location":"cli/overview.html#log-files","title":"Log Files","text":"<p>Training and processing logs are automatically saved:</p> <pre><code>./model/\n\u251c\u2500\u2500 model.pt              # Trained model\n\u251c\u2500\u2500 config.json           # Training configuration\n\u251c\u2500\u2500 training.log          # Detailed training logs\n\u251c\u2500\u2500 metrics.json          # Training metrics\n\u2514\u2500\u2500 checkpoints/          # Training checkpoints\n    \u251c\u2500\u2500 checkpoint_1000.pt\n    \u251c\u2500\u2500 checkpoint_2000.pt\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"cli/overview.html#json-output","title":"JSON Output","text":"<p>Use <code>--json</code> flag for machine-readable output:</p> <pre><code>llmbuilder model info ./model/model.pt --json\n</code></pre> <pre><code>{\n  \"model_path\": \"./model/model.pt\",\n  \"parameters\": 12500000,\n  \"architecture\": {\n    \"num_layers\": 12,\n    \"num_heads\": 12,\n    \"embedding_dim\": 768,\n    \"vocab_size\": 16000\n  },\n  \"training_info\": {\n    \"final_loss\": 2.45,\n    \"training_time\": \"45m 23s\",\n    \"epochs\": 10\n  }\n}\n</code></pre>"},{"location":"cli/overview.html#best-practices","title":"\ud83c\udfaf Best Practices","text":""},{"location":"cli/overview.html#1-start-interactive","title":"1. Start Interactive","text":"<p>For new users, always start with interactive modes:</p> <pre><code>llmbuilder welcome\nllmbuilder data load --interactive\nllmbuilder train model --interactive\n</code></pre>"},{"location":"cli/overview.html#2-use-configurations","title":"2. Use Configurations","text":"<p>Save and reuse configurations for consistency:</p> <pre><code># Create and save configuration\nllmbuilder config create --preset gpu_medium --output my_config.json\n\n# Reuse configuration\nllmbuilder train model --config my_config.json --data data.txt --output model/\n</code></pre>"},{"location":"cli/overview.html#3-validate-before-training","title":"3. Validate Before Training","text":"<p>Always validate configurations and data:</p> <pre><code>llmbuilder config validate config.json\nllmbuilder data load --input data/ --output test.txt --dry-run\n</code></pre>"},{"location":"cli/overview.html#4-monitor-progress","title":"4. Monitor Progress","text":"<p>Use verbose mode for long-running operations:</p> <pre><code>llmbuilder --verbose train model --config config.json --data data.txt --output model/\n</code></pre>"},{"location":"cli/overview.html#5-save-intermediate-results","title":"5. Save Intermediate Results","text":"<p>Use checkpointing and intermediate saves:</p> <pre><code>llmbuilder train model \\\n  --config config.json \\\n  --data data.txt \\\n  --output model/ \\\n  --save-every 1000 \\\n  --eval-every 500\n</code></pre> <p>CLI Tips</p> <ul> <li>Use tab completion if available in your shell</li> <li>Combine <code>--help</code> with any command to see all options</li> <li>Use <code>--dry-run</code> flags when available to test commands</li> <li>Save successful command combinations as shell scripts</li> <li>Use configuration files for complex setups</li> </ul>"},{"location":"examples/basic-training.html","title":"Basic Training Example","text":"<p>This comprehensive example demonstrates how to train a language model from scratch using LLMBuilder. We'll cover data preparation, tokenizer training, model training, and text generation.</p>"},{"location":"examples/basic-training.html#overview","title":"\ud83c\udfaf Overview","text":"<p>In this example, we'll:</p> <ol> <li>Prepare training data from various document formats</li> <li>Train a tokenizer on our text corpus</li> <li>Configure and train a language model</li> <li>Generate text with the trained model</li> <li>Evaluate performance and iterate</li> </ol>"},{"location":"examples/basic-training.html#project-structure","title":"\ud83d\udcc1 Project Structure","text":"<pre><code>basic_training_example/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/                    # Raw documents (PDF, DOCX, TXT)\n\u2502   \u2514\u2500\u2500 processed/              # Cleaned text files\n\u251c\u2500\u2500 config/\n\u2502   \u2514\u2500\u2500 training_config.json    # Training configuration\n\u251c\u2500\u2500 output/\n\u2502   \u251c\u2500\u2500 tokenizer/              # Trained tokenizer\n\u2502   \u251c\u2500\u2500 model/                  # Trained model\n\u2502   \u2514\u2500\u2500 logs/                   # Training logs\n\u2514\u2500\u2500 train_model.py              # Main training script\n</code></pre>"},{"location":"examples/basic-training.html#complete-training-script","title":"\ud83d\ude80 Complete Training Script","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nBasic Training Example for LLMBuilder\n\nThis script demonstrates a complete training pipeline:\n1. Data loading and preprocessing\n2. Tokenizer training\n3. Model training\n4. Text generation and evaluation\n\"\"\"\n\nimport os\nimport sys\nfrom pathlib import Path\nimport logging\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef main():\n    \"\"\"Main training pipeline.\"\"\"\n\n    # Configuration\n    project_dir = Path(__file__).parent\n    raw_data_dir = project_dir / \"data\" / \"raw\"\n    processed_data_dir = project_dir / \"data\" / \"processed\"\n    output_dir = project_dir / \"output\"\n\n    # Create directories\n    for dir_path in [processed_data_dir, output_dir]:\n        dir_path.mkdir(parents=True, exist_ok=True)\n\n    logger.info(\"\ud83d\ude80 Starting LLMBuilder training pipeline\")\n\n    # Step 1: Data Preparation\n    logger.info(\"\ud83d\udcc1 Step 1: Preparing training data\")\n    training_data_path = prepare_training_data(raw_data_dir, processed_data_dir)\n\n    # Step 2: Tokenizer Training\n    logger.info(\"\ud83d\udd24 Step 2: Training tokenizer\")\n    tokenizer_dir = train_tokenizer(training_data_path, output_dir)\n\n    # Step 3: Model Training\n    logger.info(\"\ud83e\udde0 Step 3: Training language model\")\n    model_path = train_language_model(training_data_path, tokenizer_dir, output_dir)\n\n    # Step 4: Text Generation\n    logger.info(\"\ud83c\udfaf Step 4: Testing text generation\")\n    test_text_generation(model_path, tokenizer_dir)\n\n    # Step 5: Evaluation\n    logger.info(\"\ud83d\udcca Step 5: Evaluating model performance\")\n    evaluate_model(model_path, tokenizer_dir, training_data_path)\n\n    logger.info(\"\u2705 Training pipeline completed successfully!\")\n\ndef prepare_training_data(raw_data_dir, processed_data_dir):\n    \"\"\"Load and preprocess training data from various formats.\"\"\"\n    from llmbuilder.data import DataLoader, TextCleaner\n\n    # Initialize data loader\n    loader = DataLoader(\n        min_length=50,              # Filter short texts\n        clean_text=True,            # Apply basic cleaning\n        remove_duplicates=True      # Remove duplicate content\n    )\n\n    # Load all supported files\n    texts = []\n    supported_extensions = ['.txt', '.pdf', '.docx', '.md', '.html']\n\n    logger.info(f\"Loading documents from {raw_data_dir}\")\n\n    if raw_data_dir.exists():\n        for file_path in raw_data_dir.rglob(\"*\"):\n            if file_path.suffix.lower() in supported_extensions:\n                try:\n                    text = loader.load_file(file_path)\n                    if text:\n                        texts.append(text)\n                        logger.info(f\"  \u2705 Loaded {file_path.name}: {len(text):,} characters\")\n                except Exception as e:\n                    logger.warning(f\"  \u274c Failed to load {file_path.name}: {e}\")\n\n    # If no files found, create sample data\n    if not texts:\n        logger.info(\"No data files found, creating sample training data\")\n        sample_text = create_sample_data()\n        texts = [sample_text]\n\n    # Combine and clean texts\n    combined_text = \"\\n\\n\".join(texts)\n\n    # Advanced text cleaning\n    cleaner = TextCleaner(\n        normalize_whitespace=True,\n        remove_urls=True,\n        remove_emails=True,\n        min_sentence_length=20,\n        remove_duplicates=True,\n        language_filter=\"en\"        # Keep only English text\n    )\n\n    cleaned_text = cleaner.clean(combined_text)\n    stats = cleaner.get_stats()\n\n    logger.info(f\"Text cleaning results:\")\n    logger.info(f\"  Original: {stats.original_length:,} characters\")\n    logger.info(f\"  Cleaned: {stats.cleaned_length:,} characters\")\n    logger.info(f\"  Removed: {stats.removal_percentage:.1f}%\")\n\n    # Save processed data\n    training_data_path = processed_data_dir / \"training_data.txt\"\n    with open(training_data_path, 'w', encoding='utf-8') as f:\n        f.write(cleaned_text)\n\n    logger.info(f\"Training data saved to {training_data_path}\")\n    return training_data_path\n\ndef train_tokenizer(training_data_path, output_dir):\n    \"\"\"Train a BPE tokenizer on the training data.\"\"\"\n    from llmbuilder.tokenizer import TokenizerTrainer\n    from llmbuilder.config import TokenizerConfig\n\n    tokenizer_dir = output_dir / \"tokenizer\"\n    tokenizer_dir.mkdir(exist_ok=True)\n\n    # Configure tokenizer\n    config = TokenizerConfig(\n        vocab_size=16000,           # Vocabulary size\n        model_type=\"bpe\",           # Byte-Pair Encoding\n        character_coverage=1.0,     # Cover all characters\n        max_sentence_length=4096,   # Maximum sentence length\n        special_tokens=[            # Special tokens\n            \"&lt;pad&gt;\", \"&lt;unk&gt;\", \"&lt;s&gt;\", \"&lt;/s&gt;\", \"&lt;mask&gt;\"\n        ]\n    )\n\n    # Train tokenizer\n    trainer = TokenizerTrainer(config=config)\n    results = trainer.train(\n        input_file=str(training_data_path),\n        output_dir=str(tokenizer_dir),\n        model_prefix=\"tokenizer\"\n    )\n\n    logger.info(f\"Tokenizer training completed:\")\n    logger.info(f\"  Model: {results['model_file']}\")\n    logger.info(f\"  Vocab: {results['vocab_file']}\")\n    logger.info(f\"  Training time: {results['training_time']:.1f}s\")\n\n    # Test tokenizer\n    from llmbuilder.tokenizer import Tokenizer\n    tokenizer = Tokenizer.from_pretrained(str(tokenizer_dir))\n\n    test_text = \"Hello, world! This is a test of the tokenizer.\"\n    tokens = tokenizer.encode(test_text)\n    decoded = tokenizer.decode(tokens)\n\n    logger.info(f\"Tokenizer test:\")\n    logger.info(f\"  Original: {test_text}\")\n    logger.info(f\"  Tokens: {tokens}\")\n    logger.info(f\"  Decoded: {decoded}\")\n    logger.info(f\"  Perfect reconstruction: {test_text == decoded}\")\n\n    return tokenizer_dir\n\ndef train_language_model(training_data_path, tokenizer_dir, output_dir):\n    \"\"\"Train the language model.\"\"\"\n    import llmbuilder as lb\n    from llmbuilder.config import Config, ModelConfig, TrainingConfig\n    from llmbuilder.data import TextDataset\n\n    model_dir = output_dir / \"model\"\n    model_dir.mkdir(exist_ok=True)\n\n    # Create configuration\n    config = Config(\n        model=ModelConfig(\n            vocab_size=16000,           # Must match tokenizer\n            num_layers=8,               # Number of transformer layers\n            num_heads=8,                # Number of attention heads\n            embedding_dim=512,          # Embedding dimension\n            max_seq_length=1024,        # Maximum sequence length\n            dropout=0.1,                # Dropout rate\n            model_type=\"gpt\"            # Model architecture\n        ),\n        training=TrainingConfig(\n            batch_size=8,               # Batch size (adjust for your hardware)\n            num_epochs=10,              # Number of training epochs\n            learning_rate=3e-4,         # Learning rate\n            warmup_steps=1000,          # Warmup steps\n            weight_decay=0.01,          # Weight decay\n            max_grad_norm=1.0,          # Gradient clipping\n            save_every=1000,            # Save checkpoint every N steps\n            eval_every=500,             # Evaluate every N steps\n            log_every=100               # Log every N steps\n        )\n    )\n\n    # Save configuration\n    config_path = model_dir / \"config.json\"\n    config.save(str(config_path))\n    logger.info(f\"Configuration saved to {config_path}\")\n\n    # Build model\n    model = lb.build_model(config.model)\n    num_params = sum(p.numel() for p in model.parameters())\n    logger.info(f\"Model built with {num_params:,} parameters\")\n\n    # Prepare dataset\n    dataset = TextDataset(\n        data_path=str(training_data_path),\n        block_size=config.model.max_seq_length,\n        stride=config.model.max_seq_length // 2,  # 50% overlap\n        cache_in_memory=True\n    )\n\n    logger.info(f\"Dataset prepared: {len(dataset):,} samples\")\n\n    # Train model\n    results = lb.train_model(\n        model=model,\n        dataset=dataset,\n        config=config.training,\n        checkpoint_dir=str(model_dir)\n    )\n\n    logger.info(f\"Training completed:\")\n    logger.info(f\"  Final loss: {results.final_loss:.4f}\")\n    logger.info(f\"  Training time: {results.training_time}\")\n    logger.info(f\"  Model saved to: {results.model_path}\")\n\n    return results.model_path\n\ndef test_text_generation(model_path, tokenizer_dir):\n    \"\"\"Test text generation with the trained model.\"\"\"\n    import llmbuilder as lb\n\n    test_prompts = [\n        \"Artificial intelligence is\",\n        \"The future of technology\",\n        \"Machine learning can help us\",\n        \"In the world of programming\",\n        \"The benefits of renewable energy\"\n    ]\n\n    logger.info(\"Testing text generation:\")\n\n    for prompt in test_prompts:\n        try:\n            generated_text = lb.generate_text(\n                model_path=model_path,\n                tokenizer_path=str(tokenizer_dir),\n                prompt=prompt,\n                max_new_tokens=50,\n                temperature=0.8,\n                top_k=50,\n                top_p=0.9\n            )\n\n            logger.info(f\"  Prompt: {prompt}\")\n            logger.info(f\"  Generated: {generated_text}\")\n            logger.info(\"\")\n\n        except Exception as e:\n            logger.error(f\"  Generation failed for '{prompt}': {e}\")\n\ndef evaluate_model(model_path, tokenizer_dir, training_data_path):\n    \"\"\"Evaluate model performance.\"\"\"\n    from llmbuilder.model import load_model\n    from llmbuilder.tokenizer import Tokenizer\n    from llmbuilder.data import TextDataset\n    import torch\n\n    # Load model and tokenizer\n    model = load_model(model_path)\n    tokenizer = Tokenizer.from_pretrained(str(tokenizer_dir))\n\n    # Create evaluation dataset (small sample)\n    eval_dataset = TextDataset(\n        data_path=str(training_data_path),\n        block_size=512,\n        stride=256,\n        max_samples=100  # Small sample for quick evaluation\n    )\n\n    # Calculate perplexity\n    model.eval()\n    total_loss = 0\n    total_tokens = 0\n\n    with torch.no_grad():\n        for i, batch in enumerate(eval_dataset):\n            if i &gt;= 10:  # Limit evaluation for speed\n                break\n\n            input_ids = torch.tensor([batch], dtype=torch.long)\n\n            # Forward pass\n            outputs = model(input_ids[:, :-1])\n            logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n\n            # Calculate loss\n            targets = input_ids[:, 1:]\n            loss = torch.nn.functional.cross_entropy(\n                logits.view(-1, logits.size(-1)),\n                targets.view(-1),\n                ignore_index=tokenizer.pad_token_id\n            )\n\n            total_loss += loss.item()\n            total_tokens += targets.numel()\n\n    avg_loss = total_loss / min(10, len(eval_dataset))\n    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n\n    logger.info(f\"Model evaluation:\")\n    logger.info(f\"  Average loss: {avg_loss:.4f}\")\n    logger.info(f\"  Perplexity: {perplexity:.2f}\")\n    logger.info(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\ndef create_sample_data():\n    \"\"\"Create sample training data if no files are found.\"\"\"\n    return \"\"\"\n    Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.\n\n    Machine learning (ML) is a type of artificial intelligence that allows software applications to become more accurate at predicting outcomes without being explicitly programmed to do so. Machine learning algorithms use historical data as input to predict new output values.\n\n    Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.\n\n    Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n\n    Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do.\n\n    The future of artificial intelligence holds great promise for solving complex problems in healthcare, transportation, education, and many other fields. As AI systems become more sophisticated, they will continue to transform how we work, learn, and interact with technology.\n    \"\"\"\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/basic-training.html#expected-output","title":"\ud83d\udcca Expected Output","text":"<p>When you run this script, you should see output similar to:</p> <pre><code>INFO:__main__:\ud83d\ude80 Starting LLMBuilder training pipeline\nINFO:__main__:\ud83d\udcc1 Step 1: Preparing training data\nINFO:__main__:No data files found, creating sample training data\nINFO:__main__:Text cleaning results:\nINFO:__main__:  Original: 1,234 characters\nINFO:__main__:  Cleaned: 1,180 characters\nINFO:__main__:  Removed: 4.4%\nINFO:__main__:Training data saved to basic_training_example/data/processed/training_data.txt\n\nINFO:__main__:\ud83d\udd24 Step 2: Training tokenizer\nINFO:__main__:Tokenizer training completed:\nINFO:__main__:  Model: basic_training_example/output/tokenizer/tokenizer.model\nINFO:__main__:  Vocab: basic_training_example/output/tokenizer/tokenizer.vocab\nINFO:__main__:  Training time: 5.2s\nINFO:__main__:Tokenizer test:\nINFO:__main__:  Original: Hello, world! This is a test of the tokenizer.\nINFO:__main__:  Tokens: [15496, 995, 0, 1188, 374, 264, 1296, 315, 279, 4037, 3213, 13]\nINFO:__main__:  Decoded: Hello, world! This is a test of the tokenizer.\nINFO:__main__:  Perfect reconstruction: True\n\nINFO:__main__:\ud83e\udde0 Step 3: Training language model\nINFO:__main__:Configuration saved to basic_training_example/output/model/config.json\nINFO:__main__:Model built with 42,123,456 parameters\nINFO:__main__:Dataset prepared: 156 samples\nINFO:__main__:Training completed:\nINFO:__main__:  Final loss: 2.45\nINFO:__main__:  Training time: 0:15:23\nINFO:__main__:  Model saved to: basic_training_example/output/model/model.pt\n\nINFO:__main__:\ud83c\udfaf Step 4: Testing text generation\nINFO:__main__:Testing text generation:\nINFO:__main__:  Prompt: Artificial intelligence is\nINFO:__main__:  Generated: Artificial intelligence is a rapidly evolving field that encompasses machine learning, deep learning, and neural networks...\n\nINFO:__main__:\ud83d\udcca Step 5: Evaluating model performance\nINFO:__main__:Model evaluation:\nINFO:__main__:  Average loss: 2.52\nINFO:__main__:  Perplexity: 12.4\nINFO:__main__:  Model parameters: 42,123,456\n\nINFO:__main__:\u2705 Training pipeline completed successfully!\n</code></pre>"},{"location":"examples/basic-training.html#customization-options","title":"\ud83c\udfaf Customization Options","text":""},{"location":"examples/basic-training.html#1-adjust-model-size","title":"1. Adjust Model Size","text":"<pre><code># Smaller model (faster training, less memory)\nmodel_config = ModelConfig(\n    vocab_size=8000,\n    num_layers=4,\n    num_heads=4,\n    embedding_dim=256,\n    max_seq_length=512\n)\n\n# Larger model (better quality, more resources)\nmodel_config = ModelConfig(\n    vocab_size=32000,\n    num_layers=16,\n    num_heads=16,\n    embedding_dim=1024,\n    max_seq_length=2048\n)\n</code></pre>"},{"location":"examples/basic-training.html#2-modify-training-parameters","title":"2. Modify Training Parameters","text":"<pre><code># Fast training (for testing)\ntraining_config = TrainingConfig(\n    batch_size=16,\n    num_epochs=3,\n    learning_rate=1e-3,\n    save_every=100\n)\n\n# High-quality training (for production)\ntraining_config = TrainingConfig(\n    batch_size=8,\n    num_epochs=50,\n    learning_rate=1e-4,\n    warmup_steps=2000,\n    weight_decay=0.01\n)\n</code></pre>"},{"location":"examples/basic-training.html#3-different-data-sources","title":"3. Different Data Sources","text":"<pre><code># Load from specific file types\nloader = DataLoader(\n    supported_formats=['.txt', '.md'],  # Only text and markdown\n    min_length=100,                     # Longer minimum length\n    max_length=10000,                   # Maximum length limit\n    clean_text=True\n)\n\n# Custom text cleaning\ncleaner = TextCleaner(\n    normalize_whitespace=True,\n    remove_urls=True,\n    remove_emails=True,\n    remove_phone_numbers=True,\n    min_sentence_length=30,\n    language_filter=\"en\",\n    custom_filters=[\n        lambda text: text.replace(\"specific_pattern\", \"replacement\")\n    ]\n)\n</code></pre>"},{"location":"examples/basic-training.html#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"examples/basic-training.html#common-issues","title":"Common Issues","text":""},{"location":"examples/basic-training.html#out-of-memory","title":"Out of Memory","text":"<pre><code># Reduce batch size\ntraining_config.batch_size = 4\n\n# Enable gradient checkpointing\nmodel_config.gradient_checkpointing = True\n\n# Use gradient accumulation\ntraining_config.gradient_accumulation_steps = 4\n</code></pre>"},{"location":"examples/basic-training.html#poor-generation-quality","title":"Poor Generation Quality","text":"<pre><code># Train for more epochs\ntraining_config.num_epochs = 20\n\n# Use more training data\n# Add more text files to data/raw/\n\n# Adjust generation parameters\ngenerated_text = lb.generate_text(\n    model_path=model_path,\n    tokenizer_path=str(tokenizer_dir),\n    prompt=prompt,\n    max_new_tokens=100,\n    temperature=0.7,        # Lower temperature\n    top_k=40,              # More focused sampling\n    repetition_penalty=1.1  # Reduce repetition\n)\n</code></pre>"},{"location":"examples/basic-training.html#slow-training","title":"Slow Training","text":"<pre><code># Use GPU if available\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Reduce sequence length\nmodel_config.max_seq_length = 512\n\n# Increase batch size (if memory allows)\ntraining_config.batch_size = 16\n</code></pre>"},{"location":"examples/basic-training.html#next-steps","title":"\ud83d\udcda Next Steps","text":"<p>After running this basic example:</p> <ol> <li>Experiment with different model sizes and training parameters</li> <li>Add your own training data to the <code>data/raw/</code> directory</li> <li>Try fine-tuning the trained model on specific tasks</li> <li>Export the model for deployment using the export functionality</li> <li>Implement evaluation metrics specific to your use case</li> </ol> <p>Training Tips</p> <ul> <li>Start with small models and datasets to verify everything works</li> <li>Monitor training loss to ensure the model is learning</li> <li>Save checkpoints frequently during long training runs</li> <li>Test generation quality throughout training</li> <li>Keep track of what configurations work best for your data</li> </ul>"},{"location":"getting-started/first-model.html","title":"Your First Model","text":"<p>This comprehensive tutorial will guide you through training your first language model with LLMBuilder, from data preparation to text generation. By the end, you'll have a working model and understand the entire process.</p>"},{"location":"getting-started/first-model.html#what-well-build","title":"\ud83c\udfaf What We'll Build","text":"<p>We'll create a small GPT-style language model that can: - Generate coherent text based on prompts - Complete sentences and paragraphs - Demonstrate understanding of the training data</p> <p>Estimated time: 30-60 minutes Requirements: 4GB RAM, Python 3.8+</p>"},{"location":"getting-started/first-model.html#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<p>Make sure you have LLMBuilder installed:</p> <pre><code>pip install llmbuilder\n</code></pre> <p>Verify the installation:</p> <pre><code>llmbuilder --version\n</code></pre>"},{"location":"getting-started/first-model.html#step-1-prepare-training-data","title":"\ud83d\udcda Step 1: Prepare Training Data","text":""},{"location":"getting-started/first-model.html#option-a-use-sample-data","title":"Option A: Use Sample Data","text":"<p>Create a sample dataset for testing:</p> <pre><code># create_sample_data.py\nsample_text = \"\"\"\nArtificial intelligence is a rapidly evolving field that encompasses machine learning, deep learning, and neural networks. Machine learning algorithms enable computers to learn patterns from data without explicit programming. Deep learning, a subset of machine learning, uses multi-layered neural networks to process complex information.\n\nNatural language processing is a branch of AI that focuses on the interaction between computers and human language. It enables machines to understand, interpret, and generate human language in a valuable way. Applications include chatbots, translation services, and text analysis.\n\nComputer vision is another important area of AI that enables machines to interpret and understand visual information from the world. It combines techniques from machine learning, image processing, and pattern recognition to analyze and understand images and videos.\n\nThe future of artificial intelligence holds great promise for solving complex problems in healthcare, transportation, education, and many other fields. As AI systems become more sophisticated, they will continue to transform how we work, learn, and interact with technology.\n\"\"\"\n\nwith open(\"training_data.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(sample_text)\n\nprint(\"Sample data created: training_data.txt\")\n</code></pre> <p>Run the script:</p> <pre><code>python create_sample_data.py\n</code></pre>"},{"location":"getting-started/first-model.html#option-b-use-your-own-data","title":"Option B: Use Your Own Data","text":"<p>If you have your own text data:</p> <pre><code># Process various document formats\nllmbuilder data load \\\n  --input ./documents \\\n  --output training_data.txt \\\n  --format all \\\n  --clean \\\n  --min-length 100\n</code></pre> <p>This will: - Load PDF, DOCX, TXT files from <code>./documents</code> - Clean and normalize the text - Filter out short passages - Save everything to <code>training_data.txt</code></p>"},{"location":"getting-started/first-model.html#step-2-configure-your-model","title":"\ud83d\udd27 Step 2: Configure Your Model","text":"<p>Create a configuration file for your model:</p> <pre><code>llmbuilder config create \\\n  --preset cpu_small \\\n  --output model_config.json \\\n  --interactive\n</code></pre> <p>This will create a configuration optimized for CPU training. The interactive mode will ask you questions like:</p> <pre><code>\u2699\ufe0f LLMBuilder Configuration Creator\nChoose a preset: cpu_small\nOutput file path: model_config.json\n\ud83e\udde0 Model layers: 6\n\ud83d\udccf Embedding dimension: 384\n\ud83d\udd24 Vocabulary size: 8000\n\ud83d\udce6 Batch size: 8\n\ud83d\udcc8 Learning rate: 0.0003\n</code></pre>"},{"location":"getting-started/first-model.html#understanding-the-configuration","title":"Understanding the Configuration","text":"<p>Let's examine what was created:</p> <pre><code>cat model_config.json\n</code></pre> <pre><code>{\n  \"model\": {\n    \"vocab_size\": 8000,\n    \"num_layers\": 6,\n    \"num_heads\": 6,\n    \"embedding_dim\": 384,\n    \"max_seq_length\": 512,\n    \"dropout\": 0.1\n  },\n  \"training\": {\n    \"batch_size\": 8,\n    \"num_epochs\": 10,\n    \"learning_rate\": 0.0003,\n    \"warmup_steps\": 100,\n    \"save_every\": 1000,\n    \"eval_every\": 500\n  },\n  \"system\": {\n    \"device\": \"cpu\"\n  }\n}\n</code></pre>"},{"location":"getting-started/first-model.html#step-3-train-the-tokenizer","title":"\ud83d\udd24 Step 3: Train the Tokenizer","text":"<p>Before training the model, we need to create a tokenizer:</p> <pre><code>llmbuilder data tokenizer \\\n  --input training_data.txt \\\n  --output ./tokenizer \\\n  --vocab-size 8000 \\\n  --model-type bpe\n</code></pre> <p>This creates a Byte-Pair Encoding (BPE) tokenizer with 8,000 vocabulary items. You'll see output like:</p> <pre><code>\ud83d\udd24 Training BPE tokenizer with vocab size 8000...\n\ud83d\udcca Processing text data...\n\u2699\ufe0f Training tokenizer model...\n\u2705 Tokenizer training completed!\n  Model: ./tokenizer/tokenizer.model\n  Vocab: ./tokenizer/tokenizer.vocab\n  Training time: 12.3s\n</code></pre>"},{"location":"getting-started/first-model.html#test-your-tokenizer","title":"Test Your Tokenizer","text":"<p>Let's verify the tokenizer works:</p> <pre><code># test_tokenizer.py\nfrom llmbuilder.tokenizer import Tokenizer\n\n# Load the tokenizer\ntokenizer = Tokenizer.from_pretrained(\"./tokenizer\")\n\n# Test encoding and decoding\ntext = \"Artificial intelligence is amazing!\"\ntokens = tokenizer.encode(text)\ndecoded = tokenizer.decode(tokens)\n\nprint(f\"Original: {text}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Decoded: {decoded}\")\nprint(f\"Vocabulary size: {len(tokenizer)}\")\n</code></pre>"},{"location":"getting-started/first-model.html#step-4-train-the-model","title":"\ud83e\udde0 Step 4: Train the Model","text":"<p>Now for the main event - training your language model:</p> <pre><code>llmbuilder train model \\\n  --config model_config.json \\\n  --data training_data.txt \\\n  --tokenizer ./tokenizer \\\n  --output ./my_first_model \\\n  --verbose\n</code></pre>"},{"location":"getting-started/first-model.html#what-happens-during-training","title":"What Happens During Training","text":"<p>You'll see output like this:</p> <pre><code>\ud83d\ude80 Starting model training...\n\ud83d\udcca Dataset: 1,234 tokens, 156 samples\n\ud83e\udde0 Model: 2.1M parameters\n\ud83d\udcc8 Training configuration:\n  \u2022 Epochs: 10\n  \u2022 Batch size: 8\n  \u2022 Learning rate: 0.0003\n  \u2022 Device: cpu\n\nEpoch 1/10:\n  Step 10/20: loss=4.23, lr=0.00015, time=2.1s\n  Step 20/20: loss=3.87, lr=0.00030, time=4.2s\n  Validation: loss=3.92, perplexity=50.4\n\nEpoch 2/10:\n  Step 10/20: loss=3.45, lr=0.00030, time=2.0s\n  Step 20/20: loss=3.21, lr=0.00030, time=4.1s\n  Validation: loss=3.28, perplexity=26.7\n\n...\n\n\u2705 Training completed successfully!\n\ud83d\udcca Final Results:\n  \u2022 Training Loss: 2.45\n  \u2022 Validation Loss: 2.52\n  \u2022 Training Time: 8m 34s\n  \u2022 Model saved to: ./my_first_model/model.pt\n</code></pre>"},{"location":"getting-started/first-model.html#understanding-training-metrics","title":"Understanding Training Metrics","text":"<ul> <li>Loss: How well the model predicts the next token (lower is better)</li> <li>Perplexity: Model confidence (lower means more confident)</li> <li>Learning Rate: How fast the model learns (adjusted automatically)</li> </ul>"},{"location":"getting-started/first-model.html#step-5-generate-text","title":"\ud83c\udfaf Step 5: Generate Text","text":"<p>Time to test your model! Let's generate some text:</p> <pre><code>llmbuilder generate text \\\n  --model ./my_first_model/model.pt \\\n  --tokenizer ./tokenizer \\\n  --prompt \"Artificial intelligence\" \\\n  --max-tokens 100 \\\n  --temperature 0.8\n</code></pre> <p>You should see output like:</p> <pre><code>\ud83d\udcad Prompt: Artificial intelligence\n\ud83e\udd14 Generating...\n\n\ud83c\udfaf Generated Text:\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nArtificial intelligence is a rapidly evolving field that encompasses machine learning and deep learning technologies. These systems can process vast amounts of data to identify patterns and make predictions. Natural language processing enables computers to understand and generate human language, while computer vision allows machines to interpret visual information.\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ud83d\udcca Settings: temp=0.8, max_tokens=100\n</code></pre>"},{"location":"getting-started/first-model.html#interactive-generation","title":"Interactive Generation","text":"<p>For a more interactive experience:</p> <pre><code>llmbuilder generate text \\\n  --model ./my_first_model/model.pt \\\n  --tokenizer ./tokenizer \\\n  --interactive\n</code></pre> <p>This starts an interactive session where you can try different prompts:</p> <pre><code>\ud83c\udfae Interactive Text Generation\n\ud83d\udca1 Type your prompts and watch the AI respond!\n\n&gt; Prompt: Machine learning is\nGenerated: Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed...\n\n&gt; Prompt: The future of AI\nGenerated: The future of AI holds tremendous potential for transforming industries and solving complex global challenges...\n\n&gt; Prompt: /quit\nGoodbye! \ud83d\udc4b\n</code></pre>"},{"location":"getting-started/first-model.html#step-6-evaluate-your-model","title":"\ud83d\udcca Step 6: Evaluate Your Model","text":"<p>Let's assess how well your model performed:</p> <pre><code>llmbuilder model evaluate \\\n  ./my_first_model/model.pt \\\n  --dataset training_data.txt \\\n  --batch-size 16\n</code></pre> <p>This will output metrics like:</p> <pre><code>\ud83d\udcca Model Evaluation Results:\n  \u2022 Perplexity: 15.2 (lower is better)\n  \u2022 Loss: 2.72\n  \u2022 Tokens per second: 1,234\n  \u2022 Memory usage: 1.2GB\n  \u2022 Model size: 8.4MB\n</code></pre>"},{"location":"getting-started/first-model.html#model-information","title":"Model Information","text":"<p>Get detailed information about your model:</p> <pre><code>llmbuilder model info ./my_first_model/model.pt\n</code></pre> <pre><code>\u2705 Model loaded successfully\n  Total parameters: 2,123,456\n  Trainable parameters: 2,123,456\n  Architecture: 6 layers, 6 heads\n  Embedding dim: 384\n  Vocab size: 8,000\n  Max sequence length: 512\n</code></pre>"},{"location":"getting-started/first-model.html#step-7-experiment-and-improve","title":"\ud83d\ude80 Step 7: Experiment and Improve","text":"<p>Now that you have a working model, try these experiments:</p>"},{"location":"getting-started/first-model.html#experiment-1-different-generation-parameters","title":"Experiment 1: Different Generation Parameters","text":"<pre><code># experiment_generation.py\nimport llmbuilder as lb\n\nmodel_path = \"./my_first_model/model.pt\"\ntokenizer_path = \"./tokenizer\"\nprompt = \"The future of technology\"\n\n# Conservative generation (more predictable)\nconservative = lb.generate_text(\n    model_path, tokenizer_path, prompt,\n    temperature=0.3, top_k=10, max_new_tokens=50\n)\n\n# Creative generation (more diverse)\ncreative = lb.generate_text(\n    model_path, tokenizer_path, prompt,\n    temperature=1.2, top_k=100, max_new_tokens=50\n)\n\nprint(\"Conservative:\", conservative)\nprint(\"Creative:\", creative)\n</code></pre>"},{"location":"getting-started/first-model.html#experiment-2-fine-tuning","title":"Experiment 2: Fine-tuning","text":"<p>If you have domain-specific data, try fine-tuning:</p> <pre><code># Create domain-specific data\necho \"Your domain-specific text here...\" &gt; domain_data.txt\n\n# Fine-tune the model\nllmbuilder finetune model \\\n  --model ./my_first_model/model.pt \\\n  --dataset domain_data.txt \\\n  --output ./fine_tuned_model \\\n  --epochs 5 \\\n  --lr 1e-5\n</code></pre>"},{"location":"getting-started/first-model.html#experiment-3-export-for-production","title":"Experiment 3: Export for Production","text":"<p>Export your model for different deployment scenarios:</p> <pre><code># Export to GGUF for llama.cpp\nllmbuilder export gguf \\\n  ./my_first_model/model.pt \\\n  --output my_model.gguf \\\n  --quantization q4_0\n\n# Export to ONNX for mobile/edge\nllmbuilder export onnx \\\n  ./my_first_model/model.pt \\\n  --output my_model.onnx\n</code></pre>"},{"location":"getting-started/first-model.html#troubleshooting","title":"\ud83d\udd0d Troubleshooting","text":""},{"location":"getting-started/first-model.html#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"getting-started/first-model.html#1-out-of-memory-error","title":"1. Out of Memory Error","text":"<pre><code># Reduce batch size\nllmbuilder train model --config model_config.json --batch-size 2\n\n# Or use gradient accumulation\nllmbuilder train model --config model_config.json --gradient-accumulation-steps 4\n</code></pre>"},{"location":"getting-started/first-model.html#2-poor-generation-quality","title":"2. Poor Generation Quality","text":"<pre><code># Train for more epochs\nllmbuilder train model --config model_config.json --epochs 20\n\n# Use more training data\n# Add more text files to your dataset\n\n# Adjust model size\nllmbuilder config create --preset gpu_medium --output larger_config.json\n</code></pre>"},{"location":"getting-started/first-model.html#3-training-too-slow","title":"3. Training Too Slow","text":"<pre><code># Use GPU if available\nllmbuilder train model --config model_config.json --device cuda\n\n# Reduce sequence length\n# Edit model_config.json: \"max_seq_length\": 256\n</code></pre>"},{"location":"getting-started/first-model.html#understanding-your-results","title":"\ud83d\udcc8 Understanding Your Results","text":""},{"location":"getting-started/first-model.html#what-makes-a-good-model","title":"What Makes a Good Model?","text":"<ul> <li>Perplexity &lt; 20: Good for small datasets</li> <li>Coherent text: Generated text should make sense</li> <li>Diverse outputs: Different prompts should produce varied responses</li> <li>Fast inference: Generation should be reasonably quick</li> </ul>"},{"location":"getting-started/first-model.html#improving-your-model","title":"Improving Your Model","text":"<ol> <li>More data: The most important factor</li> <li>Longer training: More epochs often help</li> <li>Better data quality: Clean, relevant text</li> <li>Hyperparameter tuning: Adjust learning rate, batch size</li> <li>Model architecture: Try different sizes</li> </ol>"},{"location":"getting-started/first-model.html#congratulations","title":"\ud83c\udf89 Congratulations!","text":"<p>You've successfully:</p> <p>\u2705 Prepared training data \u2705 Configured a model \u2705 Trained a tokenizer \u2705 Trained a language model \u2705 Generated text \u2705 Evaluated performance  </p>"},{"location":"getting-started/first-model.html#next-steps","title":"\ud83d\ude80 Next Steps","text":"<p>Now that you have the basics down, explore:</p> <ul> <li>Advanced Training - Learn advanced techniques</li> <li>Fine-tuning Guide - Adapt models to specific domains</li> <li>Model Export - Deploy your models</li> <li>CLI Reference - Complete CLI documentation</li> </ul> <p>Keep Experimenting!</p> <p>The model you just trained is small and trained on limited data, but you've learned the complete workflow. Try training on larger datasets, experimenting with different architectures, and fine-tuning for specific tasks. The possibilities are endless!</p>"},{"location":"getting-started/installation.html","title":"Installation","text":"<p>This guide will help you install LLMBuilder and set up your environment for training and deploying language models.</p>"},{"location":"getting-started/installation.html#system-requirements","title":"System Requirements","text":""},{"location":"getting-started/installation.html#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>Python: 3.8 or higher</li> <li>RAM: 4GB (8GB+ recommended)</li> <li>Storage: 2GB free space</li> <li>OS: Windows, macOS, or Linux</li> </ul>"},{"location":"getting-started/installation.html#recommended-requirements","title":"Recommended Requirements","text":"<ul> <li>Python: 3.9 or higher</li> <li>RAM: 16GB or more</li> <li>GPU: NVIDIA GPU with 8GB+ VRAM (optional but recommended)</li> <li>Storage: 10GB+ free space for models and data</li> </ul>"},{"location":"getting-started/installation.html#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation.html#method-1-pypi-installation-recommended","title":"Method 1: PyPI Installation (Recommended)","text":"<p>The easiest way to install LLMBuilder is via PyPI:</p> <pre><code>pip install llmbuilder\n</code></pre>"},{"location":"getting-started/installation.html#method-2-development-installation","title":"Method 2: Development Installation","text":"<p>For the latest features or if you want to contribute:</p> <pre><code># Clone the repository\ngit clone https://github.com/Qubasehq/llmbuilder-package.git\ncd llmbuilder-package\n\n# Install in development mode\npip install -e .\n</code></pre>"},{"location":"getting-started/installation.html#method-3-cpu-only-installation","title":"Method 3: CPU-Only Installation","text":"<p>If you don't have a GPU or want to use CPU-only PyTorch:</p> <pre><code># Install CPU-only PyTorch first\npip install torch --index-url https://download.pytorch.org/whl/cpu\n\n# Then install LLMBuilder\npip install llmbuilder\n</code></pre>"},{"location":"getting-started/installation.html#verify-installation","title":"Verify Installation","text":"<p>Test your installation by running:</p> <pre><code># Check if LLMBuilder is installed\npython -c \"import llmbuilder; print(f'LLMBuilder {llmbuilder.__version__} installed successfully!')\"\n\n# Test the CLI\nllmbuilder --version\nllmbuilder info\n</code></pre> <p>You should see output similar to:</p> <pre><code>LLMBuilder 0.2.1 installed successfully!\n\ud83e\udd16 LLMBuilder version 0.2.1\nA comprehensive toolkit for building, training, and deploying language models.\n</code></pre>"},{"location":"getting-started/installation.html#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation.html#for-gpu-training","title":"For GPU Training","text":"<p>If you have an NVIDIA GPU and want to use CUDA:</p> <pre><code># Install CUDA-enabled PyTorch (adjust version as needed)\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n</code></pre>"},{"location":"getting-started/installation.html#for-advanced-data-processing","title":"For Advanced Data Processing","text":"<p>For processing various document formats:</p> <pre><code>pip install llmbuilder[data]\n</code></pre> <p>This includes: - <code>pandas</code> - For data manipulation - <code>pymupdf</code> - For PDF processing - <code>docx2txt</code> - For DOCX files - <code>python-pptx</code> - For PowerPoint files - <code>beautifulsoup4</code> - For HTML processing</p>"},{"location":"getting-started/installation.html#for-development","title":"For Development","text":"<p>If you're contributing to LLMBuilder:</p> <pre><code>pip install llmbuilder[dev]\n</code></pre> <p>This includes: - <code>pytest</code> - For running tests - <code>black</code> - For code formatting - <code>ruff</code> - For linting - <code>mypy</code> - For type checking</p>"},{"location":"getting-started/installation.html#for-model-export","title":"For Model Export","text":"<p>For exporting models to different formats:</p> <pre><code>pip install llmbuilder[export]\n</code></pre> <p>This includes: - <code>onnx</code> - For ONNX export - <code>onnxruntime</code> - For ONNX inference</p>"},{"location":"getting-started/installation.html#environment-setup","title":"Environment Setup","text":""},{"location":"getting-started/installation.html#virtual-environment-recommended","title":"Virtual Environment (Recommended)","text":"<p>Create a dedicated virtual environment for LLMBuilder:</p> Using venvUsing conda <pre><code># Create virtual environment\npython -m venv llmbuilder-env\n\n# Activate it\n# On Windows:\nllmbuilder-env\\Scripts\\activate\n# On macOS/Linux:\nsource llmbuilder-env/bin/activate\n\n# Install LLMBuilder\npip install llmbuilder\n</code></pre> <pre><code># Create conda environment\nconda create -n llmbuilder python=3.9\nconda activate llmbuilder\n\n# Install LLMBuilder\npip install llmbuilder\n</code></pre>"},{"location":"getting-started/installation.html#environment-variables","title":"Environment Variables","text":"<p>You can set these optional environment variables:</p> <pre><code># Enable slow tests (for development)\nexport RUN_SLOW=1\n\n# Enable performance tests (for development)\nexport RUN_PERF=1\n\n# Set default device\nexport LLMBUILDER_DEVICE=cuda  # or 'cpu'\n\n# Set cache directory\nexport LLMBUILDER_CACHE_DIR=/path/to/cache\n</code></pre>"},{"location":"getting-started/installation.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation.html#common-issues","title":"Common Issues","text":""},{"location":"getting-started/installation.html#importerror-no-module-named-torch","title":"ImportError: No module named 'torch'","text":"<p>Solution: Install PyTorch first: <pre><code>pip install torch\n</code></pre></p>"},{"location":"getting-started/installation.html#cuda-out-of-memory","title":"CUDA out of memory","text":"<p>Solution: Use CPU-only installation or reduce batch size: <pre><code>pip install torch --index-url https://download.pytorch.org/whl/cpu\n</code></pre></p>"},{"location":"getting-started/installation.html#permission-denied-errors","title":"Permission denied errors","text":"<p>Solution: Use <code>--user</code> flag or virtual environment: <pre><code>pip install --user llmbuilder\n</code></pre></p>"},{"location":"getting-started/installation.html#package-conflicts","title":"Package conflicts","text":"<p>Solution: Create a fresh virtual environment: <pre><code>python -m venv fresh-env\nsource fresh-env/bin/activate  # or fresh-env\\Scripts\\activate on Windows\npip install llmbuilder\n</code></pre></p>"},{"location":"getting-started/installation.html#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the logs: LLMBuilder provides detailed error messages</li> <li>Search existing issues: GitHub Issues</li> <li>Create a new issue: Include your system info and error messages</li> <li>Join discussions: GitHub Discussions</li> </ol>"},{"location":"getting-started/installation.html#system-information","title":"System Information","text":"<p>To help with troubleshooting, you can gather system information:</p> <pre><code>import llmbuilder\nimport torch\nimport sys\nimport platform\n\nprint(f\"LLMBuilder version: {llmbuilder.__version__}\")\nprint(f\"Python version: {sys.version}\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"GPU count: {torch.cuda.device_count()}\")\nprint(f\"Platform: {platform.platform()}\")\n</code></pre>"},{"location":"getting-started/installation.html#next-steps","title":"Next Steps","text":"<p>Once you have LLMBuilder installed:</p> <ol> <li>Quick Start - Get up and running in 5 minutes</li> <li>First Model - Train your first language model</li> <li>User Guide - Learn about all features</li> </ol> <p>Pro Tip</p> <p>For the best experience, we recommend using a virtual environment and installing the GPU version of PyTorch if you have a compatible NVIDIA GPU.</p>"},{"location":"getting-started/quickstart.html","title":"Quick Start","text":"<p>Get up and running with LLMBuilder in just 5 minutes! This guide will walk you through training your first language model.</p>"},{"location":"getting-started/quickstart.html#5-minute-setup","title":"\ud83d\ude80 5-Minute Setup","text":""},{"location":"getting-started/quickstart.html#step-1-install-llmbuilder","title":"Step 1: Install LLMBuilder","text":"<pre><code>pip install llmbuilder\n</code></pre>"},{"location":"getting-started/quickstart.html#step-2-prepare-your-data","title":"Step 2: Prepare Your Data","text":"<p>Create a simple text file with some training data:</p> <pre><code># Create a sample data file\necho \"Artificial intelligence is transforming the world. Machine learning enables computers to learn from data. Deep learning uses neural networks to solve complex problems.\" &gt; sample_data.txt\n</code></pre>"},{"location":"getting-started/quickstart.html#step-3-train-your-first-model","title":"Step 3: Train Your First Model","text":"<p>Use the interactive CLI to train a model:</p> <pre><code>llmbuilder welcome\n</code></pre> <p>Or use the direct command:</p> <pre><code># Train a small model (perfect for testing)\nllmbuilder train model \\\n  --data sample_data.txt \\\n  --tokenizer ./tokenizer \\\n  --output ./my_first_model \\\n  --epochs 5 \\\n  --batch-size 2\n</code></pre>"},{"location":"getting-started/quickstart.html#step-4-generate-text","title":"Step 4: Generate Text","text":"<pre><code># Generate text with your trained model\nllmbuilder generate text \\\n  --model ./my_first_model/model.pt \\\n  --tokenizer ./tokenizer \\\n  --prompt \"Artificial intelligence\" \\\n  --max-tokens 50\n</code></pre> <p>\ud83c\udf89 Congratulations! You've just trained and used your first language model with LLMBuilder!</p>"},{"location":"getting-started/quickstart.html#python-api-quick-start","title":"\ud83d\udc0d Python API Quick Start","text":"<p>Prefer Python code? Here's the same workflow using the Python API:</p> <pre><code>import llmbuilder as lb\n\n# 1. Load configuration\ncfg = lb.load_config(preset=\"cpu_small\")\n\n# 2. Build model\nmodel = lb.build_model(cfg.model)\n\n# 3. Prepare data\nfrom llmbuilder.data import TextDataset\ndataset = TextDataset(\"sample_data.txt\", block_size=cfg.model.max_seq_length)\n\n# 4. Train model\nresults = lb.train_model(model, dataset, cfg.training)\n\n# 5. Generate text\ntext = lb.generate_text(\n    model_path=\"./checkpoints/model.pt\",\n    tokenizer_path=\"./tokenizers\",\n    prompt=\"Artificial intelligence\",\n    max_new_tokens=50\n)\nprint(text)\n</code></pre>"},{"location":"getting-started/quickstart.html#understanding-the-output","title":"\ud83d\udcca Understanding the Output","text":"<p>When training completes, you'll see output like:</p> <pre><code>\u2705 Training completed successfully!\n\ud83d\udcca Final Results:\n  \u2022 Training Loss: 2.45\n  \u2022 Validation Loss: 2.52\n  \u2022 Training Time: 3m 24s\n  \u2022 Model Parameters: 2.1M\n  \u2022 Model Size: 8.4MB\n\n\ud83d\udcbe Outputs saved to:\n  \u2022 Model: ./my_first_model/model.pt\n  \u2022 Tokenizer: ./tokenizer/\n  \u2022 Logs: ./my_first_model/training.log\n</code></pre>"},{"location":"getting-started/quickstart.html#what-just-happened","title":"\ud83c\udfaf What Just Happened?","text":"<p>Let's break down what LLMBuilder did:</p> <ol> <li>Data Processing: Loaded and cleaned your text data</li> <li>Tokenization: Created a vocabulary and tokenized the text</li> <li>Model Creation: Built a GPT-style transformer model</li> <li>Training: Trained the model to predict the next token</li> <li>Saving: Saved the model and tokenizer for later use</li> </ol>"},{"location":"getting-started/quickstart.html#customization-options","title":"\ud83d\udd27 Customization Options","text":""},{"location":"getting-started/quickstart.html#different-model-sizes","title":"Different Model Sizes","text":"<pre><code># Tiny model (fastest, least memory)\nllmbuilder train model --config-preset tiny --data data.txt --output model/\n\n# Small model (balanced)\nllmbuilder train model --config-preset cpu_small --data data.txt --output model/\n\n# Medium model (better quality, needs more resources)\nllmbuilder train model --config-preset gpu_medium --data data.txt --output model/\n</code></pre>"},{"location":"getting-started/quickstart.html#different-data-formats","title":"Different Data Formats","text":"<p>LLMBuilder supports multiple input formats:</p> <pre><code># Process PDF files\nllmbuilder data load --input documents/ --output clean_text.txt --format pdf\n\n# Process DOCX files\nllmbuilder data load --input documents/ --output clean_text.txt --format docx\n\n# Process all supported formats\nllmbuilder data load --input documents/ --output clean_text.txt --format all\n</code></pre>"},{"location":"getting-started/quickstart.html#interactive-mode","title":"Interactive Mode","text":"<p>For a guided experience:</p> <pre><code># Interactive training setup\nllmbuilder train model --interactive\n\n# Interactive text generation\nllmbuilder generate text --setup\n</code></pre>"},{"location":"getting-started/quickstart.html#next-steps","title":"\ud83d\udcc8 Next Steps","text":"<p>Now that you have a working model, here are some things to try:</p>"},{"location":"getting-started/quickstart.html#1-improve-your-model","title":"1. Improve Your Model","text":"<pre><code># Train for more epochs\nllmbuilder train model --data data.txt --output model/ --epochs 20\n\n# Use a larger model\nllmbuilder train model --data data.txt --output model/ --layers 12 --dim 768\n\n# Add more training data\nllmbuilder data load --input more_documents/ --output bigger_dataset.txt\n</code></pre>"},{"location":"getting-started/quickstart.html#2-fine-tune-on-specific-data","title":"2. Fine-tune on Specific Data","text":"<pre><code># Fine-tune your model on domain-specific data\nllmbuilder finetune model \\\n  --model ./my_first_model/model.pt \\\n  --dataset domain_specific_data.txt \\\n  --output ./fine_tuned_model\n</code></pre>"},{"location":"getting-started/quickstart.html#3-export-for-production","title":"3. Export for Production","text":"<pre><code># Export to GGUF format for llama.cpp\nllmbuilder export gguf ./my_first_model/model.pt --output model.gguf\n\n# Export to ONNX for mobile/edge deployment\nllmbuilder export onnx ./my_first_model/model.pt --output model.onnx\n</code></pre>"},{"location":"getting-started/quickstart.html#4-advanced-generation","title":"4. Advanced Generation","text":"<pre><code>import llmbuilder as lb\n\n# Interactive chat-like generation\nlb.interactive_cli(\n    model_path=\"./my_first_model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    temperature=0.8,\n    top_k=50\n)\n</code></pre>"},{"location":"getting-started/quickstart.html#configuration-presets","title":"\ud83d\udee0\ufe0f Configuration Presets","text":"<p>LLMBuilder comes with several built-in presets:</p> Preset Use Case Memory Training Time <code>tiny</code> Testing, debugging ~1GB Minutes <code>cpu_small</code> CPU training, learning ~2GB Hours <code>gpu_medium</code> Single GPU training ~8GB Hours <code>gpu_large</code> High-end GPU training ~16GB+ Days"},{"location":"getting-started/quickstart.html#monitoring-training","title":"\ud83d\udd0d Monitoring Training","text":""},{"location":"getting-started/quickstart.html#real-time-monitoring","title":"Real-time Monitoring","text":"<pre><code># Monitor training progress\ntail -f ./my_first_model/training.log\n\n# Or use the built-in progress display\nllmbuilder train model --data data.txt --output model/ --verbose\n</code></pre>"},{"location":"getting-started/quickstart.html#training-metrics","title":"Training Metrics","text":"<p>LLMBuilder tracks important metrics:</p> <ul> <li>Loss: How well the model is learning</li> <li>Perplexity: Model confidence (lower is better)</li> <li>Learning Rate: Training speed</li> <li>Memory Usage: Resource consumption</li> </ul>"},{"location":"getting-started/quickstart.html#common-issues-solutions","title":"\ud83d\udea8 Common Issues &amp; Solutions","text":""},{"location":"getting-started/quickstart.html#out-of-memory","title":"Out of Memory","text":"<pre><code># Reduce batch size\nllmbuilder train model --data data.txt --output model/ --batch-size 1\n\n# Use CPU-only mode\nllmbuilder train model --data data.txt --output model/ --device cpu\n</code></pre>"},{"location":"getting-started/quickstart.html#slow-training","title":"Slow Training","text":"<pre><code># Use GPU if available\nllmbuilder train model --data data.txt --output model/ --device cuda\n\n# Reduce model size\nllmbuilder train model --data data.txt --output model/ --layers 4 --dim 256\n</code></pre>"},{"location":"getting-started/quickstart.html#poor-generation-quality","title":"Poor Generation Quality","text":"<pre><code># Train for more epochs\nllmbuilder train model --data data.txt --output model/ --epochs 50\n\n# Use more training data\n# Add more text files to your dataset\n\n# Adjust generation parameters\nllmbuilder generate text --model model.pt --tokenizer tokenizer/ \\\n  --prompt \"Your prompt\" --temperature 0.7 --top-k 40\n</code></pre>"},{"location":"getting-started/quickstart.html#learn-more","title":"\ud83d\udcda Learn More","text":"<p>Ready to dive deeper? Check out these resources:</p> <ul> <li>First Model Tutorial - Detailed step-by-step guide</li> <li>Configuration Guide - Customize your setup</li> <li>Training Guide - Advanced training techniques</li> <li>Model Export - Deploy your models</li> <li>CLI Reference - Complete CLI documentation</li> </ul> <p>You're Ready!</p> <p>You now have a working LLMBuilder setup! The model you just trained might be small, but you've learned the complete workflow. Try experimenting with different data, model sizes, and generation parameters to see what works best for your use case.</p>"},{"location":"reference/faq.html","title":"Frequently Asked Questions","text":"<p>Common questions and answers about LLMBuilder usage, troubleshooting, and best practices.</p>"},{"location":"reference/faq.html#getting-started","title":"\ud83d\ude80 Getting Started","text":""},{"location":"reference/faq.html#q-what-are-the-minimum-system-requirements","title":"Q: What are the minimum system requirements?","text":"<p>A: LLMBuilder requires: - Python 3.8+ (3.9+ recommended) - 4GB RAM minimum (8GB+ recommended) - 2GB free disk space for installation and basic models - Optional: NVIDIA GPU with 4GB+ VRAM for faster training</p>"},{"location":"reference/faq.html#q-should-i-use-cpu-or-gpu-for-training","title":"Q: Should I use CPU or GPU for training?","text":"<p>A:  - CPU: Good for learning, small models, and development. Use <code>preset=\"cpu_small\"</code> - GPU: Recommended for production training and larger models. Use <code>preset=\"gpu_medium\"</code> or <code>preset=\"gpu_large\"</code> - Mixed: Start with CPU for prototyping, then move to GPU for final training</p>"},{"location":"reference/faq.html#q-how-long-does-it-take-to-train-a-model","title":"Q: How long does it take to train a model?","text":"<p>A: Training time depends on several factors: - Small model (10M params): 30 minutes - 2 hours on CPU, 5-15 minutes on GPU - Medium model (50M params): 2-8 hours on CPU, 30 minutes - 2 hours on GPU - Large model (200M+ params): Days on CPU, 2-12 hours on GPU</p>"},{"location":"reference/faq.html#configuration","title":"\ud83d\udd27 Configuration","text":""},{"location":"reference/faq.html#q-which-configuration-preset-should-i-use","title":"Q: Which configuration preset should I use?","text":"<p>A: Choose based on your hardware and use case:</p> Preset Use Case Hardware Model Size Training Time <code>tiny</code> Testing, debugging Any ~1M params Minutes <code>cpu_small</code> Learning, development CPU ~10M params Hours <code>gpu_medium</code> Production training Single GPU ~50M params Hours <code>gpu_large</code> High-quality models High-end GPU ~200M+ params Days"},{"location":"reference/faq.html#q-how-do-i-customize-model-architecture","title":"Q: How do I customize model architecture?","text":"<p>A: Modify the model configuration:</p> <pre><code>from llmbuilder.config import ModelConfig\n\nconfig = ModelConfig(\n    vocab_size=16000,      # Match your tokenizer\n    num_layers=12,         # More layers = more capacity\n    num_heads=12,          # Should divide embedding_dim evenly\n    embedding_dim=768,     # Larger = more capacity\n    max_seq_length=1024,   # Longer sequences = more memory\n    dropout=0.1            # Higher = more regularization\n)\n</code></pre>"},{"location":"reference/faq.html#q-what-vocabulary-size-should-i-use","title":"Q: What vocabulary size should I use?","text":"<p>A: Vocabulary size depends on your data and use case: - 8K-16K: Small datasets, specific domains - 16K-32K: General purpose, balanced size - 32K-64K: Large datasets, multilingual models - 64K+: Very large datasets, maximum coverage</p>"},{"location":"reference/faq.html#data-and-training","title":"\ud83d\udcca Data and Training","text":""},{"location":"reference/faq.html#q-how-much-training-data-do-i-need","title":"Q: How much training data do I need?","text":"<p>A: Data requirements vary by model size and quality goals: - Minimum: 1MB of text (~200K words) for basic functionality - Recommended: 10MB+ of text (~2M words) for good quality - Optimal: 100MB+ of text (~20M words) for high quality - Production: 1GB+ of text (~200M words) for best results</p>"},{"location":"reference/faq.html#q-what-file-formats-are-supported-for-training-data","title":"Q: What file formats are supported for training data?","text":"<p>A: LLMBuilder supports: - Text files: <code>.txt</code>, <code>.md</code> (best quality) - Documents: <code>.pdf</code>, <code>.docx</code> (good quality) - Web content: <code>.html</code>, <code>.htm</code> (moderate quality) - Presentations: <code>.pptx</code> (basic support) - Data files: <code>.csv</code>, <code>.json</code> (with proper formatting)</p>"},{"location":"reference/faq.html#q-how-do-i-handle-out-of-memory-errors","title":"Q: How do I handle out-of-memory errors?","text":"<p>A: Try these solutions in order:</p> <ol> <li> <p>Reduce batch size: <pre><code>config.training.batch_size = 4  # or even 2\n</code></pre></p> </li> <li> <p>Enable gradient checkpointing: <pre><code>config.model.gradient_checkpointing = True\n</code></pre></p> </li> <li> <p>Use gradient accumulation: <pre><code>config.training.gradient_accumulation_steps = 4\n</code></pre></p> </li> <li> <p>Reduce sequence length: <pre><code>config.model.max_seq_length = 512\n</code></pre></p> </li> <li> <p>Use CPU training: <pre><code>config.system.device = \"cpu\"\n</code></pre></p> </li> </ol>"},{"location":"reference/faq.html#q-my-model-isnt-learning-loss-not-decreasing-whats-wrong","title":"Q: My model isn't learning (loss not decreasing). What's wrong?","text":"<p>A: Common causes and solutions:</p> <ol> <li>Learning rate too high: Reduce to 1e-4 or 1e-5</li> <li>Learning rate too low: Increase to 3e-4 or 5e-4</li> <li>Bad data: Check for corrupted or repetitive text</li> <li>Wrong tokenizer: Ensure vocab_size matches tokenizer</li> <li>Insufficient warmup: Increase warmup_steps to 1000+</li> </ol>"},{"location":"reference/faq.html#q-how-do-i-know-if-my-model-is-overfitting","title":"Q: How do I know if my model is overfitting?","text":"<p>A: Signs of overfitting: - Training loss decreases but validation loss increases - Generated text is repetitive or memorized - Model performs poorly on new data</p> <p>Solutions: - Increase dropout rate (0.1 \u2192 0.2) - Add weight decay (0.01) - Use early stopping - Get more training data - Reduce model size</p>"},{"location":"reference/faq.html#text-generation","title":"\ud83c\udfaf Text Generation","text":""},{"location":"reference/faq.html#q-how-do-i-improve-generation-quality","title":"Q: How do I improve generation quality?","text":"<p>A: Try these techniques:</p> <ol> <li>Adjust temperature:</li> <li>Lower (0.3-0.7): More focused, predictable</li> <li> <p>Higher (0.8-1.2): More creative, diverse</p> </li> <li> <p>Use nucleus sampling: <pre><code>config = GenerationConfig(\n    temperature=0.8,\n    top_p=0.9,      # Nucleus sampling\n    top_k=50        # Top-k sampling\n)\n</code></pre></p> </li> <li> <p>Add repetition penalty: <pre><code>config.repetition_penalty = 1.1\n</code></pre></p> </li> <li> <p>Better prompts:</p> </li> <li>Be specific and clear</li> <li>Provide context and examples</li> <li>Use consistent formatting</li> </ol>"},{"location":"reference/faq.html#q-why-is-my-generated-text-repetitive","title":"Q: Why is my generated text repetitive?","text":"<p>A: Common causes and fixes:</p> <ol> <li>Insufficient training: Train for more epochs</li> <li>Poor sampling: Use top-p/top-k sampling instead of greedy</li> <li>Low temperature: Increase temperature to 0.8+</li> <li>Add repetition penalty: Set to 1.1-1.3</li> <li>Prevent n-gram repetition: Set <code>no_repeat_ngram_size=3</code></li> </ol>"},{"location":"reference/faq.html#q-how-do-i-make-generation-faster","title":"Q: How do I make generation faster?","text":"<p>A: Speed optimization techniques:</p> <ol> <li>Use GPU: Much faster than CPU</li> <li>Reduce max_tokens: Generate shorter responses</li> <li>Use greedy decoding: Set <code>do_sample=False</code></li> <li>Enable model compilation: Set <code>compile=True</code> (PyTorch 2.0+)</li> <li>Quantize model: Use 8-bit or 16-bit precision</li> </ol>"},{"location":"reference/faq.html#fine-tuning","title":"\ud83d\udd04 Fine-tuning","text":""},{"location":"reference/faq.html#q-when-should-i-fine-tune-vs-train-from-scratch","title":"Q: When should I fine-tune vs. train from scratch?","text":"<p>A:  - Fine-tune when: You have a pre-trained model and domain-specific data - Train from scratch when: You have lots of data and need full control - Fine-tuning advantages: Faster, less data needed, preserves general knowledge - Training advantages: Full customization, no dependency on base model</p>"},{"location":"reference/faq.html#q-whats-the-difference-between-lora-and-full-fine-tuning","title":"Q: What's the difference between LoRA and full fine-tuning?","text":"<p>A:</p> Aspect LoRA Full Fine-tuning Memory Low (~1% of params) High (all params) Speed Fast Slower Quality Good for most tasks Best possible Flexibility Limited adaptation Full adaptation Use case Domain adaptation Major architecture changes"},{"location":"reference/faq.html#q-how-do-i-prevent-catastrophic-forgetting-during-fine-tuning","title":"Q: How do I prevent catastrophic forgetting during fine-tuning?","text":"<p>A: Use these techniques:</p> <ol> <li>Lower learning rate: 1e-5 to 5e-5</li> <li>Fewer epochs: 3-5 epochs usually sufficient</li> <li>Regularization: Add weight decay (0.01)</li> <li>LoRA: Preserves base model weights</li> <li>Mixed training: Include general data with domain data</li> </ol>"},{"location":"reference/faq.html#deployment","title":"\ud83d\ude80 Deployment","text":""},{"location":"reference/faq.html#q-how-do-i-deploy-my-trained-model","title":"Q: How do I deploy my trained model?","text":"<p>A: LLMBuilder supports multiple deployment options:</p> <ol> <li> <p>GGUF format (for llama.cpp): <pre><code>llmbuilder export gguf model.pt --output model.gguf --quantization q4_0\n</code></pre></p> </li> <li> <p>ONNX format (for cross-platform): <pre><code>llmbuilder export onnx model.pt --output model.onnx\n</code></pre></p> </li> <li> <p>Quantized PyTorch (for production): <pre><code>llmbuilder export quantize model.pt --output model_int8.pt --bits 8\n</code></pre></p> </li> </ol>"},{"location":"reference/faq.html#q-which-export-format-should-i-choose","title":"Q: Which export format should I choose?","text":"<p>A: Choose based on your deployment target:</p> <ul> <li>GGUF: CPU inference, llama.cpp compatibility, edge devices</li> <li>ONNX: Cross-platform, mobile apps, cloud services</li> <li>Quantized PyTorch: PyTorch ecosystem, balanced performance</li> <li>HuggingFace: Easy sharing, transformers compatibility</li> </ul>"},{"location":"reference/faq.html#q-how-do-i-reduce-model-size-for-deployment","title":"Q: How do I reduce model size for deployment?","text":"<p>A: Size reduction techniques:</p> <ol> <li>Quantization: 8-bit (50% smaller) or 4-bit (75% smaller)</li> <li>Pruning: Remove least important weights</li> <li>Distillation: Train smaller model to mimic larger one</li> <li>Architecture optimization: Use efficient attention mechanisms</li> </ol>"},{"location":"reference/faq.html#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"reference/faq.html#q-i-get-cuda-out-of-memory-errors-what-should-i-do","title":"Q: I get \"CUDA out of memory\" errors. What should I do?","text":"<p>A: Try these solutions:</p> <ol> <li>Reduce batch size: Start with batch_size=1</li> <li>Enable gradient checkpointing: Trades compute for memory</li> <li>Use gradient accumulation: Simulate larger batches</li> <li>Reduce sequence length: Shorter sequences use less memory</li> <li>Use CPU: Slower but no memory limits</li> <li>Clear GPU cache: <code>torch.cuda.empty_cache()</code></li> </ol>"},{"location":"reference/faq.html#q-training-is-very-slow-how-can-i-speed-it-up","title":"Q: Training is very slow. How can I speed it up?","text":"<p>A: Speed optimization:</p> <ol> <li>Use GPU: 10-100x faster than CPU</li> <li>Increase batch size: Better GPU utilization</li> <li>Enable mixed precision: <code>fp16</code> or <code>bf16</code></li> <li>Use multiple GPUs: Distributed training</li> <li>Optimize data loading: More workers, pin memory</li> <li>Compile model: PyTorch 2.0 compilation</li> </ol>"},{"location":"reference/faq.html#q-my-tokenizer-produces-weird-results-whats-wrong","title":"Q: My tokenizer produces weird results. What's wrong?","text":"<p>A: Common tokenizer issues:</p> <ol> <li>Wrong vocabulary size: Must match model config</li> <li>Insufficient training data: Need diverse text corpus</li> <li>Character coverage too low: Increase to 0.9999</li> <li>Wrong model type: BPE usually works best</li> <li>Missing special tokens: Include <code>&lt;pad&gt;</code>, <code>&lt;unk&gt;</code>, etc.</li> </ol>"},{"location":"reference/faq.html#q-generated-text-contains-strange-characters-or-formatting","title":"Q: Generated text contains strange characters or formatting.","text":"<p>A: Text cleaning solutions:</p> <ol> <li>Improve data cleaning: Remove unwanted characters</li> <li>Filter by language: Keep only desired languages</li> <li>Normalize text: Fix encoding issues</li> <li>Add text filters: Remove specific patterns</li> <li>Better tokenizer training: Use cleaner training data</li> </ol>"},{"location":"reference/faq.html#best-practices","title":"\ud83d\udca1 Best Practices","text":""},{"location":"reference/faq.html#q-what-are-the-most-important-best-practices","title":"Q: What are the most important best practices?","text":"<p>A: Key recommendations:</p> <ol> <li>Start small: Begin with tiny models and scale up</li> <li>Clean your data: Quality over quantity</li> <li>Monitor training: Watch loss curves and generation quality</li> <li>Save checkpoints: Protect against failures</li> <li>Validate everything: Test configurations before long training</li> <li>Document experiments: Keep track of what works</li> </ol>"},{"location":"reference/faq.html#q-how-do-i-choose-hyperparameters","title":"Q: How do I choose hyperparameters?","text":"<p>A: Hyperparameter selection guide:</p> <ol> <li>Learning rate: Start with 3e-4, adjust based on loss curves</li> <li>Batch size: Largest that fits in memory</li> <li>Model size: Balance quality needs with resources</li> <li>Sequence length: Match your use case requirements</li> <li>Dropout: 0.1 is usually good, increase if overfitting</li> </ol>"},{"location":"reference/faq.html#q-how-do-i-evaluate-model-quality","title":"Q: How do I evaluate model quality?","text":"<p>A: Evaluation methods:</p> <ol> <li>Perplexity: Lower is better (&lt; 20 is good)</li> <li>Generation quality: Manual inspection of outputs</li> <li>Task-specific metrics: BLEU, ROUGE for specific tasks</li> <li>Human evaluation: Best but most expensive</li> <li>Automated metrics: Coherence, fluency scores</li> </ol>"},{"location":"reference/faq.html#getting-help","title":"\ud83c\udd98 Getting Help","text":""},{"location":"reference/faq.html#q-where-can-i-get-help-if-im-stuck","title":"Q: Where can I get help if I'm stuck?","text":"<p>A: Support resources:</p> <ol> <li>Documentation: Complete guides and examples</li> <li>GitHub Issues: Report bugs and request features</li> <li>GitHub Discussions: Community Q&amp;A</li> <li>Examples: Working code samples</li> <li>Stack Overflow: Tag questions with <code>llmbuilder</code></li> </ol>"},{"location":"reference/faq.html#q-how-do-i-report-a-bug","title":"Q: How do I report a bug?","text":"<p>A: When reporting bugs, include:</p> <ol> <li>LLMBuilder version: <code>llmbuilder --version</code></li> <li>Python version: <code>python --version</code></li> <li>Operating system: Windows/macOS/Linux</li> <li>Hardware: CPU/GPU specifications</li> <li>Error message: Full traceback</li> <li>Minimal example: Code to reproduce the issue</li> <li>Configuration: Model and training configs used</li> </ol>"},{"location":"reference/faq.html#q-how-can-i-contribute-to-llmbuilder","title":"Q: How can I contribute to LLMBuilder?","text":"<p>A: Ways to contribute:</p> <ol> <li>Report bugs: Help improve stability</li> <li>Request features: Suggest improvements</li> <li>Submit PRs: Code contributions welcome</li> <li>Improve docs: Fix typos, add examples</li> <li>Share examples: Help other users</li> <li>Test releases: Try beta versions</li> </ol> <p>Still have questions?</p> <p>If you can't find the answer here, check our GitHub Discussions or create a new issue. The community is always happy to help!</p>"},{"location":"reference/migration.html","title":"Migration Guide","text":"<p>This guide helps you migrate from legacy LLMBuilder scripts and configurations to the new package structure. Whether you're upgrading from older versions or transitioning from standalone scripts, this guide covers all the changes you need to know.</p>"},{"location":"reference/migration.html#overview","title":"\ud83c\udfaf Overview","text":"<p>The LLMBuilder package represents a major evolution from the original standalone scripts. Key improvements include:</p> <ul> <li>Unified API: Single import for all functionality</li> <li>Better Configuration: Structured, validated configurations</li> <li>CLI Interface: Comprehensive command-line tools</li> <li>Modular Design: Use only what you need</li> <li>Better Documentation: Complete guides and examples</li> </ul>"},{"location":"reference/migration.html#migration-checklist","title":"\ud83d\udccb Migration Checklist","text":"<ul> <li>[ ] Update Python version to 3.8+</li> <li>[ ] Install new LLMBuilder package</li> <li>[ ] Convert configuration files</li> <li>[ ] Update import statements</li> <li>[ ] Migrate training scripts</li> <li>[ ] Update CLI commands</li> <li>[ ] Test functionality</li> <li>[ ] Update deployment scripts</li> </ul>"},{"location":"reference/migration.html#configuration-migration","title":"\ud83d\udd04 Configuration Migration","text":""},{"location":"reference/migration.html#legacy-configuration-format","title":"Legacy Configuration Format","text":"<p>Old format (config.json): <pre><code>{\n  \"n_layer\": 12,\n  \"n_head\": 12,\n  \"n_embd\": 768,\n  \"block_size\": 1024,\n  \"dropout\": 0.1,\n  \"bias\": true,\n  \"vocab_size\": 16000,\n  \"device\": \"cuda\",\n  \"batch_size\": 16,\n  \"learning_rate\": 3e-4,\n  \"max_iters\": 10000,\n  \"eval_interval\": 500,\n  \"eval_iters\": 100\n}\n</code></pre></p>"},{"location":"reference/migration.html#new-configuration-format","title":"New Configuration Format","text":"<p>New format (structured): <pre><code>{\n  \"model\": {\n    \"vocab_size\": 16000,\n    \"num_layers\": 12,\n    \"num_heads\": 12,\n    \"embedding_dim\": 768,\n    \"max_seq_length\": 1024,\n    \"dropout\": 0.1,\n    \"bias\": true,\n    \"model_type\": \"gpt\"\n  },\n  \"training\": {\n    \"batch_size\": 16,\n    \"learning_rate\": 3e-4,\n    \"num_epochs\": 10,\n    \"warmup_steps\": 1000,\n    \"weight_decay\": 0.01,\n    \"max_grad_norm\": 1.0,\n    \"save_every\": 1000,\n    \"eval_every\": 500,\n    \"log_every\": 100\n  },\n  \"system\": {\n    \"device\": \"cuda\",\n    \"seed\": 42,\n    \"deterministic\": false\n  }\n}\n</code></pre></p>"},{"location":"reference/migration.html#automatic-migration","title":"Automatic Migration","text":"<p>LLMBuilder provides automatic migration for legacy configurations:</p> <pre><code>import llmbuilder as lb\n\n# Load legacy configuration (automatically migrated)\nconfig = lb.load_config(\"legacy_config.json\")\n\n# Or migrate explicitly\nfrom llmbuilder.config import migrate_legacy_config\nnew_config = migrate_legacy_config(\"legacy_config.json\")\nnew_config.save(\"new_config.json\")\n</code></pre>"},{"location":"reference/migration.html#configuration-key-mapping","title":"Configuration Key Mapping","text":"Legacy Key New Key Notes <code>n_layer</code> <code>model.num_layers</code> Number of transformer layers <code>n_head</code> <code>model.num_heads</code> Number of attention heads <code>n_embd</code> <code>model.embedding_dim</code> Embedding dimension <code>block_size</code> <code>model.max_seq_length</code> Maximum sequence length <code>dropout</code> <code>model.dropout</code> Dropout rate <code>bias</code> <code>model.bias</code> Use bias in linear layers <code>vocab_size</code> <code>model.vocab_size</code> Vocabulary size <code>device</code> <code>system.device</code> Training device <code>batch_size</code> <code>training.batch_size</code> Batch size <code>learning_rate</code> <code>training.learning_rate</code> Learning rate <code>max_iters</code> <code>training.max_steps</code> Maximum training steps <code>eval_interval</code> <code>training.eval_every</code> Evaluation frequency"},{"location":"reference/migration.html#code-migration","title":"\ud83d\udc0d Code Migration","text":""},{"location":"reference/migration.html#legacy-import-pattern","title":"Legacy Import Pattern","text":"<p>Old way: <pre><code># Legacy imports\nfrom model import GPTConfig, GPT\nfrom train import train\nfrom sample import sample\nimport tiktoken\n\n# Legacy usage\nconfig = GPTConfig(\n    n_layer=12,\n    n_head=12,\n    n_embd=768,\n    block_size=1024,\n    vocab_size=50257\n)\n\nmodel = GPT(config)\n</code></pre></p>"},{"location":"reference/migration.html#new-import-pattern","title":"New Import Pattern","text":"<p>New way: <pre><code># New unified import\nimport llmbuilder as lb\n\n# New usage\nconfig = lb.load_config(preset=\"gpu_medium\")\nmodel = lb.build_model(config.model)\n</code></pre></p>"},{"location":"reference/migration.html#training-script-migration","title":"Training Script Migration","text":"<p>Legacy training script: <pre><code># train.py (legacy)\nimport torch\nfrom model import GPTConfig, GPT\nfrom dataset import get_batch\n\n# Configuration\nconfig = GPTConfig(...)\nmodel = GPT(config)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\n# Training loop\nfor iter in range(max_iters):\n    X, Y = get_batch('train')\n    logits, loss = model(X, Y)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if iter % eval_interval == 0:\n        evaluate()\n</code></pre></p> <p>New training script: <pre><code># train.py (new)\nimport llmbuilder as lb\nfrom llmbuilder.data import TextDataset\n\n# Configuration\nconfig = lb.load_config(preset=\"gpu_medium\")\nmodel = lb.build_model(config.model)\n\n# Dataset\ndataset = TextDataset(\"data.txt\", block_size=config.model.max_seq_length)\n\n# Training (handled automatically)\nresults = lb.train_model(model, dataset, config.training)\n</code></pre></p>"},{"location":"reference/migration.html#generation-script-migration","title":"Generation Script Migration","text":"<p>Legacy generation: <pre><code># sample.py (legacy)\nimport torch\nfrom model import GPT\nimport tiktoken\n\n# Load model\nmodel = GPT.from_pretrained('model.pt')\nenc = tiktoken.get_encoding(\"gpt2\")\n\n# Generate\ncontext = enc.encode(\"Hello\")\ngenerated = model.generate(context, max_new_tokens=100)\ntext = enc.decode(generated)\n</code></pre></p> <p>New generation: <pre><code># generate.py (new)\nimport llmbuilder as lb\n\n# Generate (much simpler)\ntext = lb.generate_text(\n    model_path=\"model.pt\",\n    tokenizer_path=\"tokenizer/\",\n    prompt=\"Hello\",\n    max_new_tokens=100\n)\n</code></pre></p>"},{"location":"reference/migration.html#cli-migration","title":"\ud83d\udda5\ufe0f CLI Migration","text":""},{"location":"reference/migration.html#legacy-cli-commands","title":"Legacy CLI Commands","text":"<p>Old commands: <pre><code># Legacy training\npython train.py --config config.json --data data.txt\n\n# Legacy generation\npython sample.py --model model.pt --prompt \"Hello\" --num_samples 5\n\n# Legacy data preparation\npython prepare_data.py --input raw/ --output data.txt\n</code></pre></p>"},{"location":"reference/migration.html#new-cli-commands","title":"New CLI Commands","text":"<p>New commands: <pre><code># New training\nllmbuilder train model --config config.json --data data.txt --output model/\n\n# New generation\nllmbuilder generate text --model model.pt --tokenizer tokenizer/ --prompt \"Hello\"\n\n# New data preparation\nllmbuilder data load --input raw/ --output data.txt --clean\n</code></pre></p>"},{"location":"reference/migration.html#cli-command-mapping","title":"CLI Command Mapping","text":"Legacy Command New Command Notes <code>python train.py</code> <code>llmbuilder train model</code> Unified training interface <code>python sample.py</code> <code>llmbuilder generate text</code> Enhanced generation options <code>python prepare_data.py</code> <code>llmbuilder data load</code> Better data processing <code>python eval.py</code> <code>llmbuilder model evaluate</code> Built-in evaluation"},{"location":"reference/migration.html#data-format-migration","title":"\ud83d\udcca Data Format Migration","text":""},{"location":"reference/migration.html#legacy-data-format","title":"Legacy Data Format","text":"<p>Old format: - Single text file with raw concatenated text - Manual tokenization required - No metadata or structure</p>"},{"location":"reference/migration.html#new-data-format","title":"New Data Format","text":"<p>New format: - Multiple input formats supported (PDF, DOCX, etc.) - Automatic cleaning and preprocessing - Structured dataset with metadata - Built-in train/validation splitting</p> <p>Migration example: <pre><code># Legacy data preparation\nwith open('data.txt', 'r') as f:\n    text = f.read()\n\n# Tokenize manually\nimport tiktoken\nenc = tiktoken.get_encoding(\"gpt2\")\ntokens = enc.encode(text)\n\n# New data preparation\nfrom llmbuilder.data import DataLoader, TextDataset\n\n# Load and clean automatically\nloader = DataLoader(clean_text=True, remove_duplicates=True)\ntexts = loader.load_directory(\"raw_data/\")\n\n# Create structured dataset\ndataset = TextDataset(\n    texts,\n    block_size=1024,\n    stride=512,\n    train_split=0.9\n)\n</code></pre></p>"},{"location":"reference/migration.html#model-architecture-migration","title":"\ud83d\udd27 Model Architecture Migration","text":""},{"location":"reference/migration.html#legacy-model-definition","title":"Legacy Model Definition","text":"<p>Old way: <pre><code>class GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        # Manual layer definition...\n\n    def forward(self, idx, targets=None):\n        # Manual forward pass...\n</code></pre></p>"},{"location":"reference/migration.html#new-model-definition","title":"New Model Definition","text":"<p>New way: <pre><code># Use built-in model builder\nimport llmbuilder as lb\n\nconfig = lb.load_config(preset=\"gpu_medium\")\nmodel = lb.build_model(config.model)\n\n# Or customize if needed\nfrom llmbuilder.model import GPTModel\nfrom llmbuilder.config import ModelConfig\n\ncustom_config = ModelConfig(\n    vocab_size=32000,\n    num_layers=16,\n    num_heads=16,\n    embedding_dim=1024\n)\n\nmodel = GPTModel(custom_config)\n</code></pre></p>"},{"location":"reference/migration.html#deployment-migration","title":"\ud83d\ude80 Deployment Migration","text":""},{"location":"reference/migration.html#legacy-deployment","title":"Legacy Deployment","text":"<p>Old way: <pre><code># Manual model saving/loading\ntorch.save({\n    'model': model.state_dict(),\n    'config': config,\n    'iter_num': iter_num,\n}, 'model.pt')\n\n# Manual inference setup\ncheckpoint = torch.load('model.pt')\nmodel.load_state_dict(checkpoint['model'])\n</code></pre></p>"},{"location":"reference/migration.html#new-deployment","title":"New Deployment","text":"<p>New way: <pre><code># Automatic model management\nfrom llmbuilder.model import save_model, load_model\n\n# Save with metadata\nsave_model(model, \"model.pt\", metadata={\n    \"training_config\": config,\n    \"performance_metrics\": results\n})\n\n# Load with validation\nmodel = load_model(\"model.pt\")\n\n# Export for deployment\nfrom llmbuilder.export import export_gguf\nexport_gguf(\"model.pt\", \"model.gguf\", quantization=\"q4_0\")\n</code></pre></p>"},{"location":"reference/migration.html#testing-migration","title":"\ud83d\udd0d Testing Migration","text":""},{"location":"reference/migration.html#verify-migration-success","title":"Verify Migration Success","text":"<p>1. Configuration Test: <pre><code>import llmbuilder as lb\n\n# Test configuration loading\nconfig = lb.load_config(\"migrated_config.json\")\nprint(f\"Config loaded: {config.model.num_layers} layers\")\n\n# Test model building\nmodel = lb.build_model(config.model)\nprint(f\"Model built: {sum(p.numel() for p in model.parameters()):,} parameters\")\n</code></pre></p> <p>2. Training Test: <pre><code># Test training pipeline\nfrom llmbuilder.data import TextDataset\n\ndataset = TextDataset(\"test_data.txt\", block_size=512)\nresults = lb.train_model(model, dataset, config.training)\nprint(f\"Training completed: {results.final_loss:.4f} final loss\")\n</code></pre></p> <p>3. Generation Test: <pre><code># Test generation\ntext = lb.generate_text(\n    model_path=\"model.pt\",\n    tokenizer_path=\"tokenizer/\",\n    prompt=\"Test prompt\",\n    max_new_tokens=50\n)\nprint(f\"Generated: {text}\")\n</code></pre></p>"},{"location":"reference/migration.html#common-migration-issues","title":"\ud83d\udea8 Common Migration Issues","text":""},{"location":"reference/migration.html#issue-1-configuration-validation-errors","title":"Issue 1: Configuration Validation Errors","text":"<p>Problem: Legacy configs fail validation Solution: <pre><code>from llmbuilder.config import validate_config, fix_config\n\n# Validate and fix automatically\nconfig = lb.load_config(\"legacy_config.json\")\nis_valid, errors = validate_config(config)\n\nif not is_valid:\n    fixed_config = fix_config(config, errors)\n    fixed_config.save(\"fixed_config.json\")\n</code></pre></p>"},{"location":"reference/migration.html#issue-2-model-size-mismatch","title":"Issue 2: Model Size Mismatch","text":"<p>Problem: Tokenizer vocab size doesn't match model Solution: <pre><code># Check and fix vocab size mismatch\nfrom llmbuilder.tokenizer import Tokenizer\n\ntokenizer = Tokenizer.from_pretrained(\"tokenizer/\")\nconfig.model.vocab_size = len(tokenizer)\n</code></pre></p>"},{"location":"reference/migration.html#issue-3-performance-regression","title":"Issue 3: Performance Regression","text":"<p>Problem: New version is slower than legacy Solution: <pre><code># Enable performance optimizations\nconfig.system.compile = True  # PyTorch 2.0 compilation\nconfig.training.mixed_precision = \"fp16\"  # Mixed precision\nconfig.training.dataloader_num_workers = 4  # Parallel data loading\n</code></pre></p>"},{"location":"reference/migration.html#issue-4-generation-quality-differences","title":"Issue 4: Generation Quality Differences","text":"<p>Problem: Generated text quality differs from legacy Solution: <pre><code># Match legacy generation parameters\nfrom llmbuilder.inference import GenerationConfig\n\nlegacy_config = GenerationConfig(\n    temperature=0.8,\n    top_k=200,  # Legacy often used higher top_k\n    do_sample=True,\n    repetition_penalty=1.0  # Legacy default\n)\n\ntext = lb.generate_text(\n    model_path=\"model.pt\",\n    tokenizer_path=\"tokenizer/\",\n    prompt=prompt,\n    config=legacy_config\n)\n</code></pre></p>"},{"location":"reference/migration.html#migration-examples","title":"\ud83d\udcda Migration Examples","text":""},{"location":"reference/migration.html#complete-migration-script","title":"Complete Migration Script","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nComplete migration script from legacy LLMBuilder to new package.\n\"\"\"\n\nimport json\nimport shutil\nfrom pathlib import Path\nimport llmbuilder as lb\n\ndef migrate_project(legacy_dir, new_dir):\n    \"\"\"Migrate entire legacy project to new structure.\"\"\"\n\n    legacy_path = Path(legacy_dir)\n    new_path = Path(new_dir)\n    new_path.mkdir(exist_ok=True)\n\n    print(f\"Migrating {legacy_dir} -&gt; {new_dir}\")\n\n    # 1. Migrate configuration\n    legacy_config = legacy_path / \"config.json\"\n    if legacy_config.exists():\n        print(\"Migrating configuration...\")\n        config = lb.load_config(str(legacy_config))\n        config.save(str(new_path / \"config.json\"))\n\n    # 2. Copy and organize data\n    print(\"Organizing data...\")\n    data_dir = new_path / \"data\"\n    data_dir.mkdir(exist_ok=True)\n\n    for data_file in legacy_path.glob(\"*.txt\"):\n        if \"data\" in data_file.name.lower():\n            shutil.copy2(data_file, data_dir / data_file.name)\n\n    # 3. Migrate model if exists\n    model_files = list(legacy_path.glob(\"*.pt\"))\n    if model_files:\n        print(\"Migrating model...\")\n        model_dir = new_path / \"model\"\n        model_dir.mkdir(exist_ok=True)\n\n        for model_file in model_files:\n            shutil.copy2(model_file, model_dir / model_file.name)\n\n    # 4. Create new training script\n    print(\"Creating new training script...\")\n    create_training_script(new_path)\n\n    print(\"Migration completed!\")\n\ndef create_training_script(project_dir):\n    \"\"\"Create new training script using LLMBuilder package.\"\"\"\n\n    script_content = '''#!/usr/bin/env python3\n\"\"\"\nMigrated training script using LLMBuilder package.\n\"\"\"\n\nimport llmbuilder as lb\nfrom llmbuilder.data import TextDataset\n\ndef main():\n    # Load configuration\n    config = lb.load_config(\"config.json\")\n\n    # Build model\n    model = lb.build_model(config.model)\n\n    # Prepare dataset\n    dataset = TextDataset(\n        \"data/training_data.txt\",\n        block_size=config.model.max_seq_length\n    )\n\n    # Train model\n    results = lb.train_model(model, dataset, config.training)\n\n    print(f\"Training completed: {results.final_loss:.4f} final loss\")\n\nif __name__ == \"__main__\":\n    main()\n'''\n\n    script_path = project_dir / \"train.py\"\n    with open(script_path, 'w') as f:\n        f.write(script_content)\n\n# Usage\nif __name__ == \"__main__\":\n    migrate_project(\"legacy_project/\", \"new_project/\")\n</code></pre>"},{"location":"reference/migration.html#post-migration-checklist","title":"\u2705 Post-Migration Checklist","text":"<p>After migration, verify these items:</p> <ul> <li>[ ] Configuration loads without errors</li> <li>[ ] Model builds with correct architecture</li> <li>[ ] Training runs successfully</li> <li>[ ] Generation produces expected quality</li> <li>[ ] Performance meets expectations</li> <li>[ ] All CLI commands work</li> <li>[ ] Export functionality works</li> <li>[ ] Tests pass</li> </ul>"},{"location":"reference/migration.html#getting-help","title":"\ud83c\udd98 Getting Help","text":"<p>If you encounter issues during migration:</p> <ol> <li>Check the FAQ: Common migration issues are covered</li> <li>GitHub Issues: Search for similar migration problems</li> <li>GitHub Discussions: Ask the community for help</li> <li>Documentation: Review the complete guides</li> <li>Examples: Look at working migration examples</li> </ol> <p>Migration Tips</p> <ul> <li>Start with a small test project before migrating everything</li> <li>Keep backups of your legacy code and models</li> <li>Test thoroughly after migration</li> <li>Take advantage of new features like automatic data cleaning</li> <li>Update your deployment scripts to use the new export functionality</li> </ul>"},{"location":"user-guide/configuration.html","title":"Configuration","text":"<p>LLMBuilder uses a hierarchical configuration system that makes it easy to customize every aspect of your model training and inference. This guide covers all configuration options and best practices.</p>"},{"location":"user-guide/configuration.html#configuration-overview","title":"\ud83c\udfaf Configuration Overview","text":"<p>LLMBuilder configurations are organized into logical sections:</p> <pre><code>graph TB\n    A[Configuration] --&gt; B[Model Config]\n    A --&gt; C[Training Config]\n    A --&gt; D[Data Config]\n    A --&gt; E[System Config]\n\n    B --&gt; B1[Architecture]\n    B --&gt; B2[Vocabulary]\n    B --&gt; B3[Sequence Length]\n\n    C --&gt; C1[Learning Rate]\n    C --&gt; C2[Batch Size]\n    C --&gt; C3[Optimization]\n\n    D --&gt; D1[Preprocessing]\n    D --&gt; D2[Tokenization]\n    D --&gt; D3[Validation Split]\n\n    E --&gt; E1[Device]\n    E --&gt; E2[Memory]\n    E --&gt; E3[Logging]</code></pre>"},{"location":"user-guide/configuration.html#configuration-methods","title":"\ud83d\udccb Configuration Methods","text":""},{"location":"user-guide/configuration.html#method-1-using-presets-recommended","title":"Method 1: Using Presets (Recommended)","text":"<p>LLMBuilder comes with optimized presets for different use cases:</p> <pre><code>import llmbuilder as lb\n\n# Load a preset configuration\nconfig = lb.load_config(preset=\"cpu_small\")\n</code></pre> <p>Available presets:</p> Preset Use Case Memory Parameters Training Time <code>tiny</code> Testing, debugging ~500MB ~1M Minutes <code>cpu_small</code> CPU training ~2GB ~10M Hours <code>gpu_medium</code> Single GPU ~8GB ~50M Hours <code>gpu_large</code> High-end GPU ~16GB+ ~200M+ Days <code>inference</code> Text generation only ~1GB Variable N/A"},{"location":"user-guide/configuration.html#method-2-from-configuration-file","title":"Method 2: From Configuration File","text":"<pre><code># Load from JSON file\nconfig = lb.load_config(path=\"my_config.json\")\n\n# Load from YAML file\nconfig = lb.load_config(path=\"my_config.yaml\")\n</code></pre>"},{"location":"user-guide/configuration.html#method-3-programmatic-configuration","title":"Method 3: Programmatic Configuration","text":"<pre><code>from llmbuilder.config import Config, ModelConfig, TrainingConfig\n\nconfig = Config(\n    model=ModelConfig(\n        vocab_size=16000,\n        num_layers=12,\n        num_heads=12,\n        embedding_dim=768,\n        max_seq_length=1024,\n        dropout=0.1\n    ),\n    training=TrainingConfig(\n        batch_size=16,\n        num_epochs=10,\n        learning_rate=3e-4,\n        warmup_steps=1000\n    )\n)\n</code></pre>"},{"location":"user-guide/configuration.html#model-configuration","title":"\ud83e\udde0 Model Configuration","text":""},{"location":"user-guide/configuration.html#core-architecture-settings","title":"Core Architecture Settings","text":"<pre><code>{\n  \"model\": {\n    \"vocab_size\": 16000,\n    \"num_layers\": 12,\n    \"num_heads\": 12,\n    \"embedding_dim\": 768,\n    \"max_seq_length\": 1024,\n    \"dropout\": 0.1,\n    \"bias\": true,\n    \"model_type\": \"gpt\"\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration.html#parameter-explanations","title":"Parameter Explanations","text":"<p><code>vocab_size</code> (int, default: 16000) : Size of the vocabulary. Must match your tokenizer's vocabulary size.</p> <p><code>num_layers</code> (int, default: 12) : Number of transformer layers. More layers = more capacity but slower training.</p> <p><code>num_heads</code> (int, default: 12) : Number of attention heads per layer. Should divide <code>embedding_dim</code> evenly.</p> <p><code>embedding_dim</code> (int, default: 768) : Dimension of token embeddings and hidden states. Larger = more capacity.</p> <p><code>max_seq_length</code> (int, default: 1024) : Maximum sequence length the model can process. Affects memory usage quadratically.</p> <p><code>dropout</code> (float, default: 0.1) : Dropout rate for regularization. Higher values prevent overfitting.</p>"},{"location":"user-guide/configuration.html#advanced-model-settings","title":"Advanced Model Settings","text":"<pre><code>{\n  \"model\": {\n    \"activation\": \"gelu\",\n    \"layer_norm_eps\": 1e-5,\n    \"initializer_range\": 0.02,\n    \"use_cache\": true,\n    \"gradient_checkpointing\": false,\n    \"tie_word_embeddings\": true\n  }\n}\n</code></pre> <p><code>activation</code> (str, default: \"gelu\") : Activation function. Options: \"gelu\", \"relu\", \"swish\", \"silu\".</p> <p><code>gradient_checkpointing</code> (bool, default: false) : Trade compute for memory. Enables training larger models with less GPU memory.</p> <p><code>tie_word_embeddings</code> (bool, default: true) : Share input and output embedding weights. Reduces parameters by ~vocab_size * embedding_dim.</p>"},{"location":"user-guide/configuration.html#training-configuration","title":"\ud83c\udfcb\ufe0f Training Configuration","text":""},{"location":"user-guide/configuration.html#basic-training-settings","title":"Basic Training Settings","text":"<pre><code>{\n  \"training\": {\n    \"batch_size\": 16,\n    \"num_epochs\": 10,\n    \"learning_rate\": 3e-4,\n    \"weight_decay\": 0.01,\n    \"max_grad_norm\": 1.0,\n    \"warmup_steps\": 1000,\n    \"save_every\": 1000,\n    \"eval_every\": 500,\n    \"log_every\": 100\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration.html#parameter-explanations_1","title":"Parameter Explanations","text":"<p><code>batch_size</code> (int, default: 16) : Number of samples per training step. Larger batches are more stable but use more memory.</p> <p><code>learning_rate</code> (float, default: 3e-4) : Step size for parameter updates. Too high = unstable, too low = slow convergence.</p> <p><code>weight_decay</code> (float, default: 0.01) : L2 regularization strength. Helps prevent overfitting.</p> <p><code>max_grad_norm</code> (float, default: 1.0) : Gradient clipping threshold. Prevents exploding gradients.</p> <p><code>warmup_steps</code> (int, default: 1000) : Number of steps to linearly increase learning rate from 0 to target value.</p>"},{"location":"user-guide/configuration.html#advanced-training-settings","title":"Advanced Training Settings","text":"<pre><code>{\n  \"training\": {\n    \"optimizer\": \"adamw\",\n    \"scheduler\": \"cosine\",\n    \"beta1\": 0.9,\n    \"beta2\": 0.999,\n    \"eps\": 1e-8,\n    \"gradient_accumulation_steps\": 1,\n    \"mixed_precision\": \"fp16\",\n    \"dataloader_num_workers\": 4,\n    \"pin_memory\": true\n  }\n}\n</code></pre> <p><code>optimizer</code> (str, default: \"adamw\") : Optimization algorithm. Options: \"adamw\", \"adam\", \"sgd\", \"adafactor\".</p> <p><code>scheduler</code> (str, default: \"cosine\") : Learning rate schedule. Options: \"cosine\", \"linear\", \"constant\", \"polynomial\".</p> <p><code>mixed_precision</code> (str, default: \"fp16\") : Use mixed precision training. Options: \"fp16\", \"bf16\", \"fp32\".</p> <p><code>gradient_accumulation_steps</code> (int, default: 1) : Accumulate gradients over multiple steps. Effective batch size = batch_size * gradient_accumulation_steps.</p>"},{"location":"user-guide/configuration.html#data-configuration","title":"\ud83d\udcca Data Configuration","text":"<pre><code>{\n  \"data\": {\n    \"train_split\": 0.9,\n    \"val_split\": 0.1,\n    \"test_split\": 0.0,\n    \"shuffle\": true,\n    \"seed\": 42,\n    \"block_size\": 1024,\n    \"stride\": 512,\n    \"min_length\": 10,\n    \"max_length\": null,\n    \"preprocessing\": {\n      \"lowercase\": false,\n      \"remove_special_chars\": false,\n      \"normalize_whitespace\": true,\n      \"filter_languages\": null\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration.html#data-processing-options","title":"Data Processing Options","text":"<p><code>block_size</code> (int, default: 1024) : Length of input sequences. Should match model's <code>max_seq_length</code>.</p> <p><code>stride</code> (int, default: 512) : Overlap between consecutive sequences. Larger stride = less overlap = more diverse samples.</p> <p><code>min_length</code> (int, default: 10) : Minimum sequence length to keep. Filters out very short sequences.</p>"},{"location":"user-guide/configuration.html#system-configuration","title":"\ud83d\udda5\ufe0f System Configuration","text":"<pre><code>{\n  \"system\": {\n    \"device\": \"auto\",\n    \"num_gpus\": 1,\n    \"distributed\": false,\n    \"compile\": false,\n    \"seed\": 42,\n    \"deterministic\": false,\n    \"benchmark\": true,\n    \"cache_dir\": \"./cache\",\n    \"output_dir\": \"./output\",\n    \"logging_level\": \"INFO\"\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration.html#device-and-performance-settings","title":"Device and Performance Settings","text":"<p><code>device</code> (str, default: \"auto\") : Device to use. Options: \"auto\", \"cpu\", \"cuda\", \"cuda:0\", etc.</p> <p><code>compile</code> (bool, default: false) : Use PyTorch 2.0 compilation for faster training (experimental).</p> <p><code>benchmark</code> (bool, default: true) : Enable cuDNN benchmarking for consistent input sizes.</p>"},{"location":"user-guide/configuration.html#creating-custom-configurations","title":"\ud83d\udee0\ufe0f Creating Custom Configurations","text":""},{"location":"user-guide/configuration.html#using-the-cli","title":"Using the CLI","text":"<pre><code># Interactive configuration creation\nllmbuilder config create --interactive\n\n# Create from preset and customize\nllmbuilder config create --preset cpu_small --output my_config.json\n</code></pre>"},{"location":"user-guide/configuration.html#programmatic-creation","title":"Programmatic Creation","text":"<pre><code>from llmbuilder.config import Config, ModelConfig, TrainingConfig\n\n# Create custom configuration\nconfig = Config(\n    model=ModelConfig(\n        vocab_size=32000,  # Larger vocabulary\n        num_layers=24,     # Deeper model\n        num_heads=16,      # More attention heads\n        embedding_dim=1024, # Larger embeddings\n        max_seq_length=2048, # Longer sequences\n        dropout=0.1\n    ),\n    training=TrainingConfig(\n        batch_size=8,      # Smaller batch for larger model\n        num_epochs=20,     # More training\n        learning_rate=1e-4, # Lower learning rate\n        warmup_steps=2000,  # Longer warmup\n        gradient_accumulation_steps=4  # Effective batch size = 32\n    )\n)\n\n# Save configuration\nconfig.save(\"custom_config.json\")\n</code></pre>"},{"location":"user-guide/configuration.html#configuration-inheritance","title":"Configuration Inheritance","text":"<pre><code># Start with a preset\nbase_config = lb.load_config(preset=\"gpu_medium\")\n\n# Modify specific settings\nbase_config.model.num_layers = 18\nbase_config.training.learning_rate = 1e-4\nbase_config.training.batch_size = 12\n\n# Save modified configuration\nbase_config.save(\"modified_config.json\")\n</code></pre>"},{"location":"user-guide/configuration.html#configuration-validation","title":"\ud83d\udd27 Configuration Validation","text":""},{"location":"user-guide/configuration.html#automatic-validation","title":"Automatic Validation","text":"<p>LLMBuilder automatically validates configurations:</p> <pre><code>config = lb.load_config(\"my_config.json\")\n# Validation happens automatically\n\n# Manual validation\nfrom llmbuilder.config import validate_config\nis_valid, errors = validate_config(config)\nif not is_valid:\n    for error in errors:\n        print(f\"Error: {error}\")\n</code></pre>"},{"location":"user-guide/configuration.html#cli-validation","title":"CLI Validation","text":"<pre><code>llmbuilder config validate my_config.json\n</code></pre>"},{"location":"user-guide/configuration.html#common-validation-errors","title":"Common Validation Errors","text":"<p>Common Issues</p> <ul> <li><code>num_heads</code> doesn't divide <code>embedding_dim</code>: embedding_dim must be divisible by num_heads</li> <li><code>vocab_size</code> mismatch: Must match tokenizer vocabulary size</li> <li>Memory constraints: batch_size \u00d7 max_seq_length \u00d7 embedding_dim too large for available memory</li> <li>Invalid device: Specified device not available</li> </ul>"},{"location":"user-guide/configuration.html#performance-tuning","title":"\ud83d\udcc8 Performance Tuning","text":""},{"location":"user-guide/configuration.html#memory-optimization","title":"Memory Optimization","text":"<pre><code>{\n  \"model\": {\n    \"gradient_checkpointing\": true,\n    \"tie_word_embeddings\": true\n  },\n  \"training\": {\n    \"mixed_precision\": \"fp16\",\n    \"gradient_accumulation_steps\": 4,\n    \"batch_size\": 4\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration.html#speed-optimization","title":"Speed Optimization","text":"<pre><code>{\n  \"system\": {\n    \"compile\": true,\n    \"benchmark\": true\n  },\n  \"training\": {\n    \"dataloader_num_workers\": 8,\n    \"pin_memory\": true\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration.html#quality-optimization","title":"Quality Optimization","text":"<pre><code>{\n  \"model\": {\n    \"num_layers\": 24,\n    \"embedding_dim\": 1024,\n    \"dropout\": 0.1\n  },\n  \"training\": {\n    \"learning_rate\": 1e-4,\n    \"warmup_steps\": 2000,\n    \"weight_decay\": 0.01\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration.html#configuration-best-practices","title":"\ud83c\udfaf Configuration Best Practices","text":""},{"location":"user-guide/configuration.html#1-start-with-presets","title":"1. Start with Presets","text":"<p>Always start with a preset that matches your hardware:</p> <pre><code># For CPU development\nconfig = lb.load_config(preset=\"cpu_small\")\n\n# For single GPU\nconfig = lb.load_config(preset=\"gpu_medium\")\n\n# For multiple GPUs\nconfig = lb.load_config(preset=\"gpu_large\")\n</code></pre>"},{"location":"user-guide/configuration.html#2-scale-gradually","title":"2. Scale Gradually","text":"<p>When increasing model size, scale parameters proportionally:</p> <pre><code># If doubling embedding_dim, consider:\nconfig.model.embedding_dim *= 2\nconfig.model.num_heads *= 2  # Keep head_dim constant\nconfig.training.learning_rate *= 0.7  # Reduce LR for larger models\nconfig.training.warmup_steps *= 2  # Longer warmup\n</code></pre>"},{"location":"user-guide/configuration.html#3-monitor-memory-usage","title":"3. Monitor Memory Usage","text":"<pre><code># Check memory requirements before training\nfrom llmbuilder.utils import estimate_memory_usage\n\nmemory_gb = estimate_memory_usage(config)\nprint(f\"Estimated memory usage: {memory_gb:.1f} GB\")\n</code></pre>"},{"location":"user-guide/configuration.html#4-use-configuration-templates","title":"4. Use Configuration Templates","text":"<p>Create templates for common scenarios:</p> <pre><code># templates/research_config.py\ndef get_research_config(vocab_size, dataset_size):\n    \"\"\"Configuration optimized for research experiments.\"\"\"\n    return Config(\n        model=ModelConfig(\n            vocab_size=vocab_size,\n            num_layers=12 if dataset_size &lt; 1e6 else 24,\n            embedding_dim=768,\n            max_seq_length=1024,\n            dropout=0.1\n        ),\n        training=TrainingConfig(\n            batch_size=16,\n            learning_rate=3e-4,\n            num_epochs=10 if dataset_size &lt; 1e6 else 20,\n            warmup_steps=min(1000, dataset_size // 100)\n        )\n    )\n</code></pre>"},{"location":"user-guide/configuration.html#debugging-configuration-issues","title":"\ud83d\udd0d Debugging Configuration Issues","text":""},{"location":"user-guide/configuration.html#enable-verbose-logging","title":"Enable Verbose Logging","text":"<pre><code>{\n  \"system\": {\n    \"logging_level\": \"DEBUG\"\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration.html#check-configuration-summary","title":"Check Configuration Summary","text":"<pre><code>config = lb.load_config(\"my_config.json\")\nprint(config.summary())\n</code></pre>"},{"location":"user-guide/configuration.html#validate-against-hardware","title":"Validate Against Hardware","text":"<pre><code>llmbuilder config validate my_config.json --check-hardware\n</code></pre>"},{"location":"user-guide/configuration.html#configuration-examples","title":"\ud83d\udcda Configuration Examples","text":""},{"location":"user-guide/configuration.html#minimal-cpu-configuration","title":"Minimal CPU Configuration","text":"<pre><code>{\n  \"model\": {\n    \"vocab_size\": 8000,\n    \"num_layers\": 4,\n    \"num_heads\": 4,\n    \"embedding_dim\": 256,\n    \"max_seq_length\": 512\n  },\n  \"training\": {\n    \"batch_size\": 4,\n    \"num_epochs\": 5,\n    \"learning_rate\": 1e-3\n  },\n  \"system\": {\n    \"device\": \"cpu\"\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration.html#high-performance-gpu-configuration","title":"High-Performance GPU Configuration","text":"<pre><code>{\n  \"model\": {\n    \"vocab_size\": 32000,\n    \"num_layers\": 24,\n    \"num_heads\": 16,\n    \"embedding_dim\": 1024,\n    \"max_seq_length\": 2048,\n    \"gradient_checkpointing\": true\n  },\n  \"training\": {\n    \"batch_size\": 8,\n    \"num_epochs\": 20,\n    \"learning_rate\": 1e-4,\n    \"mixed_precision\": \"fp16\",\n    \"gradient_accumulation_steps\": 4\n  },\n  \"system\": {\n    \"device\": \"cuda\",\n    \"compile\": true\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration.html#fine-tuning-configuration","title":"Fine-tuning Configuration","text":"<pre><code>{\n  \"training\": {\n    \"learning_rate\": 5e-5,\n    \"num_epochs\": 3,\n    \"warmup_steps\": 100,\n    \"weight_decay\": 0.01,\n    \"save_every\": 100\n  },\n  \"data\": {\n    \"block_size\": 512,\n    \"stride\": 256\n  }\n}\n</code></pre> <p>Configuration Tips</p> <ul> <li>Always validate your configuration before training</li> <li>Start with presets and modify incrementally</li> <li>Monitor memory usage and adjust batch size accordingly</li> <li>Use gradient accumulation to simulate larger batch sizes</li> <li>Save successful configurations for future use</li> </ul>"},{"location":"user-guide/data-processing.html","title":"Data Processing","text":"<p>Data is the foundation of any successful language model. LLMBuilder provides comprehensive tools for loading, cleaning, and preparing text data from various sources. This guide covers everything from basic text files to complex document processing.</p>"},{"location":"user-guide/data-processing.html#overview","title":"\ud83c\udfaf Overview","text":"<p>LLMBuilder's data processing pipeline handles:</p> <pre><code>graph LR\n    A[Raw Documents] --&gt; B[Data Loader]\n    B --&gt; C[Text Cleaner]\n    C --&gt; D[Dataset Creator]\n    D --&gt; E[Training Ready]\n\n    A1[PDF] --&gt; B\n    A2[DOCX] --&gt; B\n    A3[TXT] --&gt; B\n    A4[HTML] --&gt; B\n    A5[Markdown] --&gt; B\n\n    style A fill:#e1f5fe\n    style E fill:#e8f5e8</code></pre>"},{"location":"user-guide/data-processing.html#supported-file-formats","title":"\ud83d\udcc1 Supported File Formats","text":"<p>LLMBuilder can process various document formats:</p> Format Extension Description Quality Plain Text <code>.txt</code> Raw text files \u2b50\u2b50\u2b50\u2b50\u2b50 PDF <code>.pdf</code> Portable Document Format \u2b50\u2b50\u2b50\u2b50 Word Documents <code>.docx</code> Microsoft Word documents \u2b50\u2b50\u2b50\u2b50 HTML <code>.html</code>, <code>.htm</code> Web pages \u2b50\u2b50\u2b50 Markdown <code>.md</code> Markdown files \u2b50\u2b50\u2b50\u2b50\u2b50 PowerPoint <code>.pptx</code> Presentation slides \u2b50\u2b50\u2b50 CSV <code>.csv</code> Comma-separated values \u2b50\u2b50\u2b50"},{"location":"user-guide/data-processing.html#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"user-guide/data-processing.html#basic-data-loading","title":"Basic Data Loading","text":"<pre><code># Load all supported files from a directory\nllmbuilder data load \\\n  --input ./documents \\\n  --output clean_text.txt \\\n  --format all \\\n  --clean\n</code></pre>"},{"location":"user-guide/data-processing.html#python-api","title":"Python API","text":"<pre><code>from llmbuilder.data import DataLoader\n\n# Initialize data loader\nloader = DataLoader(\n    min_length=50,      # Filter short texts\n    clean_text=True,    # Apply text cleaning\n    remove_duplicates=True\n)\n\n# Load single file\ntext = loader.load_file(\"document.pdf\")\n\n# Load directory\ntexts = loader.load_directory(\"./documents\")\n\n# Combine and save\ncombined = \"\\n\\n\".join(texts)\nwith open(\"training_data.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(combined)\n</code></pre>"},{"location":"user-guide/data-processing.html#file-format-specifics","title":"\ud83d\udcc4 File Format Specifics","text":""},{"location":"user-guide/data-processing.html#pdf-processing","title":"PDF Processing","text":"<p>PDFs are processed using PyMuPDF for high-quality text extraction:</p> <pre><code>from llmbuilder.data import DataLoader\n\nloader = DataLoader(\n    pdf_options={\n        \"extract_images\": False,    # Skip image text extraction\n        \"preserve_layout\": True,    # Maintain document structure\n        \"min_font_size\": 8,        # Filter small text\n        \"ignore_headers_footers\": True\n    }\n)\n\ntext = loader.load_file(\"document.pdf\")\n</code></pre>"},{"location":"user-guide/data-processing.html#pdf-processing-options","title":"PDF Processing Options","text":"<pre><code>pdf_options = {\n    \"extract_images\": False,        # Extract text from images (OCR)\n    \"preserve_layout\": True,        # Keep paragraph structure\n    \"min_font_size\": 8,            # Minimum font size to extract\n    \"ignore_headers_footers\": True, # Skip headers/footers\n    \"page_range\": (1, 10),         # Extract specific pages\n    \"password\": None               # PDF password if needed\n}\n</code></pre>"},{"location":"user-guide/data-processing.html#word-document-processing","title":"Word Document Processing","text":"<p>DOCX files are processed with full formatting awareness:</p> <pre><code>loader = DataLoader(\n    docx_options={\n        \"include_tables\": True,     # Extract table content\n        \"include_headers\": False,   # Skip headers\n        \"include_footers\": False,   # Skip footers\n        \"preserve_formatting\": True # Keep basic formatting\n    }\n)\n</code></pre>"},{"location":"user-guide/data-processing.html#html-processing","title":"HTML Processing","text":"<p>HTML content is cleaned and converted to plain text:</p> <pre><code>loader = DataLoader(\n    html_options={\n        \"remove_scripts\": True,     # Remove JavaScript\n        \"remove_styles\": True,      # Remove CSS\n        \"preserve_links\": False,    # Keep link text only\n        \"extract_tables\": True,     # Convert tables to text\n        \"min_text_length\": 20      # Filter short elements\n    }\n)\n</code></pre>"},{"location":"user-guide/data-processing.html#text-cleaning","title":"\ud83e\uddf9 Text Cleaning","text":"<p>LLMBuilder includes comprehensive text cleaning capabilities:</p>"},{"location":"user-guide/data-processing.html#automatic-cleaning","title":"Automatic Cleaning","text":"<pre><code>from llmbuilder.data import TextCleaner\n\ncleaner = TextCleaner(\n    normalize_whitespace=True,      # Fix spacing issues\n    remove_special_chars=False,     # Keep punctuation\n    fix_encoding=True,              # Fix encoding issues\n    remove_urls=True,               # Remove web URLs\n    remove_emails=True,             # Remove email addresses\n    remove_phone_numbers=True,      # Remove phone numbers\n    min_sentence_length=10,         # Filter short sentences\n    max_sentence_length=1000,       # Filter very long sentences\n    remove_duplicates=True,         # Remove duplicate sentences\n    language_filter=\"en\"            # Keep only English text\n)\n\ncleaned_text = cleaner.clean(raw_text)\n</code></pre>"},{"location":"user-guide/data-processing.html#custom-cleaning-rules","title":"Custom Cleaning Rules","text":"<pre><code># Define custom cleaning function\ndef custom_cleaner(text):\n    # Remove specific patterns\n    import re\n    text = re.sub(r'\\[.*?\\]', '', text)  # Remove bracketed content\n    text = re.sub(r'\\d{4}-\\d{2}-\\d{2}', '', text)  # Remove dates\n    return text\n\n# Apply custom cleaning\ncleaner = TextCleaner(custom_functions=[custom_cleaner])\ncleaned_text = cleaner.clean(raw_text)\n</code></pre>"},{"location":"user-guide/data-processing.html#cleaning-statistics","title":"Cleaning Statistics","text":"<pre><code>stats = cleaner.get_stats()\nprint(f\"Original length: {stats.original_length:,} characters\")\nprint(f\"Cleaned length: {stats.cleaned_length:,} characters\")\nprint(f\"Removed: {stats.removed_length:,} characters ({stats.removal_percentage:.1f}%)\")\nprint(f\"Sentences: {stats.sentence_count}\")\nprint(f\"Paragraphs: {stats.paragraph_count}\")\n</code></pre>"},{"location":"user-guide/data-processing.html#dataset-creation","title":"\ud83d\udcca Dataset Creation","text":""},{"location":"user-guide/data-processing.html#basic-dataset-creation","title":"Basic Dataset Creation","text":"<pre><code>from llmbuilder.data import TextDataset\n\n# Create dataset from text file\ndataset = TextDataset(\n    data_path=\"training_data.txt\",\n    block_size=1024,        # Sequence length\n    stride=512,             # Overlap between sequences\n    cache_in_memory=True    # Load all data into memory\n)\n\nprint(f\"Dataset size: {len(dataset):,} samples\")\n</code></pre>"},{"location":"user-guide/data-processing.html#multi-file-dataset","title":"Multi-File Dataset","text":"<pre><code>from llmbuilder.data import MultiFileDataset\n\n# Create dataset from multiple files\ndataset = MultiFileDataset(\n    file_paths=[\"file1.txt\", \"file2.txt\", \"file3.txt\"],\n    block_size=1024,\n    stride=512,\n    shuffle_files=True,     # Randomize file order\n    max_files=None          # Use all files\n)\n</code></pre>"},{"location":"user-guide/data-processing.html#dataset-splitting","title":"Dataset Splitting","text":"<pre><code>from llmbuilder.data import split_dataset\n\n# Split dataset into train/validation/test\ntrain_dataset, val_dataset, test_dataset = split_dataset(\n    dataset,\n    train_ratio=0.8,\n    val_ratio=0.1,\n    test_ratio=0.1,\n    seed=42\n)\n\nprint(f\"Train: {len(train_dataset):,} samples\")\nprint(f\"Validation: {len(val_dataset):,} samples\")\nprint(f\"Test: {len(test_dataset):,} samples\")\n</code></pre>"},{"location":"user-guide/data-processing.html#data-preprocessing-pipeline","title":"\ud83d\udd04 Data Preprocessing Pipeline","text":""},{"location":"user-guide/data-processing.html#complete-pipeline-example","title":"Complete Pipeline Example","text":"<pre><code>from llmbuilder.data import DataLoader, TextCleaner, TextDataset\nfrom pathlib import Path\n\ndef create_training_dataset(input_dir, output_file, block_size=1024):\n    \"\"\"Complete data preprocessing pipeline.\"\"\"\n\n    # Step 1: Load raw data\n    print(\"\ud83d\udcc1 Loading raw documents...\")\n    loader = DataLoader(\n        min_length=100,\n        clean_text=True,\n        remove_duplicates=True\n    )\n\n    texts = []\n    input_path = Path(input_dir)\n\n    for file_path in input_path.rglob(\"*\"):\n        if file_path.suffix.lower() in loader.supported_extensions:\n            try:\n                text = loader.load_file(file_path)\n                if text:\n                    texts.append(text)\n                    print(f\"  \u2705 {file_path.name}: {len(text):,} chars\")\n            except Exception as e:\n                print(f\"  \u274c {file_path.name}: {e}\")\n\n    # Step 2: Combine and clean\n    print(f\"\\n\ud83e\uddf9 Cleaning {len(texts)} documents...\")\n    combined_text = \"\\n\\n\".join(texts)\n\n    cleaner = TextCleaner(\n        normalize_whitespace=True,\n        remove_urls=True,\n        remove_emails=True,\n        min_sentence_length=20,\n        remove_duplicates=True\n    )\n\n    cleaned_text = cleaner.clean(combined_text)\n    stats = cleaner.get_stats()\n\n    print(f\"  Original: {stats.original_length:,} characters\")\n    print(f\"  Cleaned: {stats.cleaned_length:,} characters\")\n    print(f\"  Removed: {stats.removal_percentage:.1f}%\")\n\n    # Step 3: Save processed text\n    print(f\"\\n\ud83d\udcbe Saving to {output_file}...\")\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        f.write(cleaned_text)\n\n    # Step 4: Create dataset\n    print(f\"\\n\ud83d\udcca Creating dataset...\")\n    dataset = TextDataset(\n        data_path=output_file,\n        block_size=block_size,\n        stride=block_size // 2\n    )\n\n    print(f\"  Dataset size: {len(dataset):,} samples\")\n    print(f\"  Sequence length: {block_size}\")\n    print(f\"  Total tokens: {len(dataset) * block_size:,}\")\n\n    return dataset\n\n# Usage\ndataset = create_training_dataset(\n    input_dir=\"./raw_documents\",\n    output_file=\"./processed_data.txt\",\n    block_size=1024\n)\n</code></pre>"},{"location":"user-guide/data-processing.html#advanced-data-processing","title":"\ud83c\udf9b\ufe0f Advanced Data Processing","text":""},{"location":"user-guide/data-processing.html#streaming-large-datasets","title":"Streaming Large Datasets","text":"<p>For very large datasets that don't fit in memory:</p> <pre><code>from llmbuilder.data import StreamingDataset\n\n# Create streaming dataset\ndataset = StreamingDataset(\n    data_path=\"huge_dataset.txt\",\n    block_size=1024,\n    buffer_size=10000,      # Number of samples to keep in memory\n    shuffle_buffer=1000     # Shuffle buffer size\n)\n\n# Use with DataLoader\nfrom torch.utils.data import DataLoader\ndataloader = DataLoader(dataset, batch_size=16, num_workers=4)\n</code></pre>"},{"location":"user-guide/data-processing.html#custom-data-processing","title":"Custom Data Processing","text":"<pre><code>from llmbuilder.data import BaseDataset\n\nclass CustomDataset(BaseDataset):\n    \"\"\"Custom dataset with domain-specific processing.\"\"\"\n\n    def __init__(self, data_path, **kwargs):\n        super().__init__(**kwargs)\n        self.data = self.load_custom_data(data_path)\n\n    def load_custom_data(self, data_path):\n        \"\"\"Load and process data in custom format.\"\"\"\n        # Your custom loading logic here\n        pass\n\n    def __getitem__(self, idx):\n        \"\"\"Get a single sample.\"\"\"\n        # Your custom sample creation logic\n        pass\n\n    def __len__(self):\n        return len(self.data)\n</code></pre>"},{"location":"user-guide/data-processing.html#data-augmentation","title":"Data Augmentation","text":"<pre><code>from llmbuilder.data import DataAugmenter\n\naugmenter = DataAugmenter(\n    synonym_replacement=0.1,    # Replace 10% of words with synonyms\n    random_insertion=0.1,       # Insert random words\n    random_swap=0.1,           # Swap word positions\n    random_deletion=0.1,       # Delete random words\n    paraphrase=True            # Generate paraphrases\n)\n\n# Augment training data\naugmented_texts = []\nfor text in original_texts:\n    augmented = augmenter.augment(text, num_variants=3)\n    augmented_texts.extend(augmented)\n</code></pre>"},{"location":"user-guide/data-processing.html#data-quality-assessment","title":"\ud83d\udcc8 Data Quality Assessment","text":""},{"location":"user-guide/data-processing.html#quality-metrics","title":"Quality Metrics","text":"<pre><code>from llmbuilder.data import assess_data_quality\n\n# Analyze data quality\nquality_report = assess_data_quality(\"training_data.txt\")\n\nprint(f\"\ud83d\udcca Data Quality Report:\")\nprint(f\"  Total characters: {quality_report.total_chars:,}\")\nprint(f\"  Total words: {quality_report.total_words:,}\")\nprint(f\"  Total sentences: {quality_report.total_sentences:,}\")\nprint(f\"  Average sentence length: {quality_report.avg_sentence_length:.1f} words\")\nprint(f\"  Vocabulary size: {quality_report.vocab_size:,}\")\nprint(f\"  Duplicate sentences: {quality_report.duplicate_percentage:.1f}%\")\nprint(f\"  Language diversity: {quality_report.language_scores}\")\nprint(f\"  Readability score: {quality_report.readability_score:.1f}\")\n</code></pre>"},{"location":"user-guide/data-processing.html#data-visualization","title":"Data Visualization","text":"<pre><code>import matplotlib.pyplot as plt\nfrom llmbuilder.data import visualize_data\n\n# Create data visualizations\nfig, axes = visualize_data(\"training_data.txt\")\n\n# Sentence length distribution\naxes[0].hist(sentence_lengths, bins=50)\naxes[0].set_title(\"Sentence Length Distribution\")\n\n# Word frequency\naxes[1].bar(top_words, word_counts)\naxes[1].set_title(\"Most Common Words\")\n\nplt.tight_layout()\nplt.savefig(\"data_analysis.png\")\n</code></pre>"},{"location":"user-guide/data-processing.html#cli-data-processing","title":"\ud83d\udd27 CLI Data Processing","text":""},{"location":"user-guide/data-processing.html#interactive-data-processing","title":"Interactive Data Processing","text":"<pre><code># Interactive data loading with guided setup\nllmbuilder data load --interactive\n</code></pre> <p>This will prompt you for: - Input directory or file - Output file path - File formats to process - Cleaning options - Quality filters</p>"},{"location":"user-guide/data-processing.html#batch-processing","title":"Batch Processing","text":"<pre><code># Process multiple directories\nllmbuilder data load \\\n  --input \"dir1,dir2,dir3\" \\\n  --output combined_data.txt \\\n  --format all \\\n  --clean \\\n  --min-length 100 \\\n  --remove-duplicates\n</code></pre>"},{"location":"user-guide/data-processing.html#format-specific-processing","title":"Format-Specific Processing","text":"<pre><code># Process only PDF files\nllmbuilder data load \\\n  --input ./documents \\\n  --output pdf_text.txt \\\n  --format pdf \\\n  --pdf-extract-images \\\n  --pdf-min-font-size 10\n\n# Process only Word documents\nllmbuilder data load \\\n  --input ./documents \\\n  --output docx_text.txt \\\n  --format docx \\\n  --docx-include-tables \\\n  --docx-preserve-formatting\n</code></pre>"},{"location":"user-guide/data-processing.html#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"user-guide/data-processing.html#common-issues","title":"Common Issues","text":""},{"location":"user-guide/data-processing.html#1-encoding-problems","title":"1. Encoding Problems","text":"<pre><code># Handle encoding issues\nloader = DataLoader(\n    encoding=\"utf-8\",\n    fallback_encoding=\"latin-1\",\n    fix_encoding=True\n)\n</code></pre>"},{"location":"user-guide/data-processing.html#2-memory-issues-with-large-files","title":"2. Memory Issues with Large Files","text":"<pre><code># Process large files in chunks\nloader = DataLoader(chunk_size=1000000)  # 1MB chunks\ntexts = loader.load_file_chunked(\"huge_file.txt\")\n</code></pre>"},{"location":"user-guide/data-processing.html#3-poor-quality-extraction","title":"3. Poor Quality Extraction","text":"<pre><code># Adjust quality thresholds\nloader = DataLoader(\n    min_length=200,         # Longer minimum length\n    max_length=10000,       # Reasonable maximum\n    quality_threshold=0.7   # Higher quality threshold\n)\n</code></pre>"},{"location":"user-guide/data-processing.html#performance-optimization","title":"Performance Optimization","text":"<pre><code># Optimize for speed\nloader = DataLoader(\n    parallel_processing=True,\n    num_workers=8,\n    cache_processed=True,\n    batch_size=1000\n)\n</code></pre>"},{"location":"user-guide/data-processing.html#best-practices","title":"\ud83d\udcda Best Practices","text":""},{"location":"user-guide/data-processing.html#1-data-quality-over-quantity","title":"1. Data Quality Over Quantity","text":"<ul> <li>Clean data is more valuable than large amounts of noisy data</li> <li>Remove duplicates and near-duplicates</li> <li>Filter out low-quality content</li> </ul>"},{"location":"user-guide/data-processing.html#2-diverse-data-sources","title":"2. Diverse Data Sources","text":"<ul> <li>Use multiple document types and sources</li> <li>Include different writing styles and domains</li> <li>Balance formal and informal text</li> </ul>"},{"location":"user-guide/data-processing.html#3-preprocessing-consistency","title":"3. Preprocessing Consistency","text":"<ul> <li>Use the same preprocessing pipeline for training and inference</li> <li>Document your preprocessing steps</li> <li>Version your processed datasets</li> </ul>"},{"location":"user-guide/data-processing.html#4-memory-management","title":"4. Memory Management","text":"<ul> <li>Use streaming datasets for large corpora</li> <li>Process data in batches</li> <li>Clean up intermediate files</li> </ul>"},{"location":"user-guide/data-processing.html#5-quality-monitoring","title":"5. Quality Monitoring","text":"<ul> <li>Regularly assess data quality</li> <li>Monitor for data drift</li> <li>Keep preprocessing logs</li> </ul> <p>Data Processing Tips</p> <ul> <li>Start with a small sample to test your pipeline</li> <li>Always inspect your processed data manually</li> <li>Keep track of data sources and licenses</li> <li>Use version control for your preprocessing scripts</li> <li>Document any domain-specific preprocessing steps</li> </ul>"},{"location":"user-guide/export.html","title":"Model Export","text":"<p>Model export allows you to convert your trained LLMBuilder models into different formats for deployment, optimization, and compatibility with various inference engines. This guide covers all export options and deployment strategies.</p>"},{"location":"user-guide/export.html#export-overview","title":"\ud83c\udfaf Export Overview","text":"<p>LLMBuilder supports multiple export formats for different deployment scenarios:</p> <pre><code>graph TB\n    A[Trained Model] --&gt; B[Export Options]\n    B --&gt; C[GGUF Format]\n    B --&gt; D[ONNX Format]\n    B --&gt; E[Quantized Models]\n    B --&gt; F[HuggingFace Format]\n\n    C --&gt; C1[llama.cpp]\n    C --&gt; C2[Ollama]\n\n    D --&gt; D1[Mobile Apps]\n    D --&gt; D2[Edge Devices]\n\n    E --&gt; E1[8-bit Models]\n    E --&gt; E2[4-bit Models]\n\n    F --&gt; F1[Transformers]\n    F --&gt; F2[Model Hub]\n\n    style A fill:#e1f5fe\n    style C1 fill:#e8f5e8\n    style D1 fill:#fff3e0\n    style E1 fill:#f3e5f5\n    style F1 fill:#e0f2f1</code></pre>"},{"location":"user-guide/export.html#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"user-guide/export.html#cli-export","title":"CLI Export","text":"<pre><code># Export to GGUF format\nllmbuilder export gguf \\\n  ./model/model.pt \\\n  --output model.gguf \\\n  --quantization q4_0\n\n# Export to ONNX format\nllmbuilder export onnx \\\n  ./model/model.pt \\\n  --output model.onnx \\\n  --opset 11\n\n# Quantize model\nllmbuilder export quantize \\\n  ./model/model.pt \\\n  --output quantized_model.pt \\\n  --method dynamic \\\n  --bits 8\n</code></pre>"},{"location":"user-guide/export.html#python-api-export","title":"Python API Export","text":"<pre><code>from llmbuilder.export import export_gguf, export_onnx, quantize_model\n\n# Export to GGUF\nexport_gguf(\n    model_path=\"./model/model.pt\",\n    output_path=\"model.gguf\",\n    quantization=\"q4_0\"\n)\n\n# Export to ONNX\nexport_onnx(\n    model_path=\"./model/model.pt\",\n    output_path=\"model.onnx\",\n    opset_version=11\n)\n\n# Quantize model\nquantize_model(\n    model_path=\"./model/model.pt\",\n    output_path=\"quantized_model.pt\",\n    method=\"dynamic\",\n    bits=8\n)\n</code></pre>"},{"location":"user-guide/export.html#export-formats","title":"\ud83d\udce6 Export Formats","text":""},{"location":"user-guide/export.html#1-gguf-format","title":"1. GGUF Format","text":"<p>GGUF (GPT-Generated Unified Format) is optimized for CPU inference with llama.cpp:</p> <pre><code>from llmbuilder.export import GGUFExporter\n\nexporter = GGUFExporter(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\"\n)\n\n# Export with different quantization levels\nexporter.export(\n    output_path=\"model_f16.gguf\",\n    quantization=\"f16\"          # Full precision\n)\n\nexporter.export(\n    output_path=\"model_q8.gguf\",\n    quantization=\"q8_0\"         # 8-bit quantization\n)\n\nexporter.export(\n    output_path=\"model_q4.gguf\",\n    quantization=\"q4_0\"         # 4-bit quantization\n)\n</code></pre> <p>Quantization Options: - <code>f16</code>: 16-bit floating point (best quality, larger size) - <code>q8_0</code>: 8-bit quantization (good quality, medium size) - <code>q4_0</code>: 4-bit quantization (lower quality, smallest size) - <code>q4_1</code>: 4-bit with better quality - <code>q5_0</code>, <code>q5_1</code>: 5-bit quantization (balanced)</p>"},{"location":"user-guide/export.html#2-onnx-format","title":"2. ONNX Format","text":"<p>ONNX (Open Neural Network Exchange) for cross-platform deployment:</p> <pre><code>from llmbuilder.export import ONNXExporter\n\nexporter = ONNXExporter(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\"\n)\n\n# Export for different targets\nexporter.export(\n    output_path=\"model_cpu.onnx\",\n    target=\"cpu\",\n    opset_version=11,\n    optimize=True\n)\n\nexporter.export(\n    output_path=\"model_gpu.onnx\",\n    target=\"gpu\",\n    opset_version=14,\n    fp16=True\n)\n</code></pre> <p>ONNX Options: - <code>opset_version</code>: ONNX operator set version (11, 13, 14) - <code>target</code>: Target device (\"cpu\", \"gpu\", \"mobile\") - <code>optimize</code>: Apply ONNX optimizations - <code>fp16</code>: Use 16-bit precision for GPU</p>"},{"location":"user-guide/export.html#3-quantized-pytorch-models","title":"3. Quantized PyTorch Models","text":"<p>Quantize models while keeping PyTorch format:</p> <pre><code>from llmbuilder.export import PyTorchQuantizer\n\nquantizer = PyTorchQuantizer(\n    model_path=\"./model/model.pt\"\n)\n\n# Dynamic quantization (post-training)\nquantizer.dynamic_quantize(\n    output_path=\"model_dynamic_int8.pt\",\n    dtype=\"int8\"\n)\n\n# Static quantization (requires calibration data)\nquantizer.static_quantize(\n    output_path=\"model_static_int8.pt\",\n    calibration_data=\"calibration_data.txt\",\n    dtype=\"int8\"\n)\n\n# QAT (Quantization Aware Training)\nquantizer.qat_quantize(\n    output_path=\"model_qat_int8.pt\",\n    training_data=\"training_data.txt\",\n    epochs=3\n)\n</code></pre>"},{"location":"user-guide/export.html#4-huggingface-format","title":"4. HuggingFace Format","text":"<p>Export to HuggingFace Transformers format:</p> <pre><code>from llmbuilder.export import HuggingFaceExporter\n\nexporter = HuggingFaceExporter(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\"\n)\n\n# Export to HuggingFace format\nexporter.export(\n    output_dir=\"./huggingface_model\",\n    model_name=\"my-llm-model\",\n    push_to_hub=False,          # Set True to upload to Hub\n    private=False\n)\n</code></pre>"},{"location":"user-guide/export.html#export-configuration","title":"\u2699\ufe0f Export Configuration","text":""},{"location":"user-guide/export.html#advanced-export-settings","title":"Advanced Export Settings","text":"<pre><code>from llmbuilder.export import ExportConfig\n\nconfig = ExportConfig(\n    # Model settings\n    max_seq_length=2048,        # Maximum sequence length\n    vocab_size=32000,           # Vocabulary size\n\n    # Quantization settings\n    quantization_method=\"dynamic\",\n    calibration_samples=1000,\n    quantization_bits=8,\n\n    # Optimization settings\n    optimize_for_inference=True,\n    remove_unused_weights=True,\n    fuse_operations=True,\n\n    # Memory settings\n    memory_efficient=True,\n    low_cpu_mem_usage=True,\n\n    # Metadata\n    model_name=\"MyLLM\",\n    model_description=\"Custom language model\",\n    author=\"Your Name\",\n    license=\"MIT\"\n)\n</code></pre>"},{"location":"user-guide/export.html#deployment-scenarios","title":"\ud83d\ude80 Deployment Scenarios","text":""},{"location":"user-guide/export.html#1-cpu-inference-with-llamacpp","title":"1. CPU Inference with llama.cpp","text":"<pre><code># Export to GGUF\nllmbuilder export gguf ./model/model.pt --output model.gguf --quantization q4_0\n\n# Use with llama.cpp\n./llama.cpp/main -m model.gguf -p \"Hello, world!\" -n 100\n</code></pre>"},{"location":"user-guide/export.html#2-mobile-deployment-with-onnx","title":"2. Mobile Deployment with ONNX","text":"<pre><code># Export optimized for mobile\nfrom llmbuilder.export import export_onnx\n\nexport_onnx(\n    model_path=\"./model/model.pt\",\n    output_path=\"mobile_model.onnx\",\n    target=\"mobile\",\n    optimize=True,\n    fp16=True,\n    max_seq_length=512          # Shorter for mobile\n)\n</code></pre>"},{"location":"user-guide/export.html#3-cloud-deployment","title":"3. Cloud Deployment","text":"<pre><code># Export for cloud inference\nfrom llmbuilder.export import CloudExporter\n\nexporter = CloudExporter(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\"\n)\n\n# AWS SageMaker\nexporter.export_sagemaker(\n    output_dir=\"./sagemaker_model\",\n    instance_type=\"ml.g4dn.xlarge\"\n)\n\n# Google Cloud AI Platform\nexporter.export_vertex_ai(\n    output_dir=\"./vertex_model\",\n    machine_type=\"n1-standard-4\"\n)\n\n# Azure ML\nexporter.export_azure_ml(\n    output_dir=\"./azure_model\",\n    compute_target=\"gpu-cluster\"\n)\n</code></pre>"},{"location":"user-guide/export.html#4-edge-deployment","title":"4. Edge Deployment","text":"<pre><code># Export for edge devices\nfrom llmbuilder.export import EdgeExporter\n\nexporter = EdgeExporter(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\"\n)\n\n# TensorRT for NVIDIA devices\nexporter.export_tensorrt(\n    output_path=\"model.trt\",\n    precision=\"fp16\",\n    max_batch_size=1\n)\n\n# CoreML for Apple devices\nexporter.export_coreml(\n    output_path=\"model.mlmodel\",\n    target=\"iOS15\"\n)\n\n# TensorFlow Lite for mobile\nexporter.export_tflite(\n    output_path=\"model.tflite\",\n    quantize=True\n)\n</code></pre>"},{"location":"user-guide/export.html#export-optimization","title":"\ud83d\udcca Export Optimization","text":""},{"location":"user-guide/export.html#model-size-optimization","title":"Model Size Optimization","text":"<pre><code>from llmbuilder.export import optimize_model_size\n\n# Remove unused parameters\noptimized_model = optimize_model_size(\n    model_path=\"./model/model.pt\",\n    remove_unused=True,\n    prune_weights=0.1,          # Prune 10% of smallest weights\n    merge_layers=True           # Merge compatible layers\n)\n\n# Compare sizes\noriginal_size = get_model_size(\"./model/model.pt\")\noptimized_size = get_model_size(optimized_model)\nprint(f\"Size reduction: {(1 - optimized_size/original_size)*100:.1f}%\")\n</code></pre>"},{"location":"user-guide/export.html#inference-speed-optimization","title":"Inference Speed Optimization","text":"<pre><code>from llmbuilder.export import optimize_inference_speed\n\n# Optimize for speed\nspeed_optimized = optimize_inference_speed(\n    model_path=\"./model/model.pt\",\n    target_device=\"cpu\",\n    batch_size=1,\n    sequence_length=512,\n    fuse_attention=True,\n    use_flash_attention=False   # CPU doesn't support flash attention\n)\n</code></pre>"},{"location":"user-guide/export.html#memory-usage-optimization","title":"Memory Usage Optimization","text":"<pre><code>from llmbuilder.export import optimize_memory_usage\n\n# Optimize for memory\nmemory_optimized = optimize_memory_usage(\n    model_path=\"./model/model.pt\",\n    gradient_checkpointing=True,\n    activation_checkpointing=True,\n    weight_sharing=True,\n    low_memory_mode=True\n)\n</code></pre>"},{"location":"user-guide/export.html#export-validation","title":"\ud83d\udd0d Export Validation","text":""},{"location":"user-guide/export.html#validate-exported-models","title":"Validate Exported Models","text":"<pre><code>from llmbuilder.export import validate_export\n\n# Validate GGUF export\ngguf_validation = validate_export(\n    original_model=\"./model/model.pt\",\n    exported_model=\"model.gguf\",\n    format=\"gguf\",\n    test_prompts=[\"Hello world\", \"The future of AI\"],\n    tolerance=0.01              # Acceptable difference in outputs\n)\n\nprint(f\"GGUF validation passed: {gguf_validation.passed}\")\nprint(f\"Average difference: {gguf_validation.avg_difference:.4f}\")\n\n# Validate ONNX export\nonnx_validation = validate_export(\n    original_model=\"./model/model.pt\",\n    exported_model=\"model.onnx\",\n    format=\"onnx\",\n    test_prompts=[\"Hello world\", \"The future of AI\"]\n)\n</code></pre>"},{"location":"user-guide/export.html#performance-benchmarking","title":"Performance Benchmarking","text":"<pre><code>from llmbuilder.export import benchmark_models\n\n# Compare performance across formats\nresults = benchmark_models([\n    (\"Original PyTorch\", \"./model/model.pt\"),\n    (\"GGUF Q4\", \"model_q4.gguf\"),\n    (\"ONNX\", \"model.onnx\"),\n    (\"Quantized PyTorch\", \"model_int8.pt\")\n], test_prompts=[\"Benchmark prompt\"] * 100)\n\nfor name, metrics in results.items():\n    print(f\"{name}:\")\n    print(f\"  Tokens/sec: {metrics.tokens_per_second:.1f}\")\n    print(f\"  Memory usage: {metrics.memory_mb:.1f} MB\")\n    print(f\"  Model size: {metrics.model_size_mb:.1f} MB\")\n    print(f\"  Latency: {metrics.avg_latency_ms:.1f} ms\")\n</code></pre>"},{"location":"user-guide/export.html#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"user-guide/export.html#common-export-issues","title":"Common Export Issues","text":""},{"location":"user-guide/export.html#gguf-export-fails","title":"GGUF Export Fails","text":"<pre><code># Solution: Check model compatibility\nfrom llmbuilder.export import check_gguf_compatibility\n\ncompatibility = check_gguf_compatibility(\"./model/model.pt\")\nif not compatibility.compatible:\n    print(f\"Issues: {compatibility.issues}\")\n    # Fix issues or use alternative export\n</code></pre>"},{"location":"user-guide/export.html#onnx-export-errors","title":"ONNX Export Errors","text":"<pre><code># Solution: Use compatible operations\nfrom llmbuilder.export import fix_onnx_compatibility\n\nfixed_model = fix_onnx_compatibility(\n    model_path=\"./model/model.pt\",\n    target_opset=11,\n    replace_unsupported=True\n)\n</code></pre>"},{"location":"user-guide/export.html#quantization-quality-loss","title":"Quantization Quality Loss","text":"<pre><code># Solution: Use higher precision or calibration\nconfig = ExportConfig(\n    quantization_method=\"static\",  # Better than dynamic\n    calibration_samples=5000,      # More calibration data\n    quantization_bits=8,           # Higher precision\n    preserve_accuracy=True\n)\n</code></pre>"},{"location":"user-guide/export.html#performance-issues","title":"Performance Issues","text":""},{"location":"user-guide/export.html#slow-inference","title":"Slow Inference","text":"<pre><code># Solution: Optimize for target hardware\nexport_config = ExportConfig(\n    optimize_for_inference=True,\n    target_device=\"cpu\",           # or \"gpu\"\n    batch_size=1,                  # Optimize for single inference\n    fuse_operations=True,\n    use_optimized_kernels=True\n)\n</code></pre>"},{"location":"user-guide/export.html#high-memory-usage","title":"High Memory Usage","text":"<pre><code># Solution: Use memory-efficient formats\nexport_config = ExportConfig(\n    quantization_method=\"dynamic\",\n    quantization_bits=8,\n    memory_efficient=True,\n    streaming_weights=True         # Load weights on demand\n)\n</code></pre>"},{"location":"user-guide/export.html#best-practices","title":"\ud83d\udcda Best Practices","text":""},{"location":"user-guide/export.html#1-choose-the-right-format","title":"1. Choose the Right Format","text":"<ul> <li>GGUF: CPU inference, llama.cpp compatibility</li> <li>ONNX: Cross-platform, mobile deployment</li> <li>Quantized PyTorch: PyTorch ecosystem, balanced performance</li> <li>HuggingFace: Easy sharing, transformers compatibility</li> </ul>"},{"location":"user-guide/export.html#2-quantization-strategy","title":"2. Quantization Strategy","text":"<ul> <li>Start with dynamic quantization for quick wins</li> <li>Use static quantization for better quality</li> <li>Consider QAT for maximum quality retention</li> <li>Test different bit widths (4, 8, 16)</li> </ul>"},{"location":"user-guide/export.html#3-validation-and-testing","title":"3. Validation and Testing","text":"<ul> <li>Always validate exported models</li> <li>Test on representative data</li> <li>Benchmark performance vs. quality trade-offs</li> <li>Verify compatibility with target deployment</li> </ul>"},{"location":"user-guide/export.html#4-deployment-considerations","title":"4. Deployment Considerations","text":"<ul> <li>Consider target hardware capabilities</li> <li>Plan for model updates and versioning</li> <li>Monitor inference performance in production</li> <li>Have fallback options for compatibility issues</li> </ul> <p>Export Tips</p> <ul> <li>Always validate exported models before deployment</li> <li>Start with higher precision and reduce if needed</li> <li>Consider the trade-off between model size and quality</li> <li>Test on your target hardware before production deployment</li> <li>Keep the original model for future re-exports with different settings</li> </ul>"},{"location":"user-guide/fine-tuning.html","title":"Fine-tuning","text":"<p>Fine-tuning allows you to adapt pre-trained models to specific domains or tasks. LLMBuilder provides comprehensive fine-tuning capabilities including LoRA, full parameter fine-tuning, and domain adaptation.</p>"},{"location":"user-guide/fine-tuning.html#fine-tuning-overview","title":"\ud83c\udfaf Fine-tuning Overview","text":"<p>Fine-tuning is the process of taking a pre-trained model and adapting it to your specific use case:</p> <pre><code>graph LR\n    A[Pre-trained Model] --&gt; B[Fine-tuning Data]\n    B --&gt; C[Fine-tuning Process]\n    C --&gt; D[Adapted Model]\n\n    C --&gt; C1[LoRA]\n    C --&gt; C2[Full Fine-tuning]\n    C --&gt; C3[Domain Adaptation]\n\n    style A fill:#e1f5fe\n    style D fill:#e8f5e8</code></pre>"},{"location":"user-guide/fine-tuning.html#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"user-guide/fine-tuning.html#cli-fine-tuning","title":"CLI Fine-tuning","text":"<pre><code>llmbuilder finetune model \\\n  --model ./pretrained_model/model.pt \\\n  --dataset domain_data.txt \\\n  --output ./finetuned_model \\\n  --epochs 5 \\\n  --lr 5e-5 \\\n  --use-lora\n</code></pre>"},{"location":"user-guide/fine-tuning.html#python-api-fine-tuning","title":"Python API Fine-tuning","text":"<pre><code>import llmbuilder as lb\nfrom llmbuilder.finetune import FineTuningConfig\n\n# Load pre-trained model\nmodel = lb.load_model(\"./pretrained_model/model.pt\")\n\n# Prepare fine-tuning dataset\nfrom llmbuilder.data import TextDataset\ndataset = TextDataset(\"domain_data.txt\", block_size=1024)\n\n# Configure fine-tuning\nconfig = FineTuningConfig(\n    num_epochs=5,\n    learning_rate=5e-5,\n    use_lora=True,\n    lora_rank=16\n)\n\n# Fine-tune model\nresults = lb.finetune_model(model, dataset, config)\n</code></pre>"},{"location":"user-guide/fine-tuning.html#fine-tuning-methods","title":"\ud83d\udd27 Fine-tuning Methods","text":""},{"location":"user-guide/fine-tuning.html#1-lora-low-rank-adaptation","title":"1. LoRA (Low-Rank Adaptation)","text":"<p>LoRA is efficient and requires less memory:</p> <pre><code>from llmbuilder.finetune import LoRAConfig\n\nlora_config = LoRAConfig(\n    rank=16,                    # LoRA rank (4, 8, 16, 32)\n    alpha=32,                   # LoRA alpha (usually 2x rank)\n    dropout=0.1,                # LoRA dropout\n    target_modules=[            # Which modules to adapt\n        \"attention.query\",\n        \"attention.key\", \n        \"attention.value\",\n        \"mlp.dense\"\n    ]\n)\n\nconfig = FineTuningConfig(\n    use_lora=True,\n    lora_config=lora_config,\n    learning_rate=1e-4,\n    num_epochs=10\n)\n</code></pre> <p>Advantages: - Memory efficient (only ~1% of parameters) - Fast training - Easy to merge back to base model - Multiple adapters can be trained</p>"},{"location":"user-guide/fine-tuning.html#2-full-parameter-fine-tuning","title":"2. Full Parameter Fine-tuning","text":"<p>Fine-tune all model parameters:</p> <pre><code>config = FineTuningConfig(\n    use_lora=False,             # Full fine-tuning\n    learning_rate=1e-5,         # Lower LR for stability\n    num_epochs=3,               # Fewer epochs needed\n    weight_decay=0.01,          # Regularization\n    warmup_steps=100\n)\n</code></pre> <p>Advantages: - Maximum adaptation capability - Better performance on very different domains - Full model customization</p> <p>Disadvantages: - Requires more memory - Slower training - Risk of catastrophic forgetting</p>"},{"location":"user-guide/fine-tuning.html#3-domain-adaptation","title":"3. Domain Adaptation","text":"<p>Adapt to specific domains while preserving general capabilities:</p> <pre><code>config = FineTuningConfig(\n    adaptation_method=\"domain\",\n    domain_weight=0.7,          # Balance between domain and general\n    regularization_strength=0.1, # Prevent forgetting\n    learning_rate=3e-5,\n    num_epochs=8\n)\n</code></pre>"},{"location":"user-guide/fine-tuning.html#fine-tuning-configuration","title":"\ud83d\udcca Fine-tuning Configuration","text":""},{"location":"user-guide/fine-tuning.html#basic-configuration","title":"Basic Configuration","text":"<pre><code>from llmbuilder.finetune import FineTuningConfig\n\nconfig = FineTuningConfig(\n    # Training parameters\n    num_epochs=5,               # Usually fewer than pre-training\n    learning_rate=5e-5,         # Lower than pre-training\n    batch_size=8,               # Often smaller due to memory\n\n    # Fine-tuning specific\n    use_lora=True,              # Use LoRA for efficiency\n    freeze_embeddings=False,    # Whether to freeze embeddings\n    freeze_layers=0,            # Number of layers to freeze\n\n    # Regularization\n    weight_decay=0.01,\n    dropout_rate=0.1,\n\n    # Evaluation\n    eval_every=100,\n    save_every=500,\n    early_stopping_patience=3\n)\n</code></pre>"},{"location":"user-guide/fine-tuning.html#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>config = FineTuningConfig(\n    # LoRA settings\n    lora_rank=16,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    lora_target_modules=[\"attention\", \"mlp\"],\n\n    # Learning rate scheduling\n    scheduler=\"cosine\",\n    warmup_ratio=0.1,\n    min_lr_ratio=0.1,\n\n    # Data settings\n    max_seq_length=1024,\n    data_collator=\"default\",\n\n    # Optimization\n    optimizer=\"adamw\",\n    gradient_accumulation_steps=4,\n    max_grad_norm=1.0,\n\n    # Memory optimization\n    gradient_checkpointing=True,\n    dataloader_pin_memory=True,\n    dataloader_num_workers=4\n)\n</code></pre>"},{"location":"user-guide/fine-tuning.html#fine-tuning-strategies","title":"\ud83c\udfaf Fine-tuning Strategies","text":""},{"location":"user-guide/fine-tuning.html#1-task-specific-fine-tuning","title":"1. Task-Specific Fine-tuning","text":"<p>For specific tasks like code generation, question answering, etc.:</p> <pre><code># Prepare task-specific data\ntask_data = \"\"\"\nQuestion: What is machine learning?\nAnswer: Machine learning is a subset of artificial intelligence...\n\nQuestion: How do neural networks work?\nAnswer: Neural networks are computational models inspired by...\n\"\"\"\n\n# Configure for task adaptation\nconfig = FineTuningConfig(\n    use_lora=True,\n    lora_rank=32,               # Higher rank for complex tasks\n    learning_rate=1e-4,\n    num_epochs=10,\n    task_type=\"question_answering\"\n)\n</code></pre>"},{"location":"user-guide/fine-tuning.html#2-domain-specific-fine-tuning","title":"2. Domain-Specific Fine-tuning","text":"<p>For specific domains like medical, legal, scientific:</p> <pre><code># Medical domain example\nconfig = FineTuningConfig(\n    use_lora=True,\n    lora_rank=16,\n    learning_rate=5e-5,\n    num_epochs=15,\n    domain=\"medical\",\n    preserve_general_knowledge=True,  # Prevent catastrophic forgetting\n    domain_weight=0.8\n)\n</code></pre>"},{"location":"user-guide/fine-tuning.html#3-style-fine-tuning","title":"3. Style Fine-tuning","text":"<p>For specific writing styles or formats:</p> <pre><code># Creative writing style\nconfig = FineTuningConfig(\n    use_lora=True,\n    lora_rank=8,                # Lower rank for style changes\n    learning_rate=3e-5,\n    num_epochs=5,\n    style_adaptation=True,\n    preserve_factual_knowledge=True\n)\n</code></pre>"},{"location":"user-guide/fine-tuning.html#monitoring-fine-tuning","title":"\ud83d\udcc8 Monitoring Fine-tuning","text":""},{"location":"user-guide/fine-tuning.html#training-metrics","title":"Training Metrics","text":"<pre><code>from llmbuilder.finetune import FineTuner\n\ntrainer = FineTuner(\n    model=model,\n    train_dataset=train_dataset,\n    val_dataset=val_dataset,\n    config=config\n)\n\n# Train with monitoring\nresults = trainer.train()\n\n# Check results\nprint(f\"Base model perplexity: {results.base_perplexity:.2f}\")\nprint(f\"Fine-tuned perplexity: {results.final_perplexity:.2f}\")\nprint(f\"Improvement: {results.improvement_percentage:.1f}%\")\nprint(f\"Training time: {results.training_time}\")\n</code></pre>"},{"location":"user-guide/fine-tuning.html#evaluation-metrics","title":"Evaluation Metrics","text":"<pre><code># Evaluate on validation set\neval_results = trainer.evaluate(val_dataset)\n\nprint(f\"Validation loss: {eval_results.loss:.4f}\")\nprint(f\"Perplexity: {eval_results.perplexity:.2f}\")\nprint(f\"BLEU score: {eval_results.bleu_score:.3f}\")\nprint(f\"Task accuracy: {eval_results.task_accuracy:.3f}\")\n</code></pre>"},{"location":"user-guide/fine-tuning.html#advanced-techniques","title":"\ud83d\udd04 Advanced Techniques","text":""},{"location":"user-guide/fine-tuning.html#progressive-fine-tuning","title":"Progressive Fine-tuning","text":"<p>Gradually unfreeze layers during training:</p> <pre><code>config = FineTuningConfig(\n    progressive_unfreezing=True,\n    unfreeze_schedule=[\n        (0, 2),     # Epochs 0-2: freeze all but last 2 layers\n        (3, 4),     # Epochs 3-4: freeze all but last 4 layers\n        (5, -1),    # Epochs 5+: unfreeze all layers\n    ],\n    learning_rate_schedule=[\n        (0, 1e-5),  # Lower LR for frozen layers\n        (3, 3e-5),  # Medium LR for partial unfreezing\n        (5, 5e-5),  # Higher LR for full unfreezing\n    ]\n)\n</code></pre>"},{"location":"user-guide/fine-tuning.html#multi-task-fine-tuning","title":"Multi-task Fine-tuning","text":"<p>Fine-tune on multiple tasks simultaneously:</p> <pre><code>from llmbuilder.data import MultiTaskDataset\n\n# Prepare multi-task dataset\ndataset = MultiTaskDataset([\n    (\"qa\", qa_dataset, 0.4),        # 40% question answering\n    (\"summarization\", sum_dataset, 0.3),  # 30% summarization\n    (\"generation\", gen_dataset, 0.3),     # 30% text generation\n])\n\nconfig = FineTuningConfig(\n    multi_task=True,\n    task_weights={\"qa\": 1.0, \"summarization\": 0.8, \"generation\": 1.2},\n    shared_encoder=True,\n    task_specific_heads=True\n)\n</code></pre>"},{"location":"user-guide/fine-tuning.html#continual-learning","title":"Continual Learning","text":"<p>Fine-tune while preventing catastrophic forgetting:</p> <pre><code>config = FineTuningConfig(\n    continual_learning=True,\n    regularization_method=\"ewc\",    # Elastic Weight Consolidation\n    regularization_strength=1000,\n    memory_buffer_size=1000,        # Store important examples\n    replay_frequency=0.1            # 10% replay during training\n)\n</code></pre>"},{"location":"user-guide/fine-tuning.html#specialized-fine-tuning","title":"\ud83c\udfa8 Specialized Fine-tuning","text":""},{"location":"user-guide/fine-tuning.html#code-generation-fine-tuning","title":"Code Generation Fine-tuning","text":"<pre><code># Configure for code generation\nconfig = FineTuningConfig(\n    use_lora=True,\n    lora_rank=32,\n    target_modules=[\"attention\", \"mlp\"],\n    learning_rate=1e-4,\n    num_epochs=8,\n\n    # Code-specific settings\n    max_seq_length=2048,        # Longer sequences for code\n    special_tokens=[\"&lt;code&gt;\", \"&lt;/code&gt;\", \"&lt;comment&gt;\"],\n    code_specific_loss=True,\n    syntax_aware_training=True\n)\n</code></pre>"},{"location":"user-guide/fine-tuning.html#conversational-fine-tuning","title":"Conversational Fine-tuning","text":"<pre><code># Configure for chat/dialogue\nconfig = FineTuningConfig(\n    use_lora=True,\n    lora_rank=16,\n    learning_rate=5e-5,\n    num_epochs=6,\n\n    # Conversation-specific\n    dialogue_format=True,\n    turn_separator=\"&lt;turn&gt;\",\n    response_loss_only=True,    # Only compute loss on responses\n    max_turns=10\n)\n</code></pre>"},{"location":"user-guide/fine-tuning.html#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"user-guide/fine-tuning.html#common-issues","title":"Common Issues","text":""},{"location":"user-guide/fine-tuning.html#catastrophic-forgetting","title":"Catastrophic Forgetting","text":"<pre><code># Solution: Use regularization\nconfig = FineTuningConfig(\n    regularization_method=\"l2\",\n    regularization_strength=0.01,\n    preserve_base_capabilities=True,\n    validation_on_base_tasks=True\n)\n</code></pre>"},{"location":"user-guide/fine-tuning.html#overfitting","title":"Overfitting","text":"<pre><code># Solution: Reduce learning rate and add regularization\nconfig = FineTuningConfig(\n    learning_rate=1e-5,         # Lower LR\n    weight_decay=0.1,           # Higher weight decay\n    dropout_rate=0.2,           # Higher dropout\n    early_stopping_patience=2,  # Early stopping\n    validation_split=0.2        # More validation data\n)\n</code></pre>"},{"location":"user-guide/fine-tuning.html#poor-task-performance","title":"Poor Task Performance","text":"<pre><code># Solution: Increase adaptation capacity\nconfig = FineTuningConfig(\n    lora_rank=64,               # Higher LoRA rank\n    lora_alpha=128,             # Higher alpha\n    learning_rate=1e-4,         # Higher LR\n    num_epochs=15,              # More epochs\n    target_modules=\"all\"        # Adapt more modules\n)\n</code></pre>"},{"location":"user-guide/fine-tuning.html#best-practices","title":"\ud83d\udcda Best Practices","text":""},{"location":"user-guide/fine-tuning.html#1-data-preparation","title":"1. Data Preparation","text":"<ul> <li>Use high-quality, task-relevant data</li> <li>Balance dataset sizes across tasks</li> <li>Include diverse examples</li> <li>Validate data format and quality</li> </ul>"},{"location":"user-guide/fine-tuning.html#2-hyperparameter-selection","title":"2. Hyperparameter Selection","text":"<ul> <li>Start with lower learning rates (1e-5 to 1e-4)</li> <li>Use fewer epochs than pre-training (3-10)</li> <li>Choose appropriate LoRA rank (8-32)</li> <li>Monitor validation metrics closely</li> </ul>"},{"location":"user-guide/fine-tuning.html#3-evaluation-strategy","title":"3. Evaluation Strategy","text":"<ul> <li>Evaluate on both task-specific and general metrics</li> <li>Test for catastrophic forgetting</li> <li>Use held-out test sets</li> <li>Compare against base model performance</li> </ul>"},{"location":"user-guide/fine-tuning.html#4-resource-management","title":"4. Resource Management","text":"<ul> <li>Use LoRA for memory efficiency</li> <li>Enable gradient checkpointing if needed</li> <li>Monitor GPU memory usage</li> <li>Save checkpoints frequently</li> </ul> <p>Fine-tuning Tips</p> <ul> <li>Always start with a smaller learning rate than pre-training</li> <li>Use LoRA for most fine-tuning tasks unless you need maximum adaptation</li> <li>Monitor both task performance and general capabilities</li> <li>Save multiple checkpoints to find the best stopping point</li> <li>Test your fine-tuned model thoroughly before deployment</li> </ul>"},{"location":"user-guide/generation.html","title":"Text Generation","text":"<p>Text generation is where your trained language model comes to life. LLMBuilder provides powerful and flexible text generation capabilities with various sampling strategies, interactive modes, and customization options.</p>"},{"location":"user-guide/generation.html#generation-overview","title":"\ud83c\udfaf Generation Overview","text":"<p>Text generation transforms your trained model into a creative writing assistant:</p> <pre><code>graph LR\n    A[Prompt] --&gt; B[Tokenizer]\n    B --&gt; C[Model]\n    C --&gt; D[Sampling Strategy]\n    D --&gt; E[Generated Tokens]\n    E --&gt; F[Detokenizer]\n    F --&gt; G[Generated Text]\n\n    D --&gt; D1[Greedy]\n    D --&gt; D2[Top-k]\n    D --&gt; D3[Top-p]\n    D --&gt; D4[Temperature]\n\n    style A fill:#e1f5fe\n    style G fill:#e8f5e8</code></pre>"},{"location":"user-guide/generation.html#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"user-guide/generation.html#cli-generation","title":"CLI Generation","text":"<pre><code># Interactive generation setup\nllmbuilder generate text --setup\n\n# Direct generation\nllmbuilder generate text \\\n  --model ./model/model.pt \\\n  --tokenizer ./tokenizer \\\n  --prompt \"The future of AI is\" \\\n  --max-tokens 100 \\\n  --temperature 0.8\n\n# Interactive chat mode\nllmbuilder generate text \\\n  --model ./model/model.pt \\\n  --tokenizer ./tokenizer \\\n  --interactive\n</code></pre>"},{"location":"user-guide/generation.html#python-api-generation","title":"Python API Generation","text":"<pre><code>import llmbuilder as lb\n\n# Simple generation\ntext = lb.generate_text(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    prompt=\"The future of AI is\",\n    max_new_tokens=100,\n    temperature=0.8\n)\nprint(text)\n\n# Interactive generation\nlb.interactive_cli(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    temperature=0.8\n)\n</code></pre>"},{"location":"user-guide/generation.html#generation-parameters","title":"\u2699\ufe0f Generation Parameters","text":""},{"location":"user-guide/generation.html#core-parameters","title":"Core Parameters","text":"<pre><code>from llmbuilder.inference import GenerationConfig\n\nconfig = GenerationConfig(\n    # Length control\n    max_new_tokens=100,         # Maximum tokens to generate\n    min_new_tokens=10,          # Minimum tokens to generate\n    max_length=1024,            # Total sequence length limit\n\n    # Sampling parameters\n    temperature=0.8,            # Creativity (0.1-2.0)\n    top_k=50,                   # Top-k sampling\n    top_p=0.9,                  # Nucleus sampling\n    repetition_penalty=1.1,     # Prevent repetition\n\n    # Special tokens\n    pad_token_id=0,\n    eos_token_id=2,\n    bos_token_id=1,\n\n    # Generation strategy\n    do_sample=True,             # Use sampling vs greedy\n    num_beams=1,                # Beam search width\n    early_stopping=True         # Stop at EOS token\n)\n</code></pre>"},{"location":"user-guide/generation.html#advanced-parameters","title":"Advanced Parameters","text":"<pre><code>config = GenerationConfig(\n    # Advanced sampling\n    typical_p=0.95,             # Typical sampling\n    eta_cutoff=1e-4,            # Eta sampling cutoff\n    epsilon_cutoff=1e-4,        # Epsilon sampling cutoff\n\n    # Repetition control\n    repetition_penalty=1.1,\n    no_repeat_ngram_size=3,     # Prevent n-gram repetition\n    encoder_repetition_penalty=1.0,\n\n    # Length penalties\n    length_penalty=1.0,         # Beam search length penalty\n    exponential_decay_length_penalty=None,\n\n    # Diversity\n    num_beam_groups=1,          # Diverse beam search\n    diversity_penalty=0.0,\n\n    # Stopping criteria\n    max_time=None,              # Maximum generation time\n    stop_strings=[\"&lt;/s&gt;\", \"\\n\\n\"],  # Custom stop strings\n)\n</code></pre>"},{"location":"user-guide/generation.html#sampling-strategies","title":"\ud83c\udfa8 Sampling Strategies","text":""},{"location":"user-guide/generation.html#1-greedy-decoding","title":"1. Greedy Decoding","text":"<p>Always choose the most likely token:</p> <pre><code>config = GenerationConfig(\n    do_sample=False,            # Disable sampling\n    temperature=1.0,            # Not used in greedy\n    top_k=None,                 # Not used in greedy\n    top_p=None                  # Not used in greedy\n)\n\ntext = lb.generate_text(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    prompt=\"Machine learning is\",\n    config=config\n)\n</code></pre> <p>Use cases: - Deterministic output needed - Factual question answering - Code generation - Translation tasks</p>"},{"location":"user-guide/generation.html#2-temperature-sampling","title":"2. Temperature Sampling","text":"<p>Control randomness with temperature:</p> <pre><code># Conservative (more predictable)\nconservative_config = GenerationConfig(\n    temperature=0.3,            # Low temperature\n    do_sample=True\n)\n\n# Balanced\nbalanced_config = GenerationConfig(\n    temperature=0.8,            # Medium temperature\n    do_sample=True\n)\n\n# Creative (more diverse)\ncreative_config = GenerationConfig(\n    temperature=1.5,            # High temperature\n    do_sample=True\n)\n</code></pre> <p>Temperature effects: - 0.1-0.3: Very focused, predictable - 0.5-0.8: Balanced creativity - 1.0-1.5: More creative, diverse - 1.5+: Very creative, potentially incoherent</p>"},{"location":"user-guide/generation.html#3-top-k-sampling","title":"3. Top-k Sampling","text":"<p>Sample from top k most likely tokens:</p> <pre><code>config = GenerationConfig(\n    do_sample=True,\n    temperature=0.8,\n    top_k=40,                   # Consider top 40 tokens\n    top_p=None                  # Disable nucleus sampling\n)\n</code></pre> <p>Top-k values: - 1: Greedy decoding - 10-20: Conservative sampling - 40-100: Balanced sampling - 200+: Very diverse sampling</p>"},{"location":"user-guide/generation.html#4-top-p-nucleus-sampling","title":"4. Top-p (Nucleus) Sampling","text":"<p>Sample from tokens that make up top p probability mass:</p> <pre><code>config = GenerationConfig(\n    do_sample=True,\n    temperature=0.8,\n    top_k=None,                 # Disable top-k\n    top_p=0.9                   # Use top 90% probability mass\n)\n</code></pre> <p>Top-p values: - 0.1-0.3: Very focused - 0.5-0.7: Balanced - 0.8-0.95: Diverse - 0.95+: Very diverse</p>"},{"location":"user-guide/generation.html#5-combined-sampling","title":"5. Combined Sampling","text":"<p>Combine multiple strategies:</p> <pre><code>config = GenerationConfig(\n    do_sample=True,\n    temperature=0.8,            # Add randomness\n    top_k=50,                   # Limit to top 50 tokens\n    top_p=0.9,                  # Within 90% probability mass\n    repetition_penalty=1.1      # Reduce repetition\n)\n</code></pre>"},{"location":"user-guide/generation.html#generation-modes","title":"\ud83c\udfaf Generation Modes","text":""},{"location":"user-guide/generation.html#1-single-generation","title":"1. Single Generation","text":"<p>Generate one response to a prompt:</p> <pre><code>response = lb.generate_text(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    prompt=\"Explain quantum computing in simple terms:\",\n    max_new_tokens=200,\n    temperature=0.7\n)\n</code></pre>"},{"location":"user-guide/generation.html#2-batch-generation","title":"2. Batch Generation","text":"<p>Generate multiple responses:</p> <pre><code>from llmbuilder.inference import batch_generate\n\nprompts = [\n    \"The benefits of renewable energy are\",\n    \"Artificial intelligence will help us\",\n    \"The future of space exploration includes\"\n]\n\nresponses = batch_generate(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    prompts=prompts,\n    max_new_tokens=100,\n    temperature=0.8,\n    batch_size=8\n)\n\nfor prompt, response in zip(prompts, responses):\n    print(f\"Prompt: {prompt}\")\n    print(f\"Response: {response}\\n\")\n</code></pre>"},{"location":"user-guide/generation.html#3-interactive-generation","title":"3. Interactive Generation","text":"<p>Real-time conversation mode:</p> <pre><code>from llmbuilder.inference import InteractiveGenerator\n\ngenerator = InteractiveGenerator(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    config=GenerationConfig(\n        temperature=0.8,\n        top_k=50,\n        max_new_tokens=150\n    )\n)\n\n# Start interactive session\ngenerator.start_session()\n\n# Or use in code\nwhile True:\n    prompt = input(\"You: \")\n    if prompt.lower() == 'quit':\n        break\n\n    response = generator.generate(prompt)\n    print(f\"AI: {response}\")\n</code></pre>"},{"location":"user-guide/generation.html#4-streaming-generation","title":"4. Streaming Generation","text":"<p>Generate text token by token:</p> <pre><code>from llmbuilder.inference import stream_generate\n\nfor token in stream_generate(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    prompt=\"The history of artificial intelligence\",\n    max_new_tokens=200,\n    temperature=0.8\n):\n    print(token, end='', flush=True)\n</code></pre>"},{"location":"user-guide/generation.html#advanced-generation-features","title":"\ud83d\udd27 Advanced Generation Features","text":""},{"location":"user-guide/generation.html#1-prompt-engineering","title":"1. Prompt Engineering","text":"<p>Optimize prompts for better results:</p> <pre><code># System prompt + user prompt\nsystem_prompt = \"You are a helpful AI assistant that provides accurate and concise answers.\"\nuser_prompt = \"Explain machine learning in simple terms.\"\n\nfull_prompt = f\"System: {system_prompt}\\nUser: {user_prompt}\\nAssistant:\"\n\nresponse = lb.generate_text(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    prompt=full_prompt,\n    max_new_tokens=200\n)\n</code></pre>"},{"location":"user-guide/generation.html#2-few-shot-learning","title":"2. Few-shot Learning","text":"<p>Provide examples in the prompt:</p> <pre><code>few_shot_prompt = \"\"\"\nTranslate English to French:\n\nEnglish: Hello, how are you?\nFrench: Bonjour, comment allez-vous?\n\nEnglish: What is your name?\nFrench: Comment vous appelez-vous?\n\nEnglish: I love programming.\nFrench:\"\"\"\n\nresponse = lb.generate_text(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    prompt=few_shot_prompt,\n    max_new_tokens=50,\n    temperature=0.3  # Lower temperature for translation\n)\n</code></pre>"},{"location":"user-guide/generation.html#3-constrained-generation","title":"3. Constrained Generation","text":"<p>Generate text with constraints:</p> <pre><code>from llmbuilder.inference import ConstrainedGenerator\n\n# Generate text that must contain certain words\ngenerator = ConstrainedGenerator(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    required_words=[\"machine learning\", \"neural networks\", \"data\"],\n    forbidden_words=[\"impossible\", \"never\"],\n    max_length=200\n)\n\nresponse = generator.generate(\"Explain AI technology:\")\n</code></pre>"},{"location":"user-guide/generation.html#4-format-specific-generation","title":"4. Format-Specific Generation","text":"<p>Generate structured output:</p> <pre><code># JSON generation\njson_prompt = \"\"\"Generate a JSON object describing a person:\n{\n  \"name\": \"John Smith\",\n  \"age\": 30,\n  \"occupation\": \"Software Engineer\",\n  \"skills\": [\"Python\", \"JavaScript\", \"Machine Learning\"]\n}\n\nGenerate a similar JSON for a data scientist:\n{\"\"\"\n\nresponse = lb.generate_text(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    prompt=json_prompt,\n    max_new_tokens=150,\n    temperature=0.3,\n    stop_strings=[\"}\"]\n)\n\n# Add closing brace\ncomplete_json = response + \"}\"\n</code></pre>"},{"location":"user-guide/generation.html#generation-quality-control","title":"\ud83d\udcca Generation Quality Control","text":""},{"location":"user-guide/generation.html#1-output-filtering","title":"1. Output Filtering","text":"<p>Filter generated content:</p> <pre><code>from llmbuilder.inference import OutputFilter\n\nfilter_config = {\n    \"min_length\": 20,           # Minimum response length\n    \"max_repetition\": 0.3,      # Maximum repetition ratio\n    \"profanity_filter\": True,   # Filter inappropriate content\n    \"coherence_threshold\": 0.7, # Minimum coherence score\n    \"factuality_check\": True    # Basic fact checking\n}\n\nfiltered_response = OutputFilter.filter(response, filter_config)\n</code></pre>"},{"location":"user-guide/generation.html#2-quality-metrics","title":"2. Quality Metrics","text":"<p>Evaluate generation quality:</p> <pre><code>from llmbuilder.inference import evaluate_generation\n\nmetrics = evaluate_generation(\n    generated_text=response,\n    reference_text=None,        # Optional reference\n    prompt=prompt\n)\n\nprint(f\"Coherence: {metrics.coherence:.3f}\")\nprint(f\"Fluency: {metrics.fluency:.3f}\")\nprint(f\"Relevance: {metrics.relevance:.3f}\")\nprint(f\"Diversity: {metrics.diversity:.3f}\")\nprint(f\"Repetition: {metrics.repetition:.3f}\")\n</code></pre>"},{"location":"user-guide/generation.html#3-ab-testing","title":"3. A/B Testing","text":"<p>Compare different generation settings:</p> <pre><code>from llmbuilder.inference import compare_generations\n\nconfigs = [\n    GenerationConfig(temperature=0.7, top_k=40),\n    GenerationConfig(temperature=0.8, top_p=0.9),\n    GenerationConfig(temperature=0.9, top_k=100, top_p=0.95)\n]\n\nresults = compare_generations(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    prompt=\"Explain the benefits of renewable energy:\",\n    configs=configs,\n    num_samples=10\n)\n\nfor i, result in enumerate(results):\n    print(f\"Config {i+1}: Quality={result.avg_quality:.3f}, Diversity={result.avg_diversity:.3f}\")\n</code></pre>"},{"location":"user-guide/generation.html#interactive-features","title":"\ud83c\udfae Interactive Features","text":""},{"location":"user-guide/generation.html#1-chat-interface","title":"1. Chat Interface","text":"<p>Create a chat-like experience:</p> <pre><code>from llmbuilder.inference import ChatInterface\n\nchat = ChatInterface(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    system_prompt=\"You are a helpful AI assistant.\",\n    config=GenerationConfig(temperature=0.8, max_new_tokens=200)\n)\n\n# Start chat session\nchat.start()\n\n# Or use programmatically\nconversation = []\nwhile True:\n    user_input = input(\"You: \")\n    if user_input.lower() == 'quit':\n        break\n\n    response = chat.respond(user_input, conversation)\n    conversation.append({\"user\": user_input, \"assistant\": response})\n    print(f\"AI: {response}\")\n</code></pre>"},{"location":"user-guide/generation.html#2-creative-writing-assistant","title":"2. Creative Writing Assistant","text":"<p>Specialized interface for creative writing:</p> <pre><code>from llmbuilder.inference import CreativeWriter\n\nwriter = CreativeWriter(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    style=\"creative\",\n    config=GenerationConfig(temperature=1.0, top_p=0.95)\n)\n\n# Story continuation\nstory_start = \"It was a dark and stormy night when Sarah discovered the mysterious letter...\"\ncontinuation = writer.continue_story(story_start, length=300)\n\n# Character development\ncharacter = writer.develop_character(\"a brilliant but eccentric scientist\")\n\n# Dialogue generation\ndialogue = writer.generate_dialogue(\"two friends discussing their dreams\", turns=6)\n</code></pre>"},{"location":"user-guide/generation.html#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"user-guide/generation.html#common-issues","title":"Common Issues","text":""},{"location":"user-guide/generation.html#repetitive-output","title":"Repetitive Output","text":"<pre><code># Solution: Adjust repetition penalty and sampling\nconfig = GenerationConfig(\n    repetition_penalty=1.2,     # Higher penalty\n    no_repeat_ngram_size=3,     # Prevent 3-gram repetition\n    temperature=0.9,            # Higher temperature\n    top_p=0.9                   # Use nucleus sampling\n)\n</code></pre>"},{"location":"user-guide/generation.html#incoherent-output","title":"Incoherent Output","text":"<pre><code># Solution: Lower temperature and use top-k\nconfig = GenerationConfig(\n    temperature=0.6,            # Lower temperature\n    top_k=40,                   # Limit choices\n    top_p=0.8,                  # Conservative nucleus\n    max_new_tokens=100          # Shorter responses\n)\n</code></pre>"},{"location":"user-guide/generation.html#too-conservative-output","title":"Too Conservative Output","text":"<pre><code># Solution: Increase temperature and sampling diversity\nconfig = GenerationConfig(\n    temperature=1.0,            # Higher temperature\n    top_k=100,                  # More choices\n    top_p=0.95,                 # Broader nucleus\n    repetition_penalty=1.1      # Slight repetition penalty\n)\n</code></pre>"},{"location":"user-guide/generation.html#slow-generation","title":"Slow Generation","text":"<pre><code># Solution: Optimize for speed\nconfig = GenerationConfig(\n    max_new_tokens=50,          # Shorter responses\n    do_sample=False,            # Use greedy decoding\n    use_cache=True,             # Enable KV cache\n    batch_size=1                # Single sample\n)\n\n# Use GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n</code></pre>"},{"location":"user-guide/generation.html#best-practices","title":"\ud83d\udcda Best Practices","text":""},{"location":"user-guide/generation.html#1-parameter-selection","title":"1. Parameter Selection","text":"<ul> <li>Start with temperature=0.8, top_k=50, top_p=0.9</li> <li>Adjust based on your specific use case</li> <li>Lower temperature for factual content</li> <li>Higher temperature for creative content</li> </ul>"},{"location":"user-guide/generation.html#2-prompt-engineering","title":"2. Prompt Engineering","text":"<ul> <li>Be specific and clear in your prompts</li> <li>Use examples for complex tasks</li> <li>Include context and constraints</li> <li>Test different prompt formats</li> </ul>"},{"location":"user-guide/generation.html#3-quality-control","title":"3. Quality Control","text":"<ul> <li>Always validate generated content</li> <li>Use appropriate filtering for your use case</li> <li>Monitor for bias and inappropriate content</li> <li>Test with diverse inputs</li> </ul>"},{"location":"user-guide/generation.html#4-performance-optimization","title":"4. Performance Optimization","text":"<ul> <li>Use appropriate batch sizes</li> <li>Enable GPU acceleration when available</li> <li>Cache models for repeated use</li> <li>Consider quantization for deployment</li> </ul> <p>Generation Tips</p> <ul> <li>Experiment with different parameter combinations to find what works best</li> <li>Use lower temperatures for factual tasks and higher for creative tasks</li> <li>Always validate generated content before using in production</li> <li>Consider the trade-off between quality and speed for your use case</li> <li>Keep prompts clear and specific for better results</li> </ul>"},{"location":"user-guide/tokenization.html","title":"Tokenization","text":"<p>Tokenization is the process of converting text into numerical tokens that language models can understand. LLMBuilder provides comprehensive tokenization tools with support for multiple algorithms and customization options.</p>"},{"location":"user-guide/tokenization.html#overview","title":"\ud83c\udfaf Overview","text":"<p>Tokenization bridges the gap between human text and machine learning:</p> <pre><code>graph LR\n    A[\"Hello world!\"] --&gt; B[Tokenizer]\n    B --&gt; C[\"[15496, 995, 0]\"]\n    C --&gt; D[Model]\n    D --&gt; E[Predictions]\n    E --&gt; F[Detokenizer]\n    F --&gt; G[\"Generated text\"]\n\n    style A fill:#e1f5fe\n    style C fill:#fff3e0\n    style G fill:#e8f5e8</code></pre>"},{"location":"user-guide/tokenization.html#tokenization-algorithms","title":"\ud83d\udd24 Tokenization Algorithms","text":"<p>LLMBuilder supports several tokenization algorithms:</p>"},{"location":"user-guide/tokenization.html#byte-pair-encoding-bpe-recommended","title":"Byte-Pair Encoding (BPE) - Recommended","text":"<p>BPE is the most popular algorithm for language models:</p> <pre><code>from llmbuilder.tokenizer import TokenizerTrainer\nfrom llmbuilder.config import TokenizerConfig\n\n# Configure BPE tokenizer\nconfig = TokenizerConfig(\n    vocab_size=16000,\n    model_type=\"bpe\",\n    character_coverage=1.0,\n    max_sentence_length=4096\n)\n\n# Train tokenizer\ntrainer = TokenizerTrainer(config=config)\ntrainer.train(\n    input_file=\"training_data.txt\",\n    output_dir=\"./tokenizer\",\n    model_prefix=\"tokenizer\"\n)\n</code></pre> <p>Advantages: - Balances vocabulary size and coverage - Handles out-of-vocabulary words well - Works across multiple languages - Standard for most modern LLMs</p>"},{"location":"user-guide/tokenization.html#unigram-language-model","title":"Unigram Language Model","text":"<p>Statistical approach that optimizes vocabulary:</p> <pre><code>config = TokenizerConfig(\n    vocab_size=16000,\n    model_type=\"unigram\",\n    character_coverage=0.9995,\n    unk_token=\"&lt;unk&gt;\",\n    bos_token=\"&lt;s&gt;\",\n    eos_token=\"&lt;/s&gt;\"\n)\n</code></pre> <p>Advantages: - Probabilistically optimal vocabulary - Better handling of morphologically rich languages - Flexible subword boundaries</p>"},{"location":"user-guide/tokenization.html#word-level-tokenization","title":"Word-Level Tokenization","text":"<p>Simple word-based tokenization:</p> <pre><code>config = TokenizerConfig(\n    vocab_size=50000,\n    model_type=\"word\",\n    lowercase=True,\n    remove_accents=True\n)\n</code></pre> <p>Use cases: - Small, domain-specific datasets - Languages with clear word boundaries - When interpretability is important</p>"},{"location":"user-guide/tokenization.html#character-level-tokenization","title":"Character-Level Tokenization","text":"<p>Character-by-character tokenization:</p> <pre><code>config = TokenizerConfig(\n    model_type=\"char\",\n    vocab_size=256,  # Usually small for character-level\n    normalize=True\n)\n</code></pre> <p>Use cases: - Very small datasets - Morphologically complex languages - When dealing with noisy text</p>"},{"location":"user-guide/tokenization.html#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"user-guide/tokenization.html#cli-training","title":"CLI Training","text":"<pre><code># Train a BPE tokenizer\nllmbuilder data tokenizer \\\n  --input training_data.txt \\\n  --output ./tokenizer \\\n  --vocab-size 16000 \\\n  --model-type bpe\n</code></pre>"},{"location":"user-guide/tokenization.html#python-api","title":"Python API","text":"<pre><code>from llmbuilder.tokenizer import train_tokenizer\n\n# Train tokenizer with default settings\ntokenizer = train_tokenizer(\n    input_file=\"training_data.txt\",\n    output_dir=\"./tokenizer\",\n    vocab_size=16000,\n    special_tokens=[\"&lt;pad&gt;\", \"&lt;unk&gt;\", \"&lt;s&gt;\", \"&lt;/s&gt;\"]\n)\n\n# Test the tokenizer\ntext = \"Hello, world! This is a test.\"\ntokens = tokenizer.encode(text)\ndecoded = tokenizer.decode(tokens)\n\nprint(f\"Original: {text}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Decoded: {decoded}\")\n</code></pre>"},{"location":"user-guide/tokenization.html#configuration-options","title":"\u2699\ufe0f Configuration Options","text":""},{"location":"user-guide/tokenization.html#basic-configuration","title":"Basic Configuration","text":"<pre><code>from llmbuilder.config import TokenizerConfig\n\nconfig = TokenizerConfig(\n    # Core settings\n    vocab_size=16000,           # Vocabulary size\n    model_type=\"bpe\",           # Algorithm: bpe, unigram, word, char\n\n    # Text preprocessing\n    lowercase=False,            # Convert to lowercase\n    remove_accents=False,       # Remove accent marks\n    normalize=True,             # Unicode normalization\n\n    # Coverage and quality\n    character_coverage=1.0,     # Character coverage (0.0-1.0)\n    max_sentence_length=4096,   # Maximum sentence length\n    min_frequency=2,            # Minimum token frequency\n\n    # Special tokens\n    unk_token=\"&lt;unk&gt;\",         # Unknown token\n    bos_token=\"&lt;s&gt;\",           # Beginning of sequence\n    eos_token=\"&lt;/s&gt;\",          # End of sequence\n    pad_token=\"&lt;pad&gt;\",         # Padding token\n    mask_token=\"&lt;mask&gt;\",       # Masking token (for MLM)\n)\n</code></pre>"},{"location":"user-guide/tokenization.html#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>config = TokenizerConfig(\n    # Algorithm-specific settings\n    bpe_dropout=0.1,           # BPE dropout for regularization\n    split_digits=True,         # Split numbers into digits\n    split_by_whitespace=True,  # Pre-tokenize by whitespace\n    split_by_punctuation=True, # Pre-tokenize by punctuation\n\n    # Vocabulary control\n    max_token_length=16,       # Maximum token length\n    vocab_threshold=1e-6,      # Vocabulary pruning threshold\n    shrinking_factor=0.75,     # Unigram shrinking factor\n\n    # Training control\n    num_threads=8,             # Number of training threads\n    seed=42,                   # Random seed\n    verbose=True               # Verbose training output\n)\n</code></pre>"},{"location":"user-guide/tokenization.html#special-tokens","title":"\ud83c\udf9b\ufe0f Special Tokens","text":"<p>Special tokens serve specific purposes in language models:</p>"},{"location":"user-guide/tokenization.html#standard-special-tokens","title":"Standard Special Tokens","text":"<pre><code>special_tokens = {\n    \"&lt;pad&gt;\": \"Padding token for batch processing\",\n    \"&lt;unk&gt;\": \"Unknown/out-of-vocabulary token\", \n    \"&lt;s&gt;\": \"Beginning of sequence token\",\n    \"&lt;/s&gt;\": \"End of sequence token\",\n    \"&lt;mask&gt;\": \"Masking token for masked language modeling\"\n}\n\nconfig = TokenizerConfig(\n    vocab_size=16000,\n    special_tokens=list(special_tokens.keys())\n)\n</code></pre>"},{"location":"user-guide/tokenization.html#custom-special-tokens","title":"Custom Special Tokens","text":"<pre><code># Add domain-specific special tokens\ncustom_tokens = [\n    \"&lt;code&gt;\", \"&lt;/code&gt;\",       # Code blocks\n    \"&lt;math&gt;\", \"&lt;/math&gt;\",       # Mathematical expressions\n    \"&lt;user&gt;\", \"&lt;assistant&gt;\",   # Chat/dialogue markers\n    \"&lt;system&gt;\",                # System messages\n    \"&lt;tool_call&gt;\", \"&lt;/tool_call&gt;\"  # Function calls\n]\n\nconfig = TokenizerConfig(\n    vocab_size=16000,\n    special_tokens=[\"&lt;pad&gt;\", \"&lt;unk&gt;\", \"&lt;s&gt;\", \"&lt;/s&gt;\"] + custom_tokens\n)\n</code></pre>"},{"location":"user-guide/tokenization.html#token-id-management","title":"Token ID Management","text":"<pre><code># Access special token IDs\ntokenizer = train_tokenizer(\"data.txt\", \"./tokenizer\", config=config)\n\nprint(f\"PAD token ID: {tokenizer.pad_token_id}\")\nprint(f\"UNK token ID: {tokenizer.unk_token_id}\")\nprint(f\"BOS token ID: {tokenizer.bos_token_id}\")\nprint(f\"EOS token ID: {tokenizer.eos_token_id}\")\n\n# Custom token IDs\ncode_start_id = tokenizer.token_to_id(\"&lt;code&gt;\")\ncode_end_id = tokenizer.token_to_id(\"&lt;/code&gt;\")\n</code></pre>"},{"location":"user-guide/tokenization.html#tokenizer-analysis","title":"\ud83d\udd0d Tokenizer Analysis","text":""},{"location":"user-guide/tokenization.html#vocabulary-analysis","title":"Vocabulary Analysis","text":"<pre><code>from llmbuilder.tokenizer import analyze_tokenizer\n\n# Analyze trained tokenizer\nanalysis = analyze_tokenizer(\"./tokenizer\")\n\nprint(f\"\ud83d\udcca Tokenizer Analysis:\")\nprint(f\"  Vocabulary size: {analysis.vocab_size:,}\")\nprint(f\"  Average token length: {analysis.avg_token_length:.2f}\")\nprint(f\"  Character coverage: {analysis.character_coverage:.4f}\")\nprint(f\"  Compression ratio: {analysis.compression_ratio:.2f}\")\n\n# Most common tokens\nprint(f\"\\n\ud83d\udd24 Most common tokens:\")\nfor token, freq in analysis.top_tokens[:10]:\n    print(f\"  '{token}': {freq:,}\")\n\n# Token length distribution\nprint(f\"\\n\ud83d\udccf Token length distribution:\")\nfor length, count in analysis.length_distribution.items():\n    print(f\"  {length} chars: {count:,} tokens\")\n</code></pre>"},{"location":"user-guide/tokenization.html#text-coverage-analysis","title":"Text Coverage Analysis","text":"<pre><code># Test tokenizer on sample texts\ntest_texts = [\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"Artificial intelligence and machine learning.\",\n    \"Code: print('Hello, world!')\",\n    \"Mathematical equation: E = mc\u00b2\"\n]\n\nfor text in test_texts:\n    tokens = tokenizer.encode(text)\n    decoded = tokenizer.decode(tokens)\n\n    print(f\"\\nText: {text}\")\n    print(f\"Tokens ({len(tokens)}): {tokens}\")\n    print(f\"Decoded: {decoded}\")\n    print(f\"Perfect reconstruction: {text == decoded}\")\n</code></pre>"},{"location":"user-guide/tokenization.html#compression-analysis","title":"Compression Analysis","text":"<pre><code>def analyze_compression(tokenizer, text_file):\n    \"\"\"Analyze tokenization compression.\"\"\"\n    with open(text_file, 'r', encoding='utf-8') as f:\n        text = f.read()\n\n    # Character count\n    char_count = len(text)\n\n    # Token count\n    tokens = tokenizer.encode(text)\n    token_count = len(tokens)\n\n    # Compression ratio\n    compression_ratio = char_count / token_count\n\n    print(f\"\ud83d\udcca Compression Analysis:\")\n    print(f\"  Characters: {char_count:,}\")\n    print(f\"  Tokens: {token_count:,}\")\n    print(f\"  Compression ratio: {compression_ratio:.2f}\")\n    print(f\"  Tokens per 1000 chars: {1000 / compression_ratio:.1f}\")\n\n    return compression_ratio\n\n# Analyze compression\nratio = analyze_compression(tokenizer, \"training_data.txt\")\n</code></pre>"},{"location":"user-guide/tokenization.html#tokenizer-usage","title":"\ud83d\udd04 Tokenizer Usage","text":""},{"location":"user-guide/tokenization.html#basic-encodingdecoding","title":"Basic Encoding/Decoding","text":"<pre><code>from llmbuilder.tokenizer import Tokenizer\n\n# Load trained tokenizer\ntokenizer = Tokenizer.from_pretrained(\"./tokenizer\")\n\n# Encode text to tokens\ntext = \"Hello, how are you today?\"\ntokens = tokenizer.encode(text)\nprint(f\"Tokens: {tokens}\")\n\n# Decode tokens back to text\ndecoded_text = tokenizer.decode(tokens)\nprint(f\"Decoded: {decoded_text}\")\n\n# Encode with special tokens\ntokens_with_special = tokenizer.encode(\n    text, \n    add_bos_token=True,  # Add beginning-of-sequence token\n    add_eos_token=True   # Add end-of-sequence token\n)\nprint(f\"With special tokens: {tokens_with_special}\")\n</code></pre>"},{"location":"user-guide/tokenization.html#batch-processing","title":"Batch Processing","text":"<pre><code># Encode multiple texts\ntexts = [\n    \"First example text.\",\n    \"Second example text.\",\n    \"Third example text.\"\n]\n\n# Batch encode\nbatch_tokens = tokenizer.encode_batch(texts)\nfor i, tokens in enumerate(batch_tokens):\n    print(f\"Text {i+1}: {tokens}\")\n\n# Batch decode\nbatch_decoded = tokenizer.decode_batch(batch_tokens)\nfor i, text in enumerate(batch_decoded):\n    print(f\"Decoded {i+1}: {text}\")\n</code></pre>"},{"location":"user-guide/tokenization.html#padding-and-truncation","title":"Padding and Truncation","text":"<pre><code># Encode with padding and truncation\ntokens = tokenizer.encode(\n    text,\n    max_length=512,      # Maximum sequence length\n    padding=\"max_length\", # Pad to max_length\n    truncation=True,     # Truncate if longer\n    return_attention_mask=True  # Return attention mask\n)\n\nprint(f\"Tokens: {tokens['input_ids']}\")\nprint(f\"Attention mask: {tokens['attention_mask']}\")\n</code></pre>"},{"location":"user-guide/tokenization.html#domain-specific-tokenization","title":"\ud83c\udfaf Domain-Specific Tokenization","text":""},{"location":"user-guide/tokenization.html#code-tokenization","title":"Code Tokenization","text":"<pre><code># Configure tokenizer for code\ncode_config = TokenizerConfig(\n    vocab_size=32000,\n    model_type=\"bpe\",\n    split_by_whitespace=True,\n    split_by_punctuation=False,  # Keep operators together\n    special_tokens=[\n        \"&lt;pad&gt;\", \"&lt;unk&gt;\", \"&lt;s&gt;\", \"&lt;/s&gt;\",\n        \"&lt;code&gt;\", \"&lt;/code&gt;\",\n        \"&lt;comment&gt;\", \"&lt;/comment&gt;\",\n        \"&lt;string&gt;\", \"&lt;/string&gt;\",\n        \"&lt;number&gt;\", \"&lt;/number&gt;\"\n    ]\n)\n\n# Train on code data\ncode_tokenizer = train_tokenizer(\n    input_file=\"code_dataset.txt\",\n    output_dir=\"./code_tokenizer\",\n    config=code_config\n)\n</code></pre>"},{"location":"user-guide/tokenization.html#multilingual-tokenization","title":"Multilingual Tokenization","text":"<pre><code># Configure for multiple languages\nmultilingual_config = TokenizerConfig(\n    vocab_size=64000,  # Larger vocab for multiple languages\n    model_type=\"unigram\",  # Better for morphologically rich languages\n    character_coverage=0.9995,  # High coverage for diverse scripts\n    normalize=True,\n    special_tokens=[\n        \"&lt;pad&gt;\", \"&lt;unk&gt;\", \"&lt;s&gt;\", \"&lt;/s&gt;\",\n        \"&lt;en&gt;\", \"&lt;es&gt;\", \"&lt;fr&gt;\", \"&lt;de&gt;\",  # Language tags\n        \"&lt;zh&gt;\", \"&lt;ja&gt;\", \"&lt;ar&gt;\", \"&lt;hi&gt;\"\n    ]\n)\n</code></pre>"},{"location":"user-guide/tokenization.html#scientific-text-tokenization","title":"Scientific Text Tokenization","text":"<pre><code># Configure for scientific text\nscientific_config = TokenizerConfig(\n    vocab_size=24000,\n    model_type=\"bpe\",\n    split_digits=False,  # Keep numbers together\n    special_tokens=[\n        \"&lt;pad&gt;\", \"&lt;unk&gt;\", \"&lt;s&gt;\", \"&lt;/s&gt;\",\n        \"&lt;math&gt;\", \"&lt;/math&gt;\",\n        \"&lt;formula&gt;\", \"&lt;/formula&gt;\",\n        \"&lt;citation&gt;\", \"&lt;/citation&gt;\",\n        \"&lt;table&gt;\", \"&lt;/table&gt;\",\n        \"&lt;figure&gt;\", \"&lt;/figure&gt;\"\n    ]\n)\n</code></pre>"},{"location":"user-guide/tokenization.html#advanced-features","title":"\ud83d\udd27 Advanced Features","text":""},{"location":"user-guide/tokenization.html#tokenizer-merging","title":"Tokenizer Merging","text":"<pre><code># Merge multiple tokenizers\nfrom llmbuilder.tokenizer import merge_tokenizers\n\nbase_tokenizer = Tokenizer.from_pretrained(\"./base_tokenizer\")\ndomain_tokenizer = Tokenizer.from_pretrained(\"./domain_tokenizer\")\n\nmerged_tokenizer = merge_tokenizers(\n    [base_tokenizer, domain_tokenizer],\n    output_dir=\"./merged_tokenizer\",\n    vocab_size=32000\n)\n</code></pre>"},{"location":"user-guide/tokenization.html#vocabulary-extension","title":"Vocabulary Extension","text":"<pre><code># Extend existing tokenizer vocabulary\nnew_tokens = [\"&lt;new_token_1&gt;\", \"&lt;new_token_2&gt;\", \"domain_term\"]\n\nextended_tokenizer = tokenizer.extend_vocabulary(\n    new_tokens=new_tokens,\n    output_dir=\"./extended_tokenizer\"\n)\n\nprint(f\"Original vocab size: {len(tokenizer)}\")\nprint(f\"Extended vocab size: {len(extended_tokenizer)}\")\n</code></pre>"},{"location":"user-guide/tokenization.html#tokenizer-adaptation","title":"Tokenizer Adaptation","text":"<pre><code># Adapt tokenizer to new domain\nadapted_tokenizer = tokenizer.adapt_to_domain(\n    domain_data=\"new_domain_data.txt\",\n    adaptation_ratio=0.1,  # 10% of vocab from new domain\n    output_dir=\"./adapted_tokenizer\"\n)\n</code></pre>"},{"location":"user-guide/tokenization.html#performance-optimization","title":"\ud83d\udcca Performance Optimization","text":""},{"location":"user-guide/tokenization.html#fast-tokenization","title":"Fast Tokenization","text":"<pre><code># Enable fast tokenization\ntokenizer = Tokenizer.from_pretrained(\n    \"./tokenizer\",\n    use_fast=True,      # Use fast Rust implementation\n    num_threads=8       # Parallel processing\n)\n\n# Benchmark tokenization speed\nimport time\n\ntext = \"Your text here\" * 1000  # Large text\nstart_time = time.time()\ntokens = tokenizer.encode(text)\nend_time = time.time()\n\nprint(f\"Tokenization speed: {len(tokens) / (end_time - start_time):.0f} tokens/sec\")\n</code></pre>"},{"location":"user-guide/tokenization.html#memory-optimization","title":"Memory Optimization","text":"<pre><code># Optimize for memory usage\ntokenizer = Tokenizer.from_pretrained(\n    \"./tokenizer\",\n    low_memory=True,    # Reduce memory usage\n    mmap=True          # Memory-map vocabulary files\n)\n</code></pre>"},{"location":"user-guide/tokenization.html#caching","title":"Caching","text":"<pre><code># Enable tokenization caching\ntokenizer.enable_cache(\n    cache_dir=\"./tokenizer_cache\",\n    max_cache_size=\"1GB\"\n)\n\n# Tokenization results will be cached for repeated texts\n</code></pre>"},{"location":"user-guide/tokenization.html#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"user-guide/tokenization.html#common-issues","title":"Common Issues","text":""},{"location":"user-guide/tokenization.html#1-poor-tokenization-quality","title":"1. Poor Tokenization Quality","text":"<pre><code># Increase vocabulary size\nconfig.vocab_size = 32000\n\n# Improve character coverage\nconfig.character_coverage = 0.9999\n\n# Adjust minimum frequency\nconfig.min_frequency = 1\n</code></pre>"},{"location":"user-guide/tokenization.html#2-out-of-memory-during-training","title":"2. Out-of-Memory During Training","text":"<pre><code># Reduce training data size\nconfig.max_sentence_length = 1024\n\n# Use fewer threads\nconfig.num_threads = 4\n\n# Process in chunks\ntrainer.train_chunked(\n    input_file=\"large_data.txt\",\n    chunk_size=1000000  # 1M characters per chunk\n)\n</code></pre>"},{"location":"user-guide/tokenization.html#3-slow-tokenization","title":"3. Slow Tokenization","text":"<pre><code># Use fast tokenizer\ntokenizer = Tokenizer.from_pretrained(\"./tokenizer\", use_fast=True)\n\n# Enable parallel processing\ntokenizer.enable_parallelism(True)\n\n# Batch process texts\ntokens = tokenizer.encode_batch(texts, batch_size=1000)\n</code></pre>"},{"location":"user-guide/tokenization.html#validation-and-testing","title":"Validation and Testing","text":"<pre><code># Validate tokenizer quality\ndef validate_tokenizer(tokenizer, test_texts):\n    \"\"\"Validate tokenizer on test texts.\"\"\"\n    issues = []\n\n    for text in test_texts:\n        tokens = tokenizer.encode(text)\n        decoded = tokenizer.decode(tokens)\n\n        if text != decoded:\n            issues.append({\n                'original': text,\n                'decoded': decoded,\n                'tokens': tokens\n            })\n\n    if issues:\n        print(f\"\u26a0\ufe0f  Found {len(issues)} reconstruction issues:\")\n        for issue in issues[:5]:  # Show first 5\n            print(f\"  Original: {issue['original']}\")\n            print(f\"  Decoded:  {issue['decoded']}\")\n            print()\n    else:\n        print(\"\u2705 All texts reconstructed perfectly!\")\n\n    return len(issues) == 0\n\n# Test tokenizer\ntest_texts = [\n    \"Hello, world!\",\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"Special characters: @#$%^&amp;*()\",\n    \"Numbers: 123 456.789\",\n    \"Unicode: caf\u00e9 na\u00efve r\u00e9sum\u00e9\"\n]\n\nis_valid = validate_tokenizer(tokenizer, test_texts)\n</code></pre>"},{"location":"user-guide/tokenization.html#best-practices","title":"\ud83d\udcda Best Practices","text":""},{"location":"user-guide/tokenization.html#1-vocabulary-size-selection","title":"1. Vocabulary Size Selection","text":"<ul> <li>Small datasets (&lt; 1M tokens): 8K - 16K vocabulary</li> <li>Medium datasets (1M - 100M tokens): 16K - 32K vocabulary  </li> <li>Large datasets (&gt; 100M tokens): 32K - 64K vocabulary</li> <li>Multilingual: 64K - 128K vocabulary</li> </ul>"},{"location":"user-guide/tokenization.html#2-algorithm-selection","title":"2. Algorithm Selection","text":"<ul> <li>BPE: General purpose, most compatible</li> <li>Unigram: Better for morphologically rich languages</li> <li>Word: Simple datasets, interpretability needed</li> <li>Character: Very small datasets, noisy text</li> </ul>"},{"location":"user-guide/tokenization.html#3-special-token-strategy","title":"3. Special Token Strategy","text":"<ul> <li>Always include <code>&lt;pad&gt;</code>, <code>&lt;unk&gt;</code>, <code>&lt;s&gt;</code>, <code>&lt;/s&gt;</code></li> <li>Add domain-specific tokens for better performance</li> <li>Reserve 5-10% of vocabulary for special tokens</li> <li>Use consistent special token formats</li> </ul>"},{"location":"user-guide/tokenization.html#4-training-data-quality","title":"4. Training Data Quality","text":"<ul> <li>Use the same data distribution as your target task</li> <li>Include diverse text types and styles</li> <li>Clean data but preserve important patterns</li> <li>Ensure sufficient data size (&gt; 1M characters recommended)</li> </ul>"},{"location":"user-guide/tokenization.html#5-validation-and-testing","title":"5. Validation and Testing","text":"<ul> <li>Test on held-out data</li> <li>Verify perfect reconstruction for important text types</li> <li>Monitor compression ratios</li> <li>Check coverage of domain-specific terms</li> </ul> <p>Tokenization Tips</p> <ul> <li>Train your tokenizer on the same type of data you'll use for model training</li> <li>Always validate tokenizer quality before training your model</li> <li>Consider the trade-off between vocabulary size and model size</li> <li>Save tokenizer training logs and configurations for reproducibility</li> <li>Test tokenization on edge cases and special characters</li> </ul>"},{"location":"user-guide/training.html","title":"Model Training","text":"<p>This comprehensive guide covers everything you need to know about training language models with LLMBuilder, from basic concepts to advanced techniques.</p>"},{"location":"user-guide/training.html#training-overview","title":"\ud83c\udfaf Training Overview","text":"<p>LLMBuilder provides a complete training pipeline that handles:</p> <pre><code>graph LR\n    A[Dataset] --&gt; B[DataLoader]\n    B --&gt; C[Model]\n    C --&gt; D[Loss Function]\n    D --&gt; E[Optimizer]\n    E --&gt; F[Training Loop]\n    F --&gt; G[Checkpoints]\n    G --&gt; H[Evaluation]\n\n    style A fill:#e1f5fe\n    style G fill:#e8f5e8\n    style H fill:#fff3e0</code></pre>"},{"location":"user-guide/training.html#quick-start-training","title":"\ud83d\ude80 Quick Start Training","text":""},{"location":"user-guide/training.html#basic-training-command","title":"Basic Training Command","text":"<pre><code>llmbuilder train model \\\n  --data training_data.txt \\\n  --tokenizer ./tokenizer \\\n  --output ./model \\\n  --epochs 10 \\\n  --batch-size 16\n</code></pre>"},{"location":"user-guide/training.html#python-api-training","title":"Python API Training","text":"<pre><code>import llmbuilder as lb\n\n# Load configuration\nconfig = lb.load_config(preset=\"cpu_small\")\n\n# Build model\nmodel = lb.build_model(config.model)\n\n# Prepare dataset\nfrom llmbuilder.data import TextDataset\ndataset = TextDataset(\"training_data.txt\", block_size=config.model.max_seq_length)\n\n# Train model\nresults = lb.train_model(model, dataset, config.training)\n</code></pre>"},{"location":"user-guide/training.html#training-configuration","title":"\u2699\ufe0f Training Configuration","text":""},{"location":"user-guide/training.html#core-training-parameters","title":"Core Training Parameters","text":"<pre><code>from llmbuilder.config import TrainingConfig\n\nconfig = TrainingConfig(\n    # Basic settings\n    batch_size=16,              # Samples per training step\n    num_epochs=10,              # Number of training epochs\n    learning_rate=3e-4,         # Learning rate\n\n    # Optimization\n    optimizer=\"adamw\",          # Optimizer type\n    weight_decay=0.01,          # L2 regularization\n    max_grad_norm=1.0,          # Gradient clipping\n\n    # Learning rate scheduling\n    warmup_steps=1000,          # Warmup steps\n    scheduler=\"cosine\",         # LR scheduler type\n\n    # Checkpointing\n    save_every=1000,            # Save checkpoint every N steps\n    eval_every=500,             # Evaluate every N steps\n    max_checkpoints=5,          # Maximum checkpoints to keep\n\n    # Logging\n    log_every=100,              # Log every N steps\n    wandb_project=None,         # Weights &amp; Biases project\n)\n</code></pre>"},{"location":"user-guide/training.html#training-process","title":"\ud83c\udfcb\ufe0f Training Process","text":""},{"location":"user-guide/training.html#training-loop","title":"Training Loop","text":"<p>The training process follows these steps:</p> <ol> <li>Data Loading: Load and batch training data</li> <li>Forward Pass: Compute model predictions</li> <li>Loss Calculation: Calculate training loss</li> <li>Backward Pass: Compute gradients</li> <li>Optimization: Update model parameters</li> <li>Evaluation: Periodic validation</li> <li>Checkpointing: Save model state</li> </ol>"},{"location":"user-guide/training.html#monitoring-training","title":"Monitoring Training","text":"<pre><code>from llmbuilder.training import Trainer\n\ntrainer = Trainer(\n    model=model,\n    train_dataset=train_dataset,\n    val_dataset=val_dataset,\n    config=training_config\n)\n\n# Train with progress monitoring\nresults = trainer.train()\n\nprint(f\"Final training loss: {results.final_train_loss:.4f}\")\nprint(f\"Final validation loss: {results.final_val_loss:.4f}\")\nprint(f\"Best validation loss: {results.best_val_loss:.4f}\")\nprint(f\"Training time: {results.training_time}\")\n</code></pre>"},{"location":"user-guide/training.html#advanced-training-techniques","title":"\ud83d\udcca Advanced Training Techniques","text":""},{"location":"user-guide/training.html#mixed-precision-training","title":"Mixed Precision Training","text":"<pre><code>config = TrainingConfig(\n    mixed_precision=\"fp16\",     # Use 16-bit precision\n    gradient_accumulation_steps=4,  # Accumulate gradients\n)\n</code></pre>"},{"location":"user-guide/training.html#gradient-checkpointing","title":"Gradient Checkpointing","text":"<pre><code>from llmbuilder.config import ModelConfig\n\nmodel_config = ModelConfig(\n    gradient_checkpointing=True,  # Save memory at cost of compute\n    # ... other config\n)\n</code></pre>"},{"location":"user-guide/training.html#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"<pre><code>config = TrainingConfig(\n    scheduler=\"cosine\",         # Cosine annealing\n    warmup_steps=2000,         # Linear warmup\n    min_lr_ratio=0.1,          # Minimum LR as ratio of max LR\n)\n</code></pre>"},{"location":"user-guide/training.html#training-best-practices","title":"\ud83c\udfaf Training Best Practices","text":""},{"location":"user-guide/training.html#1-data-quality","title":"1. Data Quality","text":"<ul> <li>Use high-quality, diverse training data</li> <li>Remove duplicates and low-quality samples</li> <li>Ensure proper text preprocessing</li> </ul>"},{"location":"user-guide/training.html#2-hyperparameter-tuning","title":"2. Hyperparameter Tuning","text":"<ul> <li>Start with proven configurations</li> <li>Adjust learning rate based on model size</li> <li>Use appropriate batch sizes for your hardware</li> </ul>"},{"location":"user-guide/training.html#3-monitoring-and-evaluation","title":"3. Monitoring and Evaluation","text":"<ul> <li>Monitor both training and validation loss</li> <li>Use early stopping to prevent overfitting</li> <li>Regular checkpointing for recovery</li> </ul>"},{"location":"user-guide/training.html#4-hardware-optimization","title":"4. Hardware Optimization","text":"<ul> <li>Use GPU when available</li> <li>Enable mixed precision for faster training</li> <li>Optimize batch size for your hardware</li> </ul>"},{"location":"user-guide/training.html#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"user-guide/training.html#common-training-issues","title":"Common Training Issues","text":""},{"location":"user-guide/training.html#out-of-memory","title":"Out of Memory","text":"<pre><code># Reduce batch size\nconfig.batch_size = 8\n\n# Enable gradient checkpointing\nmodel_config.gradient_checkpointing = True\n\n# Use gradient accumulation\nconfig.gradient_accumulation_steps = 4\n</code></pre>"},{"location":"user-guide/training.html#slow-convergence","title":"Slow Convergence","text":"<pre><code># Increase learning rate\nconfig.learning_rate = 5e-4\n\n# Longer warmup\nconfig.warmup_steps = 2000\n\n# Different optimizer\nconfig.optimizer = \"adam\"\n</code></pre>"},{"location":"user-guide/training.html#unstable-training","title":"Unstable Training","text":"<pre><code># Lower learning rate\nconfig.learning_rate = 1e-4\n\n# Stronger gradient clipping\nconfig.max_grad_norm = 0.5\n\n# Add weight decay\nconfig.weight_decay = 0.1\n</code></pre> <p>Training Tips</p> <ul> <li>Start with small models and scale up gradually</li> <li>Monitor GPU memory usage and adjust batch size accordingly</li> <li>Use validation loss to detect overfitting</li> <li>Save checkpoints frequently during long training runs</li> </ul>"}]}