{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LLMBuilder Documentation\ud83e\udd16 LLMBuilder","text":"<p>A comprehensive toolkit for building, training, and deploying language models</p> <p> </p>"},{"location":"#what-is-llmbuilder","title":"What is LLMBuilder?","text":"<p>LLMBuilder is a production-ready framework for training and fine-tuning Large Language Models (LLMs) \u2014 not a model itself. Designed for developers, researchers, and AI engineers, LLMBuilder provides a complete pipeline to go from raw multi-format documents to deployable, optimized LLMs, with advanced data processing capabilities and support for both CPU and GPU training.</p>"},{"location":"#key-features","title":"\ud83c\udfaf Key Features","text":"<p>\ud83d\ude80 Easy to Use</p> <ul> <li>One-line training: <code>llmbuilder train model --data data.txt --output model/</code></li> <li>Interactive CLI: Guided setup with <code>llmbuilder welcome</code></li> <li>Python API: Simple <code>import llmbuilder as lb</code> interface</li> <li>CPU-friendly: Optimized for local development</li> </ul> <p>\ud83d\udd27 Comprehensive</p> <ul> <li>Multi-Format Ingestion: HTML, Markdown, EPUB, PDF, TXT processing</li> <li>Advanced Deduplication: Exact and semantic duplicate detection</li> <li>Flexible Tokenization: BPE, SentencePiece, Hugging Face tokenizers</li> <li>Full Training Pipeline: GPT-style transformer training with checkpointing</li> <li>Fine-tuning: LoRA and full parameter fine-tuning</li> <li>Model Export: GGUF conversion with multiple quantization levels</li> </ul> <p>\u26a1 Performance</p> <ul> <li>Memory efficient: Gradient checkpointing and mixed precision</li> <li>Scalable: Single GPU to multi-GPU training</li> <li>Fast inference: Optimized text generation</li> <li>Quantization: 8-bit and 16-bit model compression</li> </ul> <p>\ud83d\udee0\ufe0f Developer Friendly</p> <ul> <li>Modular design: Use only what you need</li> <li>Extensive docs: Complete API reference and examples</li> <li>Testing: Comprehensive test suite</li> <li>Migration: Easy upgrade from legacy scripts</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>import llmbuilder as lb\n\n# 1. Process multi-format documents\nfrom llmbuilder.data.ingest import IngestionPipeline\npipeline = IngestionPipeline()\npipeline.process_directory(\"./raw_docs\", \"./processed.txt\")\n\n# 2. Deduplicate content\nfrom llmbuilder.data.dedup import DeduplicationPipeline\ndedup = DeduplicationPipeline()\ndedup.process_file(\"./processed.txt\", \"./clean.txt\")\n\n# 3. Train custom tokenizer\nfrom llmbuilder.tokenizer import TokenizerTrainer\ntrainer = TokenizerTrainer(algorithm=\"sentencepiece\", vocab_size=16000)\ntrainer.train(\"./clean.txt\", \"./tokenizers\")\n\n# 4. Load configuration and build model\ncfg = lb.load_config(preset=\"cpu_small\")\nmodel = lb.build_model(cfg.model)\n\n# 5. Train the model\nfrom llmbuilder.data import TextDataset\ndataset = TextDataset(\"./clean.txt\", block_size=cfg.model.max_seq_length)\nresults = lb.train_model(model, dataset, cfg.training)\n\n# 6. Convert to GGUF format\nfrom llmbuilder.tools.convert_to_gguf import GGUFConverter\nconverter = GGUFConverter()\nconverter.convert_model(\"./checkpoints/model.pt\", \"./model.gguf\", \"Q8_0\")\n\n# 7. Generate text\ntext = lb.generate_text(\n    model_path=\"./checkpoints/model.pt\",\n    tokenizer_path=\"./tokenizers\",\n    prompt=\"The future of AI is\",\n    max_new_tokens=50\n)\nprint(text)\n</code></pre>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TB\n    A[Multi-Format Documents&lt;br/&gt;HTML, Markdown, EPUB, PDF, TXT] --&gt; B[Ingestion Pipeline]\n    B --&gt; C[Text Normalization]\n    C --&gt; D[Deduplication&lt;br/&gt;Exact &amp; Semantic]\n    D --&gt; E[Tokenizer Training&lt;br/&gt;BPE, SentencePiece, HF]\n    E --&gt; F[Dataset Creation]\n    F --&gt; G[Model Training&lt;br/&gt;GPT Architecture]\n    G --&gt; H[Checkpoints &amp; Validation]\n    H --&gt; I[Text Generation]\n    H --&gt; J[GGUF Conversion&lt;br/&gt;Multiple Quantization Levels]\n\n    style A fill:#e1f5fe\n    style D fill:#f3e5f5\n    style E fill:#e8f5e8\n    style I fill:#e8f5e8\n    style J fill:#fff3e0</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Choose your path to get started with LLMBuilder:</p>"},{"location":"#documentation-sections","title":"\ud83d\udcda Documentation Sections","text":"<ul> <li>Quick Start - Get up and running in 5 minutes</li> <li>Installation - Install LLMBuilder and set up your environment</li> <li>First Model - Train your first language model step by step</li> <li>User Guide - Comprehensive guides for all features and capabilities</li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":"<p>Research &amp; Experimentation</p> <p>Perfect for researchers who need to quickly prototype and experiment with different model architectures, training strategies, and datasets.</p> <p>Educational Projects</p> <p>Ideal for students and educators learning about transformer models, with clear examples and comprehensive documentation.</p> <p>Production Deployment</p> <p>Ready for production use with model export, quantization, and optimization features for deployment at scale.</p> <p>Domain-Specific Models</p> <p>Fine-tune models on your specific domain data for improved performance on specialized tasks.</p>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub: Qubasehq/llmbuilder-package</li> <li>Issues: Report bugs and request features</li> <li>Discussions: Community discussions</li> <li>Website: qubase.in</li> </ul> <p>Built with \u2764\ufe0f by Qub\u25b3se</p> <p>Empowering developers to create amazing AI applications</p>"},{"location":"api/core/","title":"Core API","text":"<p>The core LLMBuilder API provides high-level functions for common tasks. These functions are designed to be simple to use while providing access to the full power of the framework.</p>"},{"location":"api/core/#overview","title":"\ud83c\udfaf Overview","text":"<p>The core API is accessible through the main <code>llmbuilder</code> module:</p> <pre><code>import llmbuilder as lb\n\n# High-level functions\nconfig = lb.load_config(preset=\"cpu_small\")\nmodel = lb.build_model(config.model)\ntext = lb.generate_text(model_path, tokenizer_path, prompt)\n</code></pre>"},{"location":"api/core/#core-functions","title":"\ud83d\udccb Core Functions","text":""},{"location":"api/core/#configuration-functions","title":"Configuration Functions","text":""},{"location":"api/core/#llmbuilder.load_config","title":"<code>llmbuilder.load_config(path=None, preset=None)</code>","text":"<p>Load configuration from file or use preset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Optional[str]</code> <p>Optional path to a JSON/YAML configuration file.</p> <code>None</code> <code>preset</code> <code>Optional[str]</code> <p>Optional name of a built-in preset.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A configuration object suitable for model/training builders.</p> Source code in <code>llmbuilder\\__init__.py</code> <pre><code>def load_config(path: Optional[str] = None, preset: Optional[str] = None) -&gt; Any:\n    \"\"\"Load configuration from file or use preset.\n\n    Args:\n        path: Optional path to a JSON/YAML configuration file.\n        preset: Optional name of a built-in preset.\n\n    Returns:\n        A configuration object suitable for model/training builders.\n    \"\"\"\n    from .config import load_config as _load_config\n\n    return _load_config(path, preset)\n</code></pre>"},{"location":"api/core/#model-functions","title":"Model Functions","text":""},{"location":"api/core/#llmbuilder.build_model","title":"<code>llmbuilder.build_model(config)</code>","text":"<p>Build a model from configuration.</p> Source code in <code>llmbuilder\\__init__.py</code> <pre><code>def build_model(config: Any) -&gt; Any:\n    \"\"\"Build a model from configuration.\"\"\"\n    from .model import build_model as _build_model\n\n    return _build_model(config)\n</code></pre>"},{"location":"api/core/#training-functions","title":"Training Functions","text":""},{"location":"api/core/#llmbuilder.train_model","title":"<code>llmbuilder.train_model(model, dataset, config)</code>","text":"<p>Train a model with the given dataset and configuration.</p> Source code in <code>llmbuilder\\__init__.py</code> <pre><code>def train_model(model: Any, dataset: Any, config: Any) -&gt; Any:\n    \"\"\"Train a model with the given dataset and configuration.\"\"\"\n    from .training import train_model as _train_model\n\n    return _train_model(model, dataset, config)\n</code></pre>"},{"location":"api/core/#llmbuilder.train","title":"<code>llmbuilder.train(data_path, output_dir, config=None, clean=False)</code>","text":"<p>High-level training function that handles the complete training pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Union[str, Path, List[Union[str, Path]]]</code> <p>Path to input data file(s) or directory</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Directory to save outputs (tokenizer, checkpoints, etc.)</p> required <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Optional configuration dictionary</p> <code>None</code> <code>clean</code> <code>bool</code> <p>If True, clean up previous outputs before starting</p> <code>False</code> <p>Returns:</p> Name Type Description <code>TrainingPipeline</code> <code>TrainingPipeline</code> <p>The trained pipeline instance</p> Example <p>import llmbuilder</p> Source code in <code>llmbuilder\\__init__.py</code> <pre><code>def train(\n    data_path: Union[str, Path, List[Union[str, Path]]],\n    output_dir: Union[str, Path],\n    config: Optional[Dict[str, Any]] = None,\n    clean: bool = False,\n) -&gt; \"TrainingPipeline\":\n    \"\"\"\n    High-level training function that handles the complete training pipeline.\n\n    Args:\n        data_path: Path to input data file(s) or directory\n        output_dir: Directory to save outputs (tokenizer, checkpoints, etc.)\n        config: Optional configuration dictionary\n        clean: If True, clean up previous outputs before starting\n\n    Returns:\n        TrainingPipeline: The trained pipeline instance\n\n    Example:\n        &gt;&gt;&gt; import llmbuilder\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Train with default settings\n        &gt;&gt;&gt; pipeline = llmbuilder.train(\n        ...     data_path=\"./my_data/\",\n        ...     output_dir=\"./output/\"\n        ... )\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Generate text after training\n        &gt;&gt;&gt; text = pipeline.generate(\"The future of AI is\")\n    \"\"\"\n    from .pipeline import train as _train\n\n    return _train(data_path, output_dir, config or {}, clean)\n</code></pre>"},{"location":"api/core/#llmbuilder.train--train-with-default-settings","title":"Train with default settings","text":"<p>pipeline = llmbuilder.train( ...     data_path=\"./my_data/\", ...     output_dir=\"./output/\" ... )</p>"},{"location":"api/core/#llmbuilder.train--generate-text-after-training","title":"Generate text after training","text":"<p>text = pipeline.generate(\"The future of AI is\")</p>"},{"location":"api/core/#generation-functions","title":"Generation Functions","text":""},{"location":"api/core/#llmbuilder.generate_text","title":"<code>llmbuilder.generate_text(model_path, tokenizer_path, prompt, **kwargs)</code>","text":"<p>Generate text using a trained model.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to trained model checkpoint</p> required <code>tokenizer_path</code> <code>str</code> <p>Path to tokenizer directory</p> required <code>prompt</code> <code>str</code> <p>Input text prompt</p> required <code>**kwargs</code> <code>Any</code> <p>Additional generation parameters (temperature, top_k, top_p, etc.)</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Generated text string</p> Example <p>import llmbuilder</p> <p>text = llmbuilder.generate_text( ...     model_path=\"./output/checkpoints/model.pt\", ...     tokenizer_path=\"./output/tokenizer/\", ...     prompt=\"The future of AI is\", ...     max_new_tokens=100, ...     temperature=0.8 ... )</p> Source code in <code>llmbuilder\\__init__.py</code> <pre><code>def generate_text(\n    model_path: str, tokenizer_path: str, prompt: str, **kwargs: Any\n) -&gt; str:\n    \"\"\"\n    Generate text using a trained model.\n\n    Args:\n        model_path: Path to trained model checkpoint\n        tokenizer_path: Path to tokenizer directory\n        prompt: Input text prompt\n        **kwargs: Additional generation parameters (temperature, top_k, top_p, etc.)\n\n    Returns:\n        Generated text string\n\n    Example:\n        &gt;&gt;&gt; import llmbuilder\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; text = llmbuilder.generate_text(\n        ...     model_path=\"./output/checkpoints/model.pt\",\n        ...     tokenizer_path=\"./output/tokenizer/\",\n        ...     prompt=\"The future of AI is\",\n        ...     max_new_tokens=100,\n        ...     temperature=0.8\n        ... )\n    \"\"\"\n    from .inference import generate_text as _generate_text\n\n    return _generate_text(model_path, tokenizer_path, prompt, **kwargs)\n</code></pre>"},{"location":"api/core/#llmbuilder.interactive_cli","title":"<code>llmbuilder.interactive_cli(model_path, tokenizer_path, **kwargs)</code>","text":"<p>Start an interactive CLI for text generation.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to trained model checkpoint</p> required <code>tokenizer_path</code> <code>str</code> <p>Path to tokenizer directory</p> required <code>**kwargs</code> <code>Any</code> <p>Additional configuration parameters</p> <code>{}</code> Example <p>import llmbuilder</p> <p>llmbuilder.interactive_cli( ...     model_path=\"./output/checkpoints/model.pt\", ...     tokenizer_path=\"./output/tokenizer/\", ...     temperature=0.8 ... )</p> Source code in <code>llmbuilder\\__init__.py</code> <pre><code>def interactive_cli(model_path: str, tokenizer_path: str, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Start an interactive CLI for text generation.\n\n    Args:\n        model_path: Path to trained model checkpoint\n        tokenizer_path: Path to tokenizer directory\n        **kwargs: Additional configuration parameters\n\n    Example:\n        &gt;&gt;&gt; import llmbuilder\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; llmbuilder.interactive_cli(\n        ...     model_path=\"./output/checkpoints/model.pt\",\n        ...     tokenizer_path=\"./output/tokenizer/\",\n        ...     temperature=0.8\n        ... )\n    \"\"\"\n    from .inference import interactive_cli as _interactive_cli\n\n    _interactive_cli(model_path, tokenizer_path, **kwargs)\n</code></pre>"},{"location":"api/core/#fine-tuning-functions","title":"Fine-tuning Functions","text":""},{"location":"api/core/#llmbuilder.finetune_model","title":"<code>llmbuilder.finetune_model(model, dataset, config, **kwargs)</code>","text":"<p>Fine-tune a model with the given dataset and configuration.</p> Source code in <code>llmbuilder\\__init__.py</code> <pre><code>def finetune_model(model: Any, dataset: Any, config: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Fine-tune a model with the given dataset and configuration.\"\"\"\n    from .finetune import finetune_model as _finetune_model\n\n    return _finetune_model(model, dataset, config, **kwargs)\n</code></pre>"},{"location":"api/core/#quick-examples","title":"\ud83d\ude80 Quick Examples","text":""},{"location":"api/core/#basic-training-pipeline","title":"Basic Training Pipeline","text":"<pre><code>import llmbuilder as lb\n\n# 1. Load configuration\nconfig = lb.load_config(preset=\"cpu_small\")\n\n# 2. Build model\nmodel = lb.build_model(config.model)\n\n# 3. Prepare dataset\nfrom llmbuilder.data import TextDataset\ndataset = TextDataset(\"training_data.txt\", block_size=config.model.max_seq_length)\n\n# 4. Train model\nresults = lb.train_model(model, dataset, config.training)\n\n# 5. Generate text\ntext = lb.generate_text(\n    model_path=\"./checkpoints/model.pt\",\n    tokenizer_path=\"./tokenizers\",\n    prompt=\"Hello world\",\n    max_new_tokens=50\n)\n</code></pre>"},{"location":"api/core/#high-level-training","title":"High-Level Training","text":"<pre><code>import llmbuilder as lb\n\n# Complete training pipeline in one function\npipeline = lb.train(\n    data_path=\"./my_data/\",\n    output_dir=\"./output/\",\n    config={\n        \"model\": {\"num_layers\": 8, \"embedding_dim\": 512},\n        \"training\": {\"num_epochs\": 10, \"batch_size\": 16}\n    }\n)\n\n# Generate text after training\ntext = pipeline.generate(\"The future of AI is\")\n</code></pre>"},{"location":"api/core/#interactive-generation","title":"Interactive Generation","text":"<pre><code>import llmbuilder as lb\n\n# Start interactive text generation\nlb.interactive_cli(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer/\",\n    temperature=0.8,\n    max_new_tokens=100\n)\n</code></pre>"},{"location":"api/core/#advanced-usage","title":"\ud83d\udd27 Advanced Usage","text":""},{"location":"api/core/#custom-configuration","title":"Custom Configuration","text":"<pre><code>import llmbuilder as lb\nfrom llmbuilder.config import Config, ModelConfig, TrainingConfig\n\n# Create custom configuration\nconfig = Config(\n    model=ModelConfig(\n        vocab_size=32000,\n        num_layers=24,\n        num_heads=16,\n        embedding_dim=1024\n    ),\n    training=TrainingConfig(\n        batch_size=8,\n        learning_rate=1e-4,\n        num_epochs=20\n    )\n)\n\n# Use with core functions\nmodel = lb.build_model(config.model)\n</code></pre>"},{"location":"api/core/#error-handling","title":"Error Handling","text":"<pre><code>import llmbuilder as lb\nfrom llmbuilder.utils import ModelError, DataError\n\ntry:\n    config = lb.load_config(\"config.json\")\n    model = lb.build_model(config.model)\nexcept ModelError as e:\n    print(f\"Model error: {e}\")\nexcept DataError as e:\n    print(f\"Data error: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"api/core/#return-types","title":"\ud83d\udcca Return Types","text":""},{"location":"api/core/#training-results","title":"Training Results","text":"<pre><code>results = lb.train_model(model, dataset, config)\n\n# Access training metrics\nprint(f\"Final loss: {results.final_loss}\")\nprint(f\"Training time: {results.training_time}\")\nprint(f\"Best validation loss: {results.best_val_loss}\")\nprint(f\"Model path: {results.model_path}\")\n</code></pre>"},{"location":"api/core/#generation-results","title":"Generation Results","text":"<pre><code># Simple string return\ntext = lb.generate_text(model_path, tokenizer_path, prompt)\n\n# With detailed results\nfrom llmbuilder.inference import generate_with_details\n\nresult = generate_with_details(\n    model_path=model_path,\n    tokenizer_path=tokenizer_path,\n    prompt=prompt,\n    return_details=True\n)\n\nprint(f\"Generated text: {result.text}\")\nprint(f\"Generation time: {result.generation_time}\")\nprint(f\"Tokens per second: {result.tokens_per_second}\")\n</code></pre>"},{"location":"api/core/#best-practices","title":"\ud83c\udfaf Best Practices","text":""},{"location":"api/core/#1-configuration-management","title":"1. Configuration Management","text":"<pre><code># Use presets as starting points\nconfig = lb.load_config(preset=\"gpu_medium\")\n\n# Modify specific settings\nconfig.model.num_layers = 16\nconfig.training.learning_rate = 5e-5\n\n# Save for reuse\nconfig.save(\"my_config.json\")\n</code></pre>"},{"location":"api/core/#2-resource-management","title":"2. Resource Management","text":"<pre><code># Check available resources\nfrom llmbuilder.utils import get_device_info\n\ndevice_info = get_device_info()\nif device_info.has_cuda:\n    config = lb.load_config(preset=\"gpu_medium\")\nelse:\n    config = lb.load_config(preset=\"cpu_small\")\n</code></pre>"},{"location":"api/core/#3-error-recovery","title":"3. Error Recovery","text":"<pre><code># Implement checkpointing\ntry:\n    results = lb.train_model(model, dataset, config)\nexcept KeyboardInterrupt:\n    print(\"Training interrupted, saving checkpoint...\")\n    # Checkpoint is automatically saved\nexcept Exception as e:\n    print(f\"Training failed: {e}\")\n    # Resume from last checkpoint if available\n</code></pre> <p>Core API Tips</p> <ul> <li>Start with high-level functions and move to lower-level APIs as needed</li> <li>Use configuration presets as starting points</li> <li>Always handle exceptions appropriately</li> <li>Take advantage of automatic checkpointing for long training runs</li> </ul>"},{"location":"api/data-processing/","title":"Data Processing API Reference","text":""},{"location":"api/data-processing/#core-classes","title":"Core Classes","text":""},{"location":"api/data-processing/#ingestionpipeline","title":"IngestionPipeline","text":"<pre><code>from llmbuilder.data.ingest import IngestionPipeline\n\npipeline = IngestionPipeline(config)\npipeline.process_directory(\"data/raw/\")\n</code></pre>"},{"location":"api/data-processing/#deduplicationpipeline","title":"DeduplicationPipeline","text":"<pre><code>from llmbuilder.data.dedup import DeduplicationPipeline\n\ndedup = DeduplicationPipeline(config)\ndedup.process_files([\"file1.txt\", \"file2.txt\"])\n</code></pre>"},{"location":"api/data-processing/#tokenizertrainer","title":"TokenizerTrainer","text":"<pre><code>from llmbuilder.training.tokenizer import TokenizerTrainer\n\ntrainer = TokenizerTrainer(config)\ntrainer.train(corpus_files, output_dir)\n</code></pre>"},{"location":"api/data-processing/#ggufconverter","title":"GGUFConverter","text":"<pre><code>from llmbuilder.tools.convert_to_gguf import GGUFConverter\n\nconverter = GGUFConverter()\nconverter.convert(model_path, output_path)\n</code></pre>"},{"location":"api/data-processing/#configuration","title":"Configuration","text":"<p>All components use the unified config system:</p> <pre><code>from llmbuilder.config import ConfigManager\n\nconfig = ConfigManager.load_config(\"config.json\")\n</code></pre>"},{"location":"cli/overview/","title":"CLI Overview","text":"<p>LLMBuilder provides a comprehensive command-line interface (CLI) that makes it easy to train, fine-tune, and deploy language models without writing code. This guide covers all CLI commands and their usage.</p>"},{"location":"cli/overview/#getting-started","title":"\ud83d\ude80 Getting Started","text":""},{"location":"cli/overview/#installation-verification","title":"Installation Verification","text":"<p>First, verify that LLMBuilder is properly installed:</p> <pre><code>llmbuilder --version\nllmbuilder --help\n</code></pre>"},{"location":"cli/overview/#welcome-command","title":"Welcome Command","text":"<p>For first-time users, start with the welcome command:</p> <pre><code>llmbuilder welcome\n</code></pre> <p>This interactive command guides you through:</p> <ul> <li>Learning about LLMBuilder</li> <li>Creating configuration files</li> <li>Processing data</li> <li>Training models</li> <li>Generating text</li> </ul>"},{"location":"cli/overview/#command-structure","title":"\ud83d\udccb Command Structure","text":"<p>LLMBuilder CLI follows a hierarchical command structure:</p> <pre><code>llmbuilder [GLOBAL_OPTIONS] COMMAND [COMMAND_OPTIONS] [ARGS]\n</code></pre>"},{"location":"cli/overview/#global-options","title":"Global Options","text":"Option Description <code>--version</code> Show version and exit <code>--verbose</code>, <code>-v</code> Enable verbose output <code>--help</code> Show help message"},{"location":"cli/overview/#main-commands","title":"Main Commands","text":"Command Description <code>welcome</code> Interactive getting started guide <code>info</code> Display package information <code>config</code> Configuration management <code>data</code> Data processing and loading <code>train</code> Model training <code>finetune</code> Model fine-tuning <code>generate</code> Text generation <code>model</code> Model management <code>export</code> Model export utilities"},{"location":"cli/overview/#command-categories","title":"\ud83c\udfaf Command Categories","text":""},{"location":"cli/overview/#information-commands","title":"Information Commands","text":""},{"location":"cli/overview/#welcome","title":"<code>welcome</code>","text":"<p>Interactive getting started experience:</p> <pre><code>llmbuilder welcome\n</code></pre> <p>Features:</p> <ul> <li>Guided setup process</li> <li>Learn about LLMBuilder capabilities</li> <li>Quick access to common tasks</li> <li>Beginner-friendly explanations</li> </ul>"},{"location":"cli/overview/#info","title":"<code>info</code>","text":"<p>Display package information and credits:</p> <pre><code>llmbuilder info\n</code></pre> <p>Shows:</p> <ul> <li>Package version and description</li> <li>Available modules and their purposes</li> <li>Quick command examples</li> <li>Links to documentation and support</li> </ul>"},{"location":"cli/overview/#configuration-commands","title":"Configuration Commands","text":""},{"location":"cli/overview/#config-create","title":"<code>config create</code>","text":"<p>Create configuration files with presets:</p> <pre><code># Interactive configuration creation\nllmbuilder config create --interactive\n\n# Create from preset\nllmbuilder config create --preset cpu_small --output config.json\n\n# Available presets: cpu_small, gpu_medium, gpu_large, inference\n</code></pre>"},{"location":"cli/overview/#config-validate","title":"<code>config validate</code>","text":"<p>Validate configuration files:</p> <pre><code>llmbuilder config validate config.json\n</code></pre>"},{"location":"cli/overview/#config-list","title":"<code>config list</code>","text":"<p>List available configuration presets:</p> <pre><code>llmbuilder config list\n</code></pre>"},{"location":"cli/overview/#data-processing-commands","title":"Data Processing Commands","text":""},{"location":"cli/overview/#data-load","title":"<code>data load</code>","text":"<p>Load and preprocess text data from various formats:</p> <pre><code># Interactive data loading\nllmbuilder data load --interactive\n\n# Process specific directory\nllmbuilder data load \\\n  --input ./documents \\\n  --output clean_text.txt \\\n  --format all \\\n  --clean \\\n  --min-length 100\n</code></pre>"},{"location":"cli/overview/#data-tokenizer","title":"<code>data tokenizer</code>","text":"<p>Train tokenizers on text data:</p> <pre><code>llmbuilder data tokenizer \\\n  --input training_data.txt \\\n  --output ./tokenizer \\\n  --vocab-size 16000 \\\n  --model-type bpe\n</code></pre>"},{"location":"cli/overview/#training-commands","title":"Training Commands","text":""},{"location":"cli/overview/#train-model","title":"<code>train model</code>","text":"<p>Train language models from scratch:</p> <pre><code># Interactive training setup\nllmbuilder train model --interactive\n\n# Direct training\nllmbuilder train model \\\n  --config config.json \\\n  --data training_data.txt \\\n  --tokenizer ./tokenizer \\\n  --output ./model \\\n  --epochs 10 \\\n  --batch-size 16\n</code></pre>"},{"location":"cli/overview/#train-resume","title":"<code>train resume</code>","text":"<p>Resume training from checkpoints:</p> <pre><code>llmbuilder train resume \\\n  --checkpoint ./model/checkpoint_1000.pt \\\n  --data training_data.txt \\\n  --output ./continued_model\n</code></pre>"},{"location":"cli/overview/#fine-tuning-commands","title":"Fine-tuning Commands","text":""},{"location":"cli/overview/#finetune-model","title":"<code>finetune model</code>","text":"<p>Fine-tune pre-trained models:</p> <pre><code>llmbuilder finetune model \\\n  --model ./pretrained_model/model.pt \\\n  --dataset domain_data.txt \\\n  --output ./finetuned_model \\\n  --epochs 5 \\\n  --lr 5e-5 \\\n  --use-lora\n</code></pre>"},{"location":"cli/overview/#generation-commands","title":"Generation Commands","text":""},{"location":"cli/overview/#generate-text","title":"<code>generate text</code>","text":"<p>Generate text with trained models:</p> <pre><code># Interactive generation\nllmbuilder generate text --setup\n\n# Direct generation\nllmbuilder generate text \\\n  --model ./model/model.pt \\\n  --tokenizer ./tokenizer \\\n  --prompt \"The future of AI is\" \\\n  --max-tokens 100 \\\n  --temperature 0.8\n\n# Interactive chat mode\nllmbuilder generate text \\\n  --model ./model/model.pt \\\n  --tokenizer ./tokenizer \\\n  --interactive\n</code></pre>"},{"location":"cli/overview/#model-management-commands","title":"Model Management Commands","text":""},{"location":"cli/overview/#model-create","title":"<code>model create</code>","text":"<p>Create new model architectures:</p> <pre><code>llmbuilder model create \\\n  --vocab-size 16000 \\\n  --layers 12 \\\n  --heads 12 \\\n  --dim 768 \\\n  --output ./new_model\n</code></pre>"},{"location":"cli/overview/#model-info","title":"<code>model info</code>","text":"<p>Display model information:</p> <pre><code>llmbuilder model info ./model/model.pt\n</code></pre>"},{"location":"cli/overview/#model-evaluate","title":"<code>model evaluate</code>","text":"<p>Evaluate model performance:</p> <pre><code>llmbuilder model evaluate \\\n  ./model/model.pt \\\n  --dataset test_data.txt \\\n  --batch-size 32\n</code></pre>"},{"location":"cli/overview/#export-commands","title":"Export Commands","text":""},{"location":"cli/overview/#export-gguf","title":"<code>export gguf</code>","text":"<p>Export models to GGUF format:</p> <pre><code>llmbuilder export gguf \\\n  ./model/model.pt \\\n  --output model.gguf \\\n  --quantization q4_0\n</code></pre>"},{"location":"cli/overview/#export-onnx","title":"<code>export onnx</code>","text":"<p>Export models to ONNX format:</p> <pre><code>llmbuilder export onnx \\\n  ./model/model.pt \\\n  --output model.onnx \\\n  --opset 11\n</code></pre>"},{"location":"cli/overview/#export-quantize","title":"<code>export quantize</code>","text":"<p>Quantize models for deployment:</p> <pre><code>llmbuilder export quantize \\\n  ./model/model.pt \\\n  --output quantized_model.pt \\\n  --method dynamic \\\n  --bits 8\n</code></pre>"},{"location":"cli/overview/#interactive-features","title":"\ud83c\udfa8 Interactive Features","text":""},{"location":"cli/overview/#guided-setup","title":"Guided Setup","text":"<p>Many commands support <code>--interactive</code> or <code>--setup</code> flags for guided experiences:</p> <pre><code># Interactive data loading\nllmbuilder data load --interactive\n\n# Interactive model training\nllmbuilder train model --interactive\n\n# Interactive text generation setup\nllmbuilder generate text --setup\n</code></pre>"},{"location":"cli/overview/#progress-indicators","title":"Progress Indicators","text":"<p>LLMBuilder provides rich progress indicators:</p> <pre><code># Training progress with real-time metrics\nllmbuilder train model --data data.txt --output model/ --verbose\n\n# Data processing with progress bars\nllmbuilder data load --input docs/ --output data.txt --verbose\n</code></pre>"},{"location":"cli/overview/#colorful-output","title":"Colorful Output","text":"<p>The CLI uses colors and emojis for better user experience:</p> <ul> <li>\ud83d\udfe2 Green: Success messages</li> <li>\ud83d\udd35 Blue: Information and headers</li> <li>\ud83d\udfe1 Yellow: Warnings and prompts</li> <li>\ud83d\udd34 Red: Errors</li> <li>\ud83c\udfaf Emojis: Visual indicators for different operations</li> </ul>"},{"location":"cli/overview/#advanced-usage","title":"\ud83d\udd27 Advanced Usage","text":""},{"location":"cli/overview/#configuration-files","title":"Configuration Files","text":"<p>Use configuration files for complex setups:</p> <pre><code># Create configuration\nllmbuilder config create --preset gpu_medium --output training_config.json\n\n# Use configuration in training\nllmbuilder train model --config training_config.json --data data.txt --output model/\n</code></pre>"},{"location":"cli/overview/#environment-variables","title":"Environment Variables","text":"<p>Set environment variables for default behavior:</p> <pre><code># Set default device\nexport LLMBUILDER_DEVICE=cuda\n\n# Set cache directory\nexport LLMBUILDER_CACHE_DIR=/path/to/cache\n\n# Enable debug logging\nexport LLMBUILDER_LOG_LEVEL=DEBUG\n</code></pre>"},{"location":"cli/overview/#batch-processing","title":"Batch Processing","text":"<p>Process multiple files or configurations:</p> <pre><code># Process multiple data directories\nllmbuilder data load \\\n  --input \"dir1,dir2,dir3\" \\\n  --output combined_data.txt\n\n# Train multiple model variants\nfor preset in cpu_small gpu_medium gpu_large; do\n  llmbuilder config create --preset $preset --output ${preset}_config.json\n  llmbuilder train model --config ${preset}_config.json --data data.txt --output ${preset}_model/\ndone\n</code></pre>"},{"location":"cli/overview/#pipeline-automation","title":"Pipeline Automation","text":"<p>Chain commands for complete workflows:</p> <pre><code>#!/bin/bash\n# Complete training pipeline\n\n# 1. Process data\nllmbuilder data load \\\n  --input ./raw_documents \\\n  --output training_data.txt \\\n  --clean --min-length 100\n\n# 2. Train tokenizer\nllmbuilder data tokenizer \\\n  --input training_data.txt \\\n  --output ./tokenizer \\\n  --vocab-size 16000\n\n# 3. Create configuration\nllmbuilder config create \\\n  --preset gpu_medium \\\n  --output model_config.json\n\n# 4. Train model\nllmbuilder train model \\\n  --config model_config.json \\\n  --data training_data.txt \\\n  --tokenizer ./tokenizer \\\n  --output ./trained_model\n\n# 5. Test generation\nllmbuilder generate text \\\n  --model ./trained_model/model.pt \\\n  --tokenizer ./tokenizer \\\n  --prompt \"Test generation\" \\\n  --max-tokens 50\n\necho \"Training pipeline completed!\"\n</code></pre>"},{"location":"cli/overview/#error-handling","title":"\ud83d\udea8 Error Handling","text":""},{"location":"cli/overview/#common-error-messages","title":"Common Error Messages","text":""},{"location":"cli/overview/#configuration-errors","title":"Configuration Errors","text":"<pre><code>\u274c Configuration validation failed: num_heads (8) must divide embedding_dim (512)\n\ud83d\udca1 Try: Set num_heads to 4, 8, or 16\n</code></pre>"},{"location":"cli/overview/#data-errors","title":"Data Errors","text":"<pre><code>\u274c No supported files found in directory: ./documents\n\ud83d\udca1 Supported formats: .txt, .pdf, .docx, .html, .md\n</code></pre>"},{"location":"cli/overview/#memory-errors","title":"Memory Errors","text":"<pre><code>\u274c CUDA out of memory\n\ud83d\udca1 Try: Reduce batch size with --batch-size 4 or use CPU with --device cpu\n</code></pre>"},{"location":"cli/overview/#model-errors","title":"Model Errors","text":"<pre><code>\u274c Model file not found: ./model/model.pt\n\ud83d\udca1 Check the model path or train a model first with: llmbuilder train model\n</code></pre>"},{"location":"cli/overview/#debugging-tips","title":"Debugging Tips","text":"<p>Enable verbose output for detailed information:</p> <pre><code>llmbuilder --verbose train model --data data.txt --output model/\n</code></pre> <p>Check system information:</p> <pre><code>llmbuilder info --system\n</code></pre> <p>Validate configurations before use:</p> <pre><code>llmbuilder config validate config.json --strict\n</code></pre>"},{"location":"cli/overview/#output-and-logging","title":"\ud83d\udcca Output and Logging","text":""},{"location":"cli/overview/#standard-output","title":"Standard Output","text":"<p>LLMBuilder provides structured output:</p> <pre><code>\ud83d\ude80 Starting model training...\n\ud83d\udcca Dataset: 10,000 samples\n\ud83e\udde0 Model: 12.5M parameters\n\ud83d\udcc8 Training progress:\n  Epoch 1/10: loss=3.45, lr=0.0003, time=2m 15s\n  Epoch 2/10: loss=2.87, lr=0.0003, time=2m 12s\n  ...\n\u2705 Training completed successfully!\n\ud83d\udcbe Model saved to: ./model/model.pt\n</code></pre>"},{"location":"cli/overview/#log-files","title":"Log Files","text":"<p>Training and processing logs are automatically saved:</p> <pre><code>./model/\n\u251c\u2500\u2500 model.pt              # Trained model\n\u251c\u2500\u2500 config.json           # Training configuration\n\u251c\u2500\u2500 training.log          # Detailed training logs\n\u251c\u2500\u2500 metrics.json          # Training metrics\n\u2514\u2500\u2500 checkpoints/          # Training checkpoints\n    \u251c\u2500\u2500 checkpoint_1000.pt\n    \u251c\u2500\u2500 checkpoint_2000.pt\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"cli/overview/#json-output","title":"JSON Output","text":"<p>Use <code>--json</code> flag for machine-readable output:</p> <pre><code>llmbuilder model info ./model/model.pt --json\n</code></pre> <pre><code>{\n  \"model_path\": \"./model/model.pt\",\n  \"parameters\": 12500000,\n  \"architecture\": {\n    \"num_layers\": 12,\n    \"num_heads\": 12,\n    \"embedding_dim\": 768,\n    \"vocab_size\": 16000\n  },\n  \"training_info\": {\n    \"final_loss\": 2.45,\n    \"training_time\": \"45m 23s\",\n    \"epochs\": 10\n  }\n}\n</code></pre>"},{"location":"cli/overview/#best-practices","title":"\ud83c\udfaf Best Practices","text":""},{"location":"cli/overview/#1-start-interactive","title":"1. Start Interactive","text":"<p>For new users, always start with interactive modes:</p> <pre><code>llmbuilder welcome\nllmbuilder data load --interactive\nllmbuilder train model --interactive\n</code></pre>"},{"location":"cli/overview/#2-use-configurations","title":"2. Use Configurations","text":"<p>Save and reuse configurations for consistency:</p> <pre><code># Create and save configuration\nllmbuilder config create --preset gpu_medium --output my_config.json\n\n# Reuse configuration\nllmbuilder train model --config my_config.json --data data.txt --output model/\n</code></pre>"},{"location":"cli/overview/#3-validate-before-training","title":"3. Validate Before Training","text":"<p>Always validate configurations and data:</p> <pre><code>llmbuilder config validate config.json\nllmbuilder data load --input data/ --output test.txt --dry-run\n</code></pre>"},{"location":"cli/overview/#4-monitor-progress","title":"4. Monitor Progress","text":"<p>Use verbose mode for long-running operations:</p> <pre><code>llmbuilder --verbose train model --config config.json --data data.txt --output model/\n</code></pre>"},{"location":"cli/overview/#5-save-intermediate-results","title":"5. Save Intermediate Results","text":"<p>Use checkpointing and intermediate saves:</p> <pre><code>llmbuilder train model \\\n  --config config.json \\\n  --data data.txt \\\n  --output model/ \\\n  --save-every 1000 \\\n  --eval-every 500\n</code></pre> <p>CLI Tips</p> <ul> <li>Use tab completion if available in your shell</li> <li>Combine <code>--help</code> with any command to see all options</li> <li>Use <code>--dry-run</code> flags when available to test commands</li> <li>Save successful command combinations as shell scripts</li> <li>Use configuration files for complex setups</li> </ul>"},{"location":"development/code-style/","title":"Code Style Guide","text":"<p>This guide outlines the coding standards and style conventions for LLMBuilder to ensure consistent, readable, and maintainable code.</p>"},{"location":"development/code-style/#overview","title":"Overview","text":"<p>LLMBuilder follows PEP 8 with some modifications and uses automated tools to enforce consistency:</p> <ul> <li>Black for code formatting</li> <li>isort for import sorting</li> <li>flake8 for style checking</li> <li>mypy for type checking</li> <li>pre-commit for automated checks</li> </ul>"},{"location":"development/code-style/#automated-formatting","title":"Automated Formatting","text":""},{"location":"development/code-style/#setup","title":"Setup","text":"<pre><code># Install development dependencies\npip install -e \".[dev]\"\n\n# Set up pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"development/code-style/#usage","title":"Usage","text":"<pre><code># Format code automatically\nblack llmbuilder/ tests/ examples/\nisort llmbuilder/ tests/ examples/\n\n# Check style\nflake8 llmbuilder/ tests/\nmypy llmbuilder/\n\n# Run all pre-commit checks\npre-commit run --all-files\n</code></pre>"},{"location":"development/code-style/#python-code-style","title":"Python Code Style","text":""},{"location":"development/code-style/#general-principles","title":"General Principles","text":"<ol> <li>Readability counts - Code is read more often than written</li> <li>Explicit is better than implicit - Be clear about intentions</li> <li>Simple is better than complex - Prefer straightforward solutions</li> <li>Consistency matters - Follow established patterns in the codebase</li> </ol>"},{"location":"development/code-style/#naming-conventions","title":"Naming Conventions","text":"<pre><code># Classes: PascalCase\nclass DataProcessor:\n    pass\n\nclass HTMLProcessor(DataProcessor):\n    pass\n\n# Functions and methods: snake_case\ndef process_documents():\n    pass\n\ndef load_configuration_file():\n    pass\n\n# Variables: snake_case\ninput_file = \"data.txt\"\nprocessing_results = []\nuser_config = {}\n\n# Constants: UPPER_CASE\nMAX_BATCH_SIZE = 1000\nDEFAULT_TIMEOUT = 30\nSUPPORTED_FORMATS = [\"html\", \"pdf\", \"txt\"]\n\n# Private attributes: leading underscore\nclass MyClass:\n    def __init__(self):\n        self._internal_state = {}\n        self.__private_data = []  # Name mangling for truly private\n\n# Type variables: PascalCase with T suffix\nfrom typing import TypeVar\nT = TypeVar('T')\nConfigT = TypeVar('ConfigT', bound='BaseConfig')\n</code></pre>"},{"location":"development/code-style/#function-and-method-design","title":"Function and Method Design","text":"<pre><code>def process_file(\n    input_path: str,\n    output_path: str,\n    config: Optional[ProcessingConfig] = None,\n    *,  # Force keyword-only arguments after this\n    validate_output: bool = True,\n    progress_callback: Optional[Callable[[int], None]] = None\n) -&gt; ProcessingResult:\n    \"\"\"\n    Process a single file with the specified configuration.\n\n    Args:\n        input_path: Path to the input file\n        output_path: Path where processed output will be saved\n        config: Processing configuration (uses default if None)\n        validate_output: Whether to validate the output file\n        progress_callback: Optional callback for progress updates\n\n    Returns:\n        ProcessingResult with statistics and status information\n\n    Raises:\n        FileNotFoundError: If input_path doesn't exist\n        PermissionError: If unable to write to output_path\n        ProcessingError: If processing fails\n\n    Example:\n        &gt;&gt;&gt; config = ProcessingConfig(batch_size=100)\n        &gt;&gt;&gt; result = process_file(\"input.txt\", \"output.txt\", config)\n        &gt;&gt;&gt; print(f\"Processed {result.items_count} items\")\n    \"\"\"\n    # Implementation here\n    pass\n</code></pre>"},{"location":"development/code-style/#class-design","title":"Class Design","text":"<pre><code>from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Protocol, runtime_checkable\n\n# Use dataclasses for simple data containers\n@dataclass\nclass ProcessingResult:\n    \"\"\"Result of a processing operation.\"\"\"\n    success: bool\n    items_processed: int\n    processing_time: float\n    error_message: Optional[str] = None\n\n    def __post_init__(self):\n        \"\"\"Validate the result after initialization.\"\"\"\n        if self.items_processed &lt; 0:\n            raise ValueError(\"items_processed cannot be negative\")\n\n# Use ABC for base classes with abstract methods\nclass BaseProcessor(ABC):\n    \"\"\"Base class for all document processors.\"\"\"\n\n    def __init__(self, config: ProcessorConfig):\n        self.config = config\n        self._stats = ProcessingStats()\n\n    @abstractmethod\n    def process_file(self, file_path: str) -&gt; ProcessingResult:\n        \"\"\"Process a single file.\"\"\"\n        pass\n\n    @property\n    def stats(self) -&gt; ProcessingStats:\n        \"\"\"Get processing statistics.\"\"\"\n        return self._stats\n\n# Use Protocol for structural typing\n@runtime_checkable\nclass Configurable(Protocol):\n    \"\"\"Protocol for objects that can be configured.\"\"\"\n\n    def configure(self, config: Dict[str, Any]) -&gt; None:\n        \"\"\"Configure the object with the given settings.\"\"\"\n        ...\n\n    def get_config(self) -&gt; Dict[str, Any]:\n        \"\"\"Get current configuration.\"\"\"\n        ...\n</code></pre>"},{"location":"development/code-style/#error-handling","title":"Error Handling","text":"<pre><code># Define custom exceptions\nclass LLMBuilderError(Exception):\n    \"\"\"Base exception for LLMBuilder.\"\"\"\n    pass\n\nclass ProcessingError(LLMBuilderError):\n    \"\"\"Raised when processing fails.\"\"\"\n    pass\n\nclass ConfigurationError(LLMBuilderError):\n    \"\"\"Raised when configuration is invalid.\"\"\"\n    pass\n\n# Use specific exception handling\ndef process_document(file_path: str) -&gt; ProcessingResult:\n    \"\"\"Process a document with proper error handling.\"\"\"\n    try:\n        # Validate input\n        if not Path(file_path).exists():\n            raise FileNotFoundError(f\"File not found: {file_path}\")\n\n        # Process file\n        result = _do_processing(file_path)\n\n        # Validate result\n        if not result.success:\n            raise ProcessingError(f\"Processing failed: {result.error_message}\")\n\n        return result\n\n    except FileNotFoundError:\n        # Re-raise file system errors as-is\n        raise\n    except ProcessingError:\n        # Re-raise processing errors as-is\n        raise\n    except Exception as e:\n        # Wrap unexpected errors\n        raise ProcessingError(f\"Unexpected error processing {file_path}: {e}\") from e\n\n# Use context managers for resource management\nfrom contextlib import contextmanager\n\n@contextmanager\ndef processing_context(config: ProcessingConfig):\n    \"\"\"Context manager for processing operations.\"\"\"\n    logger.info(\"Starting processing context\")\n    try:\n        # Setup\n        setup_processing(config)\n        yield\n    except Exception as e:\n        logger.error(f\"Processing failed: {e}\")\n        raise\n    finally:\n        # Cleanup\n        cleanup_processing()\n        logger.info(\"Processing context closed\")\n</code></pre>"},{"location":"development/code-style/#type-hints","title":"Type Hints","text":"<pre><code>from typing import (\n    Any, Dict, List, Optional, Union, Tuple, Callable,\n    TypeVar, Generic, Protocol, Literal, overload\n)\nfrom pathlib import Path\n\n# Use specific types\ndef load_config(config_path: Union[str, Path]) -&gt; Dict[str, Any]:\n    \"\"\"Load configuration from file.\"\"\"\n    pass\n\n# Use generics for reusable code\nT = TypeVar('T')\n\nclass Cache(Generic[T]):\n    \"\"\"Generic cache implementation.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self._data: Dict[str, T] = {}\n\n    def get(self, key: str) -&gt; Optional[T]:\n        \"\"\"Get item from cache.\"\"\"\n        return self._data.get(key)\n\n    def set(self, key: str, value: T) -&gt; None:\n        \"\"\"Set item in cache.\"\"\"\n        self._data[key] = value\n\n# Use Literal for constrained values\nProcessingMode = Literal[\"fast\", \"accurate\", \"balanced\"]\n\ndef set_processing_mode(mode: ProcessingMode) -&gt; None:\n    \"\"\"Set the processing mode.\"\"\"\n    pass\n\n# Use overload for different signatures\n@overload\ndef process_data(data: str) -&gt; str: ...\n\n@overload\ndef process_data(data: List[str]) -&gt; List[str]: ...\n\ndef process_data(data: Union[str, List[str]]) -&gt; Union[str, List[str]]:\n    \"\"\"Process data with different input types.\"\"\"\n    if isinstance(data, str):\n        return _process_single(data)\n    else:\n        return [_process_single(item) for item in data]\n</code></pre>"},{"location":"development/code-style/#documentation","title":"Documentation","text":"<pre><code>def complex_processing_function(\n    input_data: List[Dict[str, Any]],\n    config: ProcessingConfig,\n    *,\n    parallel: bool = True,\n    max_workers: Optional[int] = None,\n    progress_callback: Optional[Callable[[float], None]] = None\n) -&gt; ProcessingResult:\n    \"\"\"\n    Process complex data with configurable parallelization.\n\n    This function processes a list of data items according to the provided\n    configuration. It supports parallel processing for improved performance\n    on multi-core systems.\n\n    Args:\n        input_data: List of data items to process. Each item should be a\n            dictionary with required keys 'id' and 'content'.\n        config: Processing configuration specifying how to handle the data.\n            Must include 'batch_size' and 'processing_mode' settings.\n        parallel: Whether to use parallel processing. Defaults to True.\n            Set to False for debugging or when processing order matters.\n        max_workers: Maximum number of worker processes. If None, uses\n            the number of CPU cores. Ignored if parallel=False.\n        progress_callback: Optional callback function that receives progress\n            updates as a float between 0.0 and 1.0.\n\n    Returns:\n        ProcessingResult containing:\n        - success: Whether all items were processed successfully\n        - items_processed: Number of items successfully processed\n        - processing_time: Total processing time in seconds\n        - error_message: Error description if success=False\n        - detailed_stats: Dictionary with per-item processing statistics\n\n    Raises:\n        ValueError: If input_data is empty or contains invalid items\n        ConfigurationError: If config is missing required settings\n        ProcessingError: If processing fails for any item\n\n    Example:\n        Basic usage:\n        &gt;&gt;&gt; data = [{\"id\": \"1\", \"content\": \"text1\"}, {\"id\": \"2\", \"content\": \"text2\"}]\n        &gt;&gt;&gt; config = ProcessingConfig(batch_size=10, processing_mode=\"fast\")\n        &gt;&gt;&gt; result = complex_processing_function(data, config)\n        &gt;&gt;&gt; print(f\"Processed {result.items_processed} items in {result.processing_time:.2f}s\")\n\n        With progress tracking:\n        &gt;&gt;&gt; def progress_handler(progress: float):\n        ...     print(f\"Progress: {progress:.1%}\")\n        &gt;&gt;&gt; result = complex_processing_function(\n        ...     data, config, progress_callback=progress_handler\n        ... )\n\n        Sequential processing for debugging:\n        &gt;&gt;&gt; result = complex_processing_function(data, config, parallel=False)\n\n    Note:\n        When using parallel processing, ensure that your data items are\n        independent and don't require shared state. The function uses\n        multiprocessing, so large data items may impact performance due\n        to serialization overhead.\n\n    See Also:\n        - ProcessingConfig: Configuration options\n        - ProcessingResult: Return value structure\n        - simple_processing_function: For basic use cases\n    \"\"\"\n    # Implementation here\n    pass\n</code></pre>"},{"location":"development/code-style/#import-organization","title":"Import Organization","text":"<pre><code># Standard library imports (alphabetical)\nimport json\nimport logging\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\n# Third-party imports (alphabetical)\nimport click\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer\n\n# Local imports (alphabetical)\nfrom llmbuilder.config import Config, ModelConfig\nfrom llmbuilder.data import DataLoader, TextProcessor\nfrom llmbuilder.utils import setup_logging\n\n# Relative imports (if needed, but prefer absolute)\nfrom .base import BaseProcessor\nfrom .utils import validate_input\n</code></pre>"},{"location":"development/code-style/#testing-style","title":"Testing Style","text":""},{"location":"development/code-style/#test-structure","title":"Test Structure","text":"<pre><code>import pytest\nimport tempfile\nfrom pathlib import Path\nfrom unittest.mock import Mock, patch, MagicMock\n\nfrom llmbuilder.data.processor import DocumentProcessor\nfrom llmbuilder.config import ProcessingConfig\n\nclass TestDocumentProcessor:\n    \"\"\"Test cases for DocumentProcessor.\"\"\"\n\n    def setup_method(self):\n        \"\"\"Set up test environment before each test.\"\"\"\n        self.temp_dir = Path(tempfile.mkdtemp())\n        self.config = ProcessingConfig(batch_size=10)\n        self.processor = DocumentProcessor(self.config)\n\n    def teardown_method(self):\n        \"\"\"Clean up after each test.\"\"\"\n        import shutil\n        shutil.rmtree(self.temp_dir, ignore_errors=True)\n\n    def test_process_valid_document_returns_success(self):\n        \"\"\"Test that processing a valid document returns success.\"\"\"\n        # Arrange\n        document_content = \"This is a test document with valid content.\"\n        document_file = self.temp_dir / \"test.txt\"\n        document_file.write_text(document_content)\n\n        # Act\n        result = self.processor.process_file(str(document_file))\n\n        # Assert\n        assert result.success is True\n        assert result.items_processed == 1\n        assert result.processing_time &gt; 0\n        assert result.error_message is None\n\n    def test_process_nonexistent_file_raises_file_not_found(self):\n        \"\"\"Test that processing non-existent file raises FileNotFoundError.\"\"\"\n        # Arrange\n        nonexistent_file = str(self.temp_dir / \"nonexistent.txt\")\n\n        # Act &amp; Assert\n        with pytest.raises(FileNotFoundError, match=\"File not found\"):\n            self.processor.process_file(nonexistent_file)\n\n    @pytest.mark.parametrize(\"content,expected_items\", [\n        (\"Single line\", 1),\n        (\"Line 1\\nLine 2\", 2),\n        (\"\", 0),\n        (\"Line 1\\n\\nLine 3\", 2),  # Empty lines ignored\n    ])\n    def test_process_file_counts_items_correctly(self, content, expected_items):\n        \"\"\"Test that file processing counts items correctly.\"\"\"\n        # Arrange\n        test_file = self.temp_dir / \"test.txt\"\n        test_file.write_text(content)\n\n        # Act\n        result = self.processor.process_file(str(test_file))\n\n        # Assert\n        assert result.items_processed == expected_items\n\n    def test_process_file_with_mock_dependencies(self):\n        \"\"\"Test file processing with mocked dependencies.\"\"\"\n        # Arrange\n        test_file = self.temp_dir / \"test.txt\"\n        test_file.write_text(\"test content\")\n\n        with patch.object(self.processor, '_validate_content') as mock_validate:\n            mock_validate.return_value = True\n\n            # Act\n            result = self.processor.process_file(str(test_file))\n\n            # Assert\n            assert result.success is True\n            mock_validate.assert_called_once_with(\"test content\")\n\n    @pytest.fixture\n    def sample_documents(self):\n        \"\"\"Provide sample documents for testing.\"\"\"\n        documents = []\n        for i in range(3):\n            doc_file = self.temp_dir / f\"doc_{i}.txt\"\n            doc_file.write_text(f\"Document {i} content\")\n            documents.append(str(doc_file))\n        return documents\n\n    def test_batch_processing_with_fixture(self, sample_documents):\n        \"\"\"Test batch processing using fixture.\"\"\"\n        # Act\n        results = self.processor.process_batch(sample_documents)\n\n        # Assert\n        assert len(results) == 3\n        assert all(result.success for result in results)\n</code></pre>"},{"location":"development/code-style/#test-naming","title":"Test Naming","text":"<pre><code># Good test names - describe what is being tested\ndef test_process_valid_html_file_extracts_text_content():\n    \"\"\"Test that HTML processor extracts text from valid HTML.\"\"\"\n    pass\n\ndef test_process_empty_file_returns_empty_result():\n    \"\"\"Test that processing empty file returns appropriate result.\"\"\"\n    pass\n\ndef test_process_file_with_invalid_encoding_raises_encoding_error():\n    \"\"\"Test that invalid encoding raises appropriate error.\"\"\"\n    pass\n\n# Bad test names - not descriptive\ndef test_process():  # What does it process? What's expected?\n    pass\n\ndef test_html():  # What about HTML?\n    pass\n\ndef test_error():  # What error? When?\n    pass\n</code></pre>"},{"location":"development/code-style/#configuration-and-data-classes","title":"Configuration and Data Classes","text":"<pre><code>from dataclasses import dataclass, field\nfrom typing import List, Dict, Any, Optional\n\n@dataclass\nclass ProcessingConfig:\n    \"\"\"Configuration for document processing operations.\"\"\"\n\n    # Required fields first\n    batch_size: int\n    output_format: str\n\n    # Optional fields with defaults\n    max_workers: int = 4\n    timeout_seconds: int = 300\n    enable_validation: bool = True\n\n    # Complex defaults using field()\n    supported_formats: List[str] = field(\n        default_factory=lambda: [\"txt\", \"html\", \"pdf\"]\n    )\n    processing_options: Dict[str, Any] = field(default_factory=dict)\n\n    def __post_init__(self):\n        \"\"\"Validate configuration after initialization.\"\"\"\n        if self.batch_size &lt;= 0:\n            raise ValueError(\"batch_size must be positive\")\n\n        if self.output_format not in [\"txt\", \"json\", \"xml\"]:\n            raise ValueError(f\"Unsupported output format: {self.output_format}\")\n\n        if self.max_workers &lt;= 0:\n            raise ValueError(\"max_workers must be positive\")\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; 'ProcessingConfig':\n        \"\"\"Create configuration from dictionary.\"\"\"\n        return cls(**data)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert configuration to dictionary.\"\"\"\n        from dataclasses import asdict\n        return asdict(self)\n</code></pre>"},{"location":"development/code-style/#cli-style","title":"CLI Style","text":"<pre><code>import click\nfrom typing import Optional\n\n@click.group()\n@click.version_option()\n@click.option('--verbose', '-v', is_flag=True, help='Enable verbose output')\n@click.option('--config', '-c', type=click.Path(exists=True), help='Configuration file path')\n@click.pass_context\ndef main(ctx: click.Context, verbose: bool, config: Optional[str]):\n    \"\"\"\n    LLMBuilder - A comprehensive toolkit for language model development.\n\n    Use 'llmbuilder COMMAND --help' for more information on specific commands.\n    \"\"\"\n    ctx.ensure_object(dict)\n    ctx.obj['verbose'] = verbose\n    ctx.obj['config'] = config\n\n    if verbose:\n        click.echo(\"Verbose mode enabled\")\n\n@main.command()\n@click.argument('input_file', type=click.Path(exists=True))\n@click.argument('output_file', type=click.Path())\n@click.option('--format', 'output_format',\n              type=click.Choice(['txt', 'json', 'xml']),\n              default='txt',\n              help='Output format')\n@click.option('--batch-size', default=100,\n              help='Processing batch size')\n@click.option('--workers', default=4,\n              help='Number of worker processes')\n@click.option('--dry-run', is_flag=True,\n              help='Show what would be done without executing')\n@click.pass_context\ndef process(ctx: click.Context, input_file: str, output_file: str,\n           output_format: str, batch_size: int, workers: int, dry_run: bool):\n    \"\"\"Process documents with specified options.\"\"\"\n\n    if ctx.obj['verbose']:\n        click.echo(f\"Processing {input_file} -&gt; {output_file}\")\n        click.echo(f\"Format: {output_format}, Batch: {batch_size}, Workers: {workers}\")\n\n    if dry_run:\n        click.echo(\"DRY RUN: Would process file with these settings\")\n        return\n\n    try:\n        # Processing logic here\n        with click.progressbar(length=100, label='Processing') as bar:\n            # Simulate progress\n            for i in range(100):\n                # Do work\n                bar.update(1)\n\n        click.echo(f\"\u2705 Successfully processed {input_file}\")\n\n    except Exception as e:\n        click.echo(f\"\u274c Processing failed: {e}\", err=True)\n        ctx.exit(1)\n</code></pre>"},{"location":"development/code-style/#performance-considerations","title":"Performance Considerations","text":"<pre><code># Use generators for memory efficiency\ndef process_large_file(file_path: str) -&gt; Iterator[ProcessedItem]:\n    \"\"\"Process large file without loading everything into memory.\"\"\"\n    with open(file_path, 'r') as f:\n        for line_num, line in enumerate(f, 1):\n            if line.strip():  # Skip empty lines\n                yield ProcessedItem(\n                    line_number=line_num,\n                    content=line.strip(),\n                    processed_at=datetime.now()\n                )\n\n# Use appropriate data structures\nfrom collections import defaultdict, deque\nfrom typing import DefaultDict\n\n# For counting/grouping\nitem_counts: DefaultDict[str, int] = defaultdict(int)\n\n# For FIFO operations\nprocessing_queue: deque[ProcessingTask] = deque()\n\n# Use context managers for resources\n@contextmanager\ndef managed_processor(config: ProcessingConfig):\n    \"\"\"Context manager for processor lifecycle.\"\"\"\n    processor = None\n    try:\n        processor = create_processor(config)\n        processor.initialize()\n        yield processor\n    finally:\n        if processor:\n            processor.cleanup()\n</code></pre>"},{"location":"development/code-style/#pre-commit-configuration","title":"Pre-commit Configuration","text":"<p>The project uses pre-commit hooks to enforce code quality:</p> <pre><code># .pre-commit-config.yaml\nrepos:\n  - repo: https://github.com/psf/black\n    rev: 23.12.1\n    hooks:\n      - id: black\n        args: [--line-length=88]\n\n  - repo: https://github.com/pycqa/isort\n    rev: 5.13.2\n    hooks:\n      - id: isort\n        args: [--profile=black]\n\n  - repo: https://github.com/pycqa/flake8\n    rev: 7.0.0\n    hooks:\n      - id: flake8\n        args: [--max-line-length=88, --extend-ignore=E203,W503]\n</code></pre>"},{"location":"development/code-style/#ide-configuration","title":"IDE Configuration","text":""},{"location":"development/code-style/#vs-code-settings","title":"VS Code Settings","text":"<pre><code>{\n    \"python.formatting.provider\": \"black\",\n    \"python.linting.enabled\": true,\n    \"python.linting.flake8Enabled\": true,\n    \"python.linting.mypyEnabled\": true,\n    \"editor.formatOnSave\": true,\n    \"editor.codeActionsOnSave\": {\n        \"source.organizeImports\": true\n    },\n    \"python.testing.pytestEnabled\": true\n}\n</code></pre>"},{"location":"development/code-style/#summary","title":"Summary","text":"<p>Following these style guidelines ensures:</p> <ul> <li>Consistency across the codebase</li> <li>Readability for all contributors</li> <li>Maintainability over time</li> <li>Quality through automated checks</li> </ul> <p>The automated tools handle most formatting concerns, allowing you to focus on writing great code. When in doubt, run the pre-commit hooks to catch any style issues before committing.</p> <p>For questions about code style, check existing code for examples or ask in GitHub Discussions.</p>"},{"location":"development/setup/","title":"Development Setup","text":"<p>This guide will help you set up a development environment for contributing to LLMBuilder.</p>"},{"location":"development/setup/#quick-setup","title":"Quick Setup","text":"<p>The fastest way to get started is using our automated setup script:</p> <pre><code># Clone the repository\ngit clone https://github.com/Qubasehq/llmbuilder-package.git\ncd llmbuilder-package\n\n# Run the automated setup script\npython scripts/setup_dev.py --full\n</code></pre> <p>This script will:</p> <ul> <li>Check your Python version</li> <li>Set up a virtual environment (if needed)</li> <li>Install LLMBuilder in development mode</li> <li>Install all development dependencies</li> <li>Install optional dependencies</li> <li>Set up pre-commit hooks</li> <li>Run initial tests and checks</li> </ul>"},{"location":"development/setup/#manual-setup","title":"Manual Setup","text":"<p>If you prefer to set up manually or need more control:</p>"},{"location":"development/setup/#1-prerequisites","title":"1. Prerequisites","text":"<p>Python Requirements:</p> <ul> <li>Python 3.8 or higher</li> <li>pip (latest version recommended)</li> <li>git</li> </ul> <p>System Dependencies (Optional):</p> <ul> <li>Tesseract OCR: For PDF processing with OCR</li> <li>Ubuntu/Debian: <code>sudo apt-get install tesseract-ocr tesseract-ocr-eng</code></li> <li>macOS: <code>brew install tesseract</code></li> <li> <p>Windows: Download from GitHub</p> </li> <li> <p>llama.cpp: For GGUF model conversion</p> </li> </ul> <pre><code>git clone https://github.com/ggerganov/llama.cpp\ncd llama.cpp &amp;&amp; make\nexport PATH=$PATH:$(pwd)\n</code></pre>"},{"location":"development/setup/#2-environment-setup","title":"2. Environment Setup","text":"<pre><code># Clone the repository\ngit clone https://github.com/Qubasehq/llmbuilder-package.git\ncd llmbuilder-package\n\n# Create and activate virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Upgrade pip\npip install --upgrade pip\n</code></pre>"},{"location":"development/setup/#3-install-dependencies","title":"3. Install Dependencies","text":"<pre><code># Install LLMBuilder in development mode\npip install -e .\n\n# Install development dependencies\npip install -e \".[dev]\"\n\n# Install all optional dependencies (recommended for full testing)\npip install -e \".[all]\"\n</code></pre>"},{"location":"development/setup/#4-set-up-development-tools","title":"4. Set Up Development Tools","text":"<pre><code># Install pre-commit hooks\npre-commit install\n\n# Verify installation\npre-commit run --all-files\n</code></pre>"},{"location":"development/setup/#development-dependencies","title":"Development Dependencies","text":"<p>LLMBuilder uses several dependency groups:</p>"},{"location":"development/setup/#core-dependencies","title":"Core Dependencies","text":"<pre><code>pip install -e \".\"\n</code></pre> <p>Installs the basic LLMBuilder package with core functionality.</p>"},{"location":"development/setup/#development-dependencies_1","title":"Development Dependencies","text":"<pre><code>pip install -e \".[dev]\"\n</code></pre> <p>Includes:</p> <ul> <li>Testing: pytest, pytest-cov, pytest-xdist, pytest-mock</li> <li>Code quality: black, isort, flake8, mypy</li> <li>Pre-commit: pre-commit, bandit, pydocstyle</li> <li>Documentation: sphinx, mkdocs, mkdocs-material</li> </ul>"},{"location":"development/setup/#optional-feature-dependencies","title":"Optional Feature Dependencies","text":"<pre><code># PDF processing with OCR\npip install -e \".[pdf]\"\n\n# EPUB processing\npip install -e \".[epub]\"\n\n# HTML processing\npip install -e \".[html]\"\n\n# Semantic deduplication\npip install -e \".[semantic]\"\n\n# GGUF model conversion\npip install -e \".[conversion]\"\n\n# All optional features\npip install -e \".[all]\"\n</code></pre>"},{"location":"development/setup/#verification","title":"Verification","text":"<p>After setup, verify everything works:</p> <pre><code># Run tests\npython -m pytest tests/ -v\n\n# Check code style\nblack --check llmbuilder/\nisort --check-only llmbuilder/\nflake8 llmbuilder/\n\n# Run type checking\nmypy llmbuilder/\n\n# Test CLI\nllmbuilder --help\nllmbuilder info\n</code></pre>"},{"location":"development/setup/#ide-configuration","title":"IDE Configuration","text":""},{"location":"development/setup/#vs-code","title":"VS Code","text":"<p>Recommended extensions:</p> <ul> <li>Python</li> <li>Pylance</li> <li>Black Formatter</li> <li>isort</li> <li>GitLens</li> <li>Markdown All in One</li> </ul> <p>Settings (<code>.vscode/settings.json</code>):</p> <pre><code>{\n    \"python.defaultInterpreterPath\": \"./venv/bin/python\",\n    \"python.formatting.provider\": \"black\",\n    \"python.linting.enabled\": true,\n    \"python.linting.flake8Enabled\": true,\n    \"python.linting.mypyEnabled\": true,\n    \"python.testing.pytestEnabled\": true,\n    \"python.testing.pytestArgs\": [\"tests/\"],\n    \"editor.formatOnSave\": true,\n    \"editor.codeActionsOnSave\": {\n        \"source.organizeImports\": true\n    }\n}\n</code></pre>"},{"location":"development/setup/#pycharm","title":"PyCharm","text":"<ol> <li>Open the project directory</li> <li>Configure Python interpreter to use the virtual environment</li> <li>Enable pytest as the test runner</li> <li>Install plugins: Black, IdeaVim (optional)</li> <li>Configure code style to use Black formatting</li> </ol>"},{"location":"development/setup/#environment-variables","title":"Environment Variables","text":"<p>Set these environment variables for development:</p> <pre><code># Enable debug logging\nexport LLMBUILDER_LOG_LEVEL=DEBUG\n\n# Enable slow tests (optional)\nexport RUN_SLOW=1\n\n# Enable performance tests (optional)\nexport RUN_PERF=1\n\n# Tesseract path (if not in system PATH)\nexport TESSDATA_PREFIX=/usr/share/tesseract-ocr/4.00/tessdata\n</code></pre>"},{"location":"development/setup/#common-issues","title":"Common Issues","text":""},{"location":"development/setup/#python-version-issues","title":"Python Version Issues","text":"<pre><code># Check Python version\npython --version\n\n# If using pyenv\npyenv install 3.9.16\npyenv local 3.9.16\n</code></pre>"},{"location":"development/setup/#virtual-environment-issues","title":"Virtual Environment Issues","text":"<pre><code># Recreate virtual environment\nrm -rf venv\npython -m venv venv\nsource venv/bin/activate\npip install --upgrade pip\n</code></pre>"},{"location":"development/setup/#dependency-conflicts","title":"Dependency Conflicts","text":"<pre><code># Clear pip cache\npip cache purge\n\n# Reinstall dependencies\npip uninstall llmbuilder\npip install -e \".[dev,all]\"\n</code></pre>"},{"location":"development/setup/#pre-commit-issues","title":"Pre-commit Issues","text":"<pre><code># Reinstall pre-commit hooks\npre-commit uninstall\npre-commit install\n\n# Update pre-commit hooks\npre-commit autoupdate\n</code></pre>"},{"location":"development/setup/#test-failures","title":"Test Failures","text":"<pre><code># Run tests with verbose output\npython -m pytest tests/ -v -s\n\n# Run specific test\npython -m pytest tests/test_specific.py::test_function -v\n\n# Run tests without optional dependencies\npython -m pytest tests/ -m \"not optional_deps\"\n</code></pre>"},{"location":"development/setup/#docker-development-optional","title":"Docker Development (Optional)","text":"<p>For consistent development environments:</p> <pre><code># Dockerfile.dev\nFROM python:3.9-slim\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    git \\\n    tesseract-ocr \\\n    tesseract-ocr-eng \\\n    build-essential \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Set working directory\nWORKDIR /app\n\n# Copy requirements and install Python dependencies\nCOPY pyproject.toml .\nRUN pip install -e \".[dev,all]\"\n\n# Copy source code\nCOPY . .\n\n# Install in development mode\nRUN pip install -e .\n\n# Set up pre-commit\nRUN pre-commit install\n\nCMD [\"bash\"]\n</code></pre> <pre><code># Build and run development container\ndocker build -f Dockerfile.dev -t llmbuilder-dev .\ndocker run -it -v $(pwd):/app llmbuilder-dev\n</code></pre>"},{"location":"development/setup/#performance-profiling","title":"Performance Profiling","text":"<p>For performance development:</p> <pre><code># Install profiling tools\npip install py-spy memory-profiler line-profiler\n\n# Profile CPU usage\npy-spy record -o profile.svg -- python your_script.py\n\n# Profile memory usage\nmprof run your_script.py\nmprof plot\n\n# Line-by-line profiling\nkernprof -l -v your_script.py\n</code></pre>"},{"location":"development/setup/#documentation-development","title":"Documentation Development","text":"<pre><code># Install documentation dependencies\npip install -e \".[docs]\"\n\n# Serve documentation locally\ncd docs/\nmkdocs serve\n\n# Build documentation\nmkdocs build\n\n# Check for broken links\nmkdocs build --strict\n</code></pre>"},{"location":"development/setup/#next-steps","title":"Next Steps","text":"<p>After setting up your development environment:</p> <ol> <li>Read the contributing guide: Contributing Guide</li> <li>Explore the codebase: Start with <code>llmbuilder/__init__.py</code></li> <li>Run the examples: Try <code>examples/basic_training.py</code></li> <li>Check existing issues: Look for \"good first issue\" labels</li> <li>Join discussions: Participate in GitHub Discussions</li> </ol>"},{"location":"development/setup/#getting-help","title":"Getting Help","text":"<p>If you encounter issues during setup:</p> <ol> <li>Check this guide for common solutions</li> <li>Search existing issues on GitHub</li> <li>Ask in GitHub Discussions for setup help</li> <li>Create an issue if you find a bug in the setup process</li> </ol> <p>Happy coding! \ud83d\ude80</p>"},{"location":"development/testing/","title":"Testing Guide","text":"<p>This guide covers testing practices, conventions, and tools used in LLMBuilder development.</p>"},{"location":"development/testing/#testing-philosophy","title":"Testing Philosophy","text":"<p>LLMBuilder follows a comprehensive testing approach:</p> <ul> <li>Unit Tests: Test individual functions and classes in isolation</li> <li>Integration Tests: Test component interactions and workflows</li> <li>Performance Tests: Ensure performance requirements are met</li> <li>CLI Tests: Test command-line interface functionality</li> </ul>"},{"location":"development/testing/#test-structure","title":"Test Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 unit/                    # Unit tests\n\u2502   \u251c\u2500\u2500 test_config.py\n\u2502   \u251c\u2500\u2500 test_data_processing.py\n\u2502   \u2514\u2500\u2500 test_tokenizer.py\n\u251c\u2500\u2500 integration/             # Integration tests\n\u2502   \u251c\u2500\u2500 test_pipeline_workflow.py\n\u2502   \u251c\u2500\u2500 test_data_processing.py\n\u2502   \u2514\u2500\u2500 test_tokenizer_training_integration.py\n\u251c\u2500\u2500 performance/             # Performance tests\n\u2502   \u251c\u2500\u2500 test_processing_speed.py\n\u2502   \u2514\u2500\u2500 test_memory_usage.py\n\u251c\u2500\u2500 cli/                     # CLI tests\n\u2502   \u2514\u2500\u2500 test_cli_commands.py\n\u251c\u2500\u2500 fixtures/                # Test data and fixtures\n\u2502   \u251c\u2500\u2500 sample_data/\n\u2502   \u2514\u2500\u2500 conftest.py\n\u2514\u2500\u2500 conftest.py              # Global test configuration\n</code></pre>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":""},{"location":"development/testing/#basic-test-execution","title":"Basic Test Execution","text":"<pre><code># Run all tests\npython -m pytest tests/ -v\n\n# Run specific test categories\npython -m pytest tests/unit/ -v           # Unit tests only\npython -m pytest tests/integration/ -v    # Integration tests only\npython -m pytest tests/performance/ -v    # Performance tests only\n\n# Run specific test file\npython -m pytest tests/test_config.py -v\n\n# Run specific test function\npython -m pytest tests/test_config.py::test_config_validation -v\n</code></pre>"},{"location":"development/testing/#test-markers","title":"Test Markers","text":"<pre><code># Run tests by marker\npython -m pytest -m \"not slow\" -v         # Skip slow tests\npython -m pytest -m \"integration\" -v      # Integration tests only\npython -m pytest -m \"performance\" -v      # Performance tests only\npython -m pytest -m \"optional_deps\" -v    # Tests requiring optional deps\n\n# Run slow tests (requires environment variable)\nRUN_SLOW=1 python -m pytest -m \"slow\" -v\n\n# Run performance tests (requires environment variable)\nRUN_PERF=1 python -m pytest -m \"performance\" -v\n</code></pre>"},{"location":"development/testing/#coverage-reports","title":"Coverage Reports","text":"<pre><code># Run tests with coverage\npython -m pytest tests/ --cov=llmbuilder --cov-report=html\n\n# View coverage report\nopen htmlcov/index.html  # macOS\nxdg-open htmlcov/index.html  # Linux\nstart htmlcov/index.html  # Windows\n</code></pre>"},{"location":"development/testing/#writing-tests","title":"Writing Tests","text":""},{"location":"development/testing/#unit-test-example","title":"Unit Test Example","text":"<pre><code>import pytest\nimport tempfile\nfrom pathlib import Path\nfrom unittest.mock import Mock, patch, MagicMock\n\nfrom llmbuilder.config import Config, ModelConfig\nfrom llmbuilder.data.processor import DocumentProcessor\n\nclass TestDocumentProcessor:\n    \"\"\"Test cases for DocumentProcessor.\"\"\"\n\n    def setup_method(self):\n        \"\"\"Set up test environment before each test.\"\"\"\n        self.temp_dir = Path(tempfile.mkdtemp())\n        self.config = Config()\n        self.processor = DocumentProcessor(self.config)\n\n    def teardown_method(self):\n        \"\"\"Clean up after each test.\"\"\"\n        import shutil\n        shutil.rmtree(self.temp_dir, ignore_errors=True)\n\n    def test_process_valid_document_success(self):\n        \"\"\"Test processing valid document returns success.\"\"\"\n        # Arrange\n        content = \"This is test content for processing.\"\n        test_file = self.temp_dir / \"test.txt\"\n        test_file.write_text(content)\n\n        # Act\n        result = self.processor.process_file(str(test_file))\n\n        # Assert\n        assert result.success is True\n        assert result.items_processed &gt; 0\n        assert result.error_message is None\n\n    def test_process_nonexistent_file_raises_error(self):\n        \"\"\"Test processing non-existent file raises FileNotFoundError.\"\"\"\n        # Arrange\n        nonexistent_file = str(self.temp_dir / \"nonexistent.txt\")\n\n        # Act &amp; Assert\n        with pytest.raises(FileNotFoundError):\n            self.processor.process_file(nonexistent_file)\n\n    @pytest.mark.parametrize(\"content,expected_lines\", [\n        (\"Single line\", 1),\n        (\"Line 1\\nLine 2\", 2),\n        (\"\", 0),\n        (\"Line 1\\n\\nLine 3\", 2),  # Empty lines ignored\n    ])\n    def test_process_content_line_counting(self, content, expected_lines):\n        \"\"\"Test that content processing counts lines correctly.\"\"\"\n        # Arrange\n        test_file = self.temp_dir / \"test.txt\"\n        test_file.write_text(content)\n\n        # Act\n        result = self.processor.process_file(str(test_file))\n\n        # Assert\n        assert result.items_processed == expected_lines\n\n    @patch('llmbuilder.data.processor.external_service_call')\n    def test_process_with_mocked_external_service(self, mock_service):\n        \"\"\"Test processing with mocked external dependencies.\"\"\"\n        # Arrange\n        mock_service.return_value = {\"status\": \"success\", \"data\": \"processed\"}\n        test_file = self.temp_dir / \"test.txt\"\n        test_file.write_text(\"test content\")\n\n        # Act\n        result = self.processor.process_file(str(test_file))\n\n        # Assert\n        assert result.success is True\n        mock_service.assert_called_once()\n</code></pre>"},{"location":"development/testing/#integration-test-example","title":"Integration Test Example","text":"<pre><code>import pytest\nfrom pathlib import Path\nimport tempfile\nimport shutil\n\nfrom llmbuilder.data.ingest import IngestionPipeline\nfrom llmbuilder.data.dedup import DeduplicationPipeline\nfrom llmbuilder.config.manager import create_config_from_template\n\nclass TestDataProcessingPipeline:\n    \"\"\"Integration tests for complete data processing pipeline.\"\"\"\n\n    def setup_method(self):\n        \"\"\"Set up test environment.\"\"\"\n        self.temp_dir = Path(tempfile.mkdtemp())\n        self.input_dir = self.temp_dir / \"input\"\n        self.output_dir = self.temp_dir / \"output\"\n        self.input_dir.mkdir()\n        self.output_dir.mkdir()\n\n    def teardown_method(self):\n        \"\"\"Clean up test environment.\"\"\"\n        shutil.rmtree(self.temp_dir, ignore_errors=True)\n\n    def test_complete_processing_pipeline(self):\n        \"\"\"Test complete pipeline from ingestion to deduplication.\"\"\"\n        # Arrange - Create test documents\n        self._create_test_documents()\n\n        config = create_config_from_template(\"basic_config\", {\n            \"data\": {\n                \"ingestion\": {\"batch_size\": 10},\n                \"deduplication\": {\"similarity_threshold\": 0.8}\n            }\n        })\n\n        # Act - Run ingestion pipeline\n        ingestion = IngestionPipeline(config.data.ingestion)\n        ingestion_result = ingestion.process_directory(\n            str(self.input_dir),\n            str(self.output_dir / \"ingested.txt\")\n        )\n\n        # Act - Run deduplication pipeline\n        dedup = DeduplicationPipeline(config.data.deduplication)\n        dedup_result = dedup.process_file(\n            str(self.output_dir / \"ingested.txt\"),\n            str(self.output_dir / \"deduplicated.txt\")\n        )\n\n        # Assert\n        assert ingestion_result[\"success\"] is True\n        assert ingestion_result[\"files_processed\"] &gt; 0\n\n        assert dedup_result[\"success\"] is True\n        assert dedup_result[\"final_count\"] &lt;= dedup_result[\"original_count\"]\n\n        # Verify output files exist\n        assert (self.output_dir / \"ingested.txt\").exists()\n        assert (self.output_dir / \"deduplicated.txt\").exists()\n\n    def _create_test_documents(self):\n        \"\"\"Create test documents for pipeline testing.\"\"\"\n        documents = [\n            (\"doc1.txt\", \"This is the first test document.\"),\n            (\"doc2.html\", \"&lt;html&gt;&lt;body&gt;&lt;p&gt;HTML document content&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;\"),\n            (\"doc3.txt\", \"This is the first test document.\"),  # Duplicate\n            (\"doc4.txt\", \"Another unique document with different content.\"),\n        ]\n\n        for filename, content in documents:\n            (self.input_dir / filename).write_text(content)\n\n@pytest.mark.slow\n@pytest.mark.integration\nclass TestTokenizerTrainingIntegration:\n    \"\"\"Integration tests for tokenizer training workflow.\"\"\"\n\n    @pytest.mark.optional_deps\n    def test_sentencepiece_training_workflow(self):\n        \"\"\"Test complete SentencePiece tokenizer training workflow.\"\"\"\n        # This test requires sentence-transformers\n        pytest.importorskip(\"sentencepiece\")\n\n        # Test implementation here\n        pass\n</code></pre>"},{"location":"development/testing/#performance-test-example","title":"Performance Test Example","text":"<pre><code>import pytest\nimport time\nimport psutil\nimport os\nfrom pathlib import Path\n\n@pytest.mark.performance\nclass TestProcessingPerformance:\n    \"\"\"Performance tests for data processing components.\"\"\"\n\n    def test_large_file_processing_performance(self):\n        \"\"\"Test that large file processing meets performance requirements.\"\"\"\n        # Skip if performance tests not enabled\n        if not os.getenv(\"RUN_PERF\"):\n            pytest.skip(\"Performance tests not enabled (set RUN_PERF=1)\")\n\n        # Arrange\n        large_content = \"Test line content.\\n\" * 10000  # 10k lines\n        test_file = Path(\"large_test_file.txt\")\n        test_file.write_text(large_content)\n\n        try:\n            # Act\n            start_time = time.time()\n            start_memory = psutil.Process().memory_info().rss\n\n            # Process the large file\n            from llmbuilder.data.processor import DocumentProcessor\n            processor = DocumentProcessor()\n            result = processor.process_file(str(test_file))\n\n            end_time = time.time()\n            end_memory = psutil.Process().memory_info().rss\n\n            # Assert performance requirements\n            processing_time = end_time - start_time\n            memory_used = end_memory - start_memory\n\n            assert processing_time &lt; 5.0, f\"Processing took {processing_time:.2f}s, expected &lt; 5s\"\n            assert memory_used &lt; 100 * 1024 * 1024, f\"Used {memory_used / 1024 / 1024:.1f}MB, expected &lt; 100MB\"\n            assert result.success is True\n\n        finally:\n            test_file.unlink(missing_ok=True)\n\n    @pytest.mark.parametrize(\"file_size\", [1000, 5000, 10000])\n    def test_processing_scales_linearly(self, file_size):\n        \"\"\"Test that processing time scales roughly linearly with file size.\"\"\"\n        if not os.getenv(\"RUN_PERF\"):\n            pytest.skip(\"Performance tests not enabled\")\n\n        # Implementation here\n        pass\n</code></pre>"},{"location":"development/testing/#cli-test-example","title":"CLI Test Example","text":"<pre><code>import pytest\nfrom click.testing import CliRunner\nfrom pathlib import Path\nimport tempfile\n\nfrom llmbuilder.cli import main\n\nclass TestCLICommands:\n    \"\"\"Test cases for CLI commands.\"\"\"\n\n    def setup_method(self):\n        \"\"\"Set up test environment.\"\"\"\n        self.runner = CliRunner()\n        self.temp_dir = Path(tempfile.mkdtemp())\n\n    def teardown_method(self):\n        \"\"\"Clean up test environment.\"\"\"\n        import shutil\n        shutil.rmtree(self.temp_dir, ignore_errors=True)\n\n    def test_cli_help_command(self):\n        \"\"\"Test that help command works.\"\"\"\n        result = self.runner.invoke(main, ['--help'])\n\n        assert result.exit_code == 0\n        assert \"LLMBuilder\" in result.output\n        assert \"Usage:\" in result.output\n\n    def test_cli_info_command(self):\n        \"\"\"Test info command displays package information.\"\"\"\n        result = self.runner.invoke(main, ['info'])\n\n        assert result.exit_code == 0\n        assert \"LLMBuilder\" in result.output\n        assert \"version\" in result.output.lower()\n\n    def test_config_templates_command(self):\n        \"\"\"Test config templates command lists available templates.\"\"\"\n        result = self.runner.invoke(main, ['config', 'templates'])\n\n        assert result.exit_code == 0\n        assert \"basic_config\" in result.output\n        assert \"cpu_optimized_config\" in result.output\n\n    def test_config_validate_command_with_valid_config(self):\n        \"\"\"Test config validation with valid configuration.\"\"\"\n        # Create valid config file\n        config_content = '{\"model\": {\"vocab_size\": 16000}}'\n        config_file = self.temp_dir / \"valid_config.json\"\n        config_file.write_text(config_content)\n\n        result = self.runner.invoke(main, ['config', 'validate', str(config_file)])\n\n        assert result.exit_code == 0\n        assert \"valid\" in result.output.lower()\n\n    def test_config_validate_command_with_invalid_config(self):\n        \"\"\"Test config validation with invalid configuration.\"\"\"\n        # Create invalid config file\n        config_content = '{\"invalid\": \"json\"'  # Missing closing brace\n        config_file = self.temp_dir / \"invalid_config.json\"\n        config_file.write_text(config_content)\n\n        result = self.runner.invoke(main, ['config', 'validate', str(config_file)])\n\n        assert result.exit_code != 0\n        assert \"failed\" in result.output.lower() or \"error\" in result.output.lower()\n</code></pre>"},{"location":"development/testing/#test-fixtures-and-utilities","title":"Test Fixtures and Utilities","text":""},{"location":"development/testing/#global-fixtures-conftestpy","title":"Global Fixtures (conftest.py)","text":"<pre><code># tests/conftest.py\nimport pytest\nimport tempfile\nfrom pathlib import Path\nimport shutil\n\n@pytest.fixture(scope=\"session\")\ndef sample_data_dir():\n    \"\"\"Provide directory with sample test data.\"\"\"\n    return Path(__file__).parent / \"fixtures\" / \"sample_data\"\n\n@pytest.fixture\ndef temp_directory():\n    \"\"\"Provide temporary directory for tests.\"\"\"\n    temp_dir = Path(tempfile.mkdtemp())\n    yield temp_dir\n    shutil.rmtree(temp_dir, ignore_errors=True)\n\n@pytest.fixture\ndef sample_config():\n    \"\"\"Provide sample configuration for tests.\"\"\"\n    from llmbuilder.config import Config, ModelConfig\n    return Config(\n        model=ModelConfig(\n            vocab_size=8000,\n            num_layers=4,\n            num_heads=4,\n            embedding_dim=256\n        )\n    )\n\n@pytest.fixture\ndef mock_file_system(temp_directory):\n    \"\"\"Create mock file system structure for testing.\"\"\"\n    # Create directory structure\n    (temp_directory / \"input\").mkdir()\n    (temp_directory / \"output\").mkdir()\n\n    # Create sample files\n    (temp_directory / \"input\" / \"test1.txt\").write_text(\"Sample content 1\")\n    (temp_directory / \"input\" / \"test2.txt\").write_text(\"Sample content 2\")\n\n    return temp_directory\n\ndef pytest_configure(config):\n    \"\"\"Configure pytest with custom markers.\"\"\"\n    config.addinivalue_line(\n        \"markers\", \"slow: marks tests as slow (deselect with '-m \\\"not slow\\\"')\"\n    )\n    config.addinivalue_line(\n        \"markers\", \"integration: marks tests as integration tests\"\n    )\n    config.addinivalue_line(\n        \"markers\", \"performance: marks tests as performance tests\"\n    )\n    config.addinivalue_line(\n        \"markers\", \"optional_deps: marks tests that require optional dependencies\"\n    )\n\ndef pytest_collection_modifyitems(config, items):\n    \"\"\"Modify test collection to handle markers and environment variables.\"\"\"\n    import os\n\n    # Skip slow tests unless RUN_SLOW is set\n    if not os.getenv(\"RUN_SLOW\"):\n        skip_slow = pytest.mark.skip(reason=\"Slow tests disabled (set RUN_SLOW=1 to enable)\")\n        for item in items:\n            if \"slow\" in item.keywords:\n                item.add_marker(skip_slow)\n\n    # Skip performance tests unless RUN_PERF is set\n    if not os.getenv(\"RUN_PERF\"):\n        skip_perf = pytest.mark.skip(reason=\"Performance tests disabled (set RUN_PERF=1 to enable)\")\n        for item in items:\n            if \"performance\" in item.keywords:\n                item.add_marker(skip_perf)\n</code></pre>"},{"location":"development/testing/#test-utilities","title":"Test Utilities","text":"<pre><code># tests/utils.py\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nimport json\n\ndef create_sample_documents(directory: Path, documents: List[Dict[str, str]]) -&gt; List[Path]:\n    \"\"\"Create sample documents for testing.\"\"\"\n    created_files = []\n\n    for doc in documents:\n        file_path = directory / doc[\"filename\"]\n        file_path.write_text(doc[\"content\"])\n        created_files.append(file_path)\n\n    return created_files\n\ndef create_test_config(overrides: Dict[str, Any] = None) -&gt; Dict[str, Any]:\n    \"\"\"Create test configuration with optional overrides.\"\"\"\n    base_config = {\n        \"model\": {\n            \"vocab_size\": 8000,\n            \"num_layers\": 4,\n            \"num_heads\": 4,\n            \"embedding_dim\": 256\n        },\n        \"training\": {\n            \"batch_size\": 8,\n            \"num_epochs\": 2,\n            \"learning_rate\": 1e-3\n        }\n    }\n\n    if overrides:\n        # Deep merge overrides\n        def deep_merge(base, override):\n            for key, value in override.items():\n                if key in base and isinstance(base[key], dict) and isinstance(value, dict):\n                    deep_merge(base[key], value)\n                else:\n                    base[key] = value\n\n        deep_merge(base_config, overrides)\n\n    return base_config\n\ndef assert_file_contains(file_path: Path, expected_content: str):\n    \"\"\"Assert that file contains expected content.\"\"\"\n    assert file_path.exists(), f\"File does not exist: {file_path}\"\n    content = file_path.read_text()\n    assert expected_content in content, f\"Expected content not found in {file_path}\"\n\ndef assert_processing_result_valid(result):\n    \"\"\"Assert that processing result has valid structure.\"\"\"\n    assert hasattr(result, 'success')\n    assert hasattr(result, 'items_processed')\n    assert hasattr(result, 'processing_time')\n    assert isinstance(result.success, bool)\n    assert isinstance(result.items_processed, int)\n    assert isinstance(result.processing_time, (int, float))\n    assert result.items_processed &gt;= 0\n    assert result.processing_time &gt;= 0\n</code></pre>"},{"location":"development/testing/#test-configuration","title":"Test Configuration","text":""},{"location":"development/testing/#pytestini","title":"pytest.ini","text":"<pre><code>[tool:pytest]\nminversion = 7.0\naddopts =\n    -ra\n    --strict-markers\n    --strict-config\n    --cov=llmbuilder\n    --cov-report=term-missing\n    --cov-report=html\n    --cov-report=xml\ntestpaths = tests\nmarkers =\n    slow: marks tests as slow (deselect with '-m \"not slow\"')\n    integration: marks tests as integration tests\n    performance: marks tests as performance tests\n    gpu: marks tests that require GPU\n    optional_deps: marks tests that require optional dependencies\nfilterwarnings =\n    error\n    ignore::UserWarning\n    ignore::DeprecationWarning\n</code></pre>"},{"location":"development/testing/#continuous-integration","title":"Continuous Integration","text":""},{"location":"development/testing/#github-actions-test-workflow","title":"GitHub Actions Test Workflow","text":"<pre><code># .github/workflows/test.yml\nname: Tests\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  test:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest, windows-latest, macos-latest]\n        python-version: [3.8, 3.9, \"3.10\", \"3.11\", \"3.12\"]\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Set up Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v4\n      with:\n        python-version: ${{ matrix.python-version }}\n\n    - name: Install system dependencies (Ubuntu)\n      if: matrix.os == 'ubuntu-latest'\n      run: |\n        sudo apt-get update\n        sudo apt-get install -y tesseract-ocr tesseract-ocr-eng\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -e \".[dev,test]\"\n\n    - name: Run tests\n      run: |\n        python -m pytest tests/ -v --cov=llmbuilder --cov-report=xml\n\n    - name: Upload coverage to Codecov\n      uses: codecov/codecov-action@v3\n      with:\n        file: ./coverage.xml\n        flags: unittests\n        name: codecov-umbrella\n</code></pre>"},{"location":"development/testing/#best-practices","title":"Best Practices","text":""},{"location":"development/testing/#test-organization","title":"Test Organization","text":"<ol> <li>One test class per class being tested</li> <li>Group related tests together</li> <li>Use descriptive test names</li> <li>Follow AAA pattern (Arrange, Act, Assert)</li> <li>Keep tests independent and isolated</li> </ol>"},{"location":"development/testing/#test-data-management","title":"Test Data Management","text":"<ol> <li>Use fixtures for common test data</li> <li>Create minimal test data</li> <li>Clean up after tests</li> <li>Use temporary directories for file operations</li> <li>Mock external dependencies</li> </ol>"},{"location":"development/testing/#performance-testing","title":"Performance Testing","text":"<ol> <li>Set clear performance requirements</li> <li>Test with realistic data sizes</li> <li>Monitor memory usage</li> <li>Use environment variables to control execution</li> <li>Document performance expectations</li> </ol>"},{"location":"development/testing/#mocking-guidelines","title":"Mocking Guidelines","text":"<ol> <li>Mock external services and APIs</li> <li>Mock file system operations when appropriate</li> <li>Don't mock the code you're testing</li> <li>Use specific assertions on mocks</li> <li>Reset mocks between tests</li> </ol>"},{"location":"development/testing/#debugging-tests","title":"Debugging Tests","text":""},{"location":"development/testing/#running-tests-in-debug-mode","title":"Running Tests in Debug Mode","text":"<pre><code># Run with verbose output\npython -m pytest tests/ -v -s\n\n# Run specific test with debugging\npython -m pytest tests/test_specific.py::test_function -v -s --pdb\n\n# Run with coverage and keep temporary files\npython -m pytest tests/ --cov=llmbuilder --keep-temp-files\n</code></pre>"},{"location":"development/testing/#common-issues","title":"Common Issues","text":"<ol> <li>Test isolation: Ensure tests don't depend on each other</li> <li>Resource cleanup: Always clean up temporary files and resources</li> <li>Mock configuration: Ensure mocks are properly configured and reset</li> <li>Environment dependencies: Handle optional dependencies gracefully</li> <li>Timing issues: Use appropriate timeouts and retries for async operations</li> </ol> <p>Following these testing practices ensures reliable, maintainable tests that provide confidence in code changes and help catch regressions early.</p>"},{"location":"examples/advanced-deduplication/","title":"Advanced Deduplication","text":"<p>This example demonstrates LLMBuilder's advanced deduplication capabilities, including both exact and semantic duplicate detection with configurable similarity thresholds.</p>"},{"location":"examples/advanced-deduplication/#overview","title":"Overview","text":"<p>LLMBuilder provides two types of deduplication:</p> <ul> <li>Exact Deduplication: Removes identical text using normalized hashing</li> <li>Semantic Deduplication: Removes similar content using sentence embeddings</li> <li>Combined Approach: Uses both methods for comprehensive cleaning</li> </ul>"},{"location":"examples/advanced-deduplication/#complete-example","title":"Complete Example","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nAdvanced Deduplication Example\n\nDemonstrates exact and semantic duplicate detection using LLMBuilder's\ndeduplication pipeline with configurable similarity thresholds.\n\"\"\"\n\nimport logging\nfrom pathlib import Path\nfrom llmbuilder.data.dedup import DeduplicationPipeline, ExactDuplicateDetector, SemanticDuplicateDetector\nfrom llmbuilder.config.manager import create_config_from_template\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef main():\n    \"\"\"Demonstrate advanced deduplication capabilities.\"\"\"\n\n    # Create sample data with duplicates\n    sample_file = create_sample_data_with_duplicates()\n\n    # Demonstrate different deduplication approaches\n    demonstrate_exact_deduplication(sample_file)\n    demonstrate_semantic_deduplication(sample_file)\n    demonstrate_combined_deduplication(sample_file)\n    demonstrate_batch_deduplication()\n\ndef create_sample_data_with_duplicates():\n    \"\"\"Create sample data containing various types of duplicates.\"\"\"\n\n    sample_texts = [\n        # Original content\n        \"Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed.\",\n\n        # Exact duplicate\n        \"Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed.\",\n\n        # Near-exact duplicate (minor differences)\n        \"Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed. \",\n\n        # Semantic duplicate (same meaning, different words)\n        \"ML is a branch of AI that allows computers to learn automatically without explicit programming.\",\n\n        # Similar but not duplicate\n        \"Deep learning is a subset of machine learning that uses neural networks with multiple layers.\",\n\n        # Another original\n        \"Natural language processing helps computers understand and interpret human language.\",\n\n        # Exact duplicate of above\n        \"Natural language processing helps computers understand and interpret human language.\",\n\n        # Semantic duplicate\n        \"NLP enables machines to comprehend and analyze human speech and text.\",\n\n        # Different content\n        \"Computer vision allows machines to interpret and understand visual information from the world.\",\n\n        # Whitespace variations (should be caught by exact deduplication)\n        \"  Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed.  \",\n\n        # Case variations\n        \"MACHINE LEARNING IS A SUBSET OF ARTIFICIAL INTELLIGENCE THAT ENABLES COMPUTERS TO LEARN WITHOUT BEING EXPLICITLY PROGRAMMED.\",\n\n        # Punctuation variations\n        \"Machine learning is a subset of artificial intelligence that enables computers to learn without being explicitly programmed!\",\n\n        # More diverse content\n        \"Reinforcement learning is a type of machine learning where agents learn through interaction with an environment.\",\n        \"Supervised learning uses labeled data to train models that can make predictions on new, unseen data.\",\n        \"Unsupervised learning finds patterns in data without using labeled examples or target outputs.\",\n\n        # Some more semantic duplicates\n        \"Reinforcement learning involves agents that learn by interacting with their environment and receiving feedback.\",\n        \"In supervised learning, models are trained using labeled datasets to make predictions on new data.\",\n    ]\n\n    # Create sample file\n    sample_dir = Path(\"./dedup_samples\")\n    sample_dir.mkdir(exist_ok=True)\n\n    sample_file = sample_dir / \"sample_with_duplicates.txt\"\n    sample_file.write_text('\\n\\n'.join(sample_texts), encoding='utf-8')\n\n    logger.info(f\"Created sample data with {len(sample_texts)} texts\")\n    logger.info(f\"Sample file: {sample_file}\")\n\n    return sample_file\n\ndef demonstrate_exact_deduplication(sample_file):\n    \"\"\"Demonstrate exact duplicate detection and removal.\"\"\"\n\n    logger.info(\"\\n\" + \"=\"*50)\n    logger.info(\"EXACT DEDUPLICATION DEMO\")\n    logger.info(\"=\"*50)\n\n    # Configure exact deduplication only\n    config = create_config_from_template(\"basic_config\", {\n        \"data\": {\n            \"deduplication\": {\n                \"enable_exact_deduplication\": True,\n                \"enable_semantic_deduplication\": False,\n                \"normalize_text\": True,\n                \"min_text_length\": 10,\n                \"batch_size\": 100\n            }\n        }\n    })\n\n    # Create exact deduplication detector\n    detector = ExactDuplicateDetector(config.data.deduplication)\n\n    # Read sample data\n    text = sample_file.read_text(encoding='utf-8')\n    texts = [t.strip() for t in text.split('\\n\\n') if t.strip()]\n\n    logger.info(f\"Original texts: {len(texts)}\")\n\n    # Detect duplicates\n    unique_texts, duplicate_info = detector.deduplicate(texts)\n\n    logger.info(f\"After exact deduplication: {len(unique_texts)}\")\n    logger.info(f\"Removed {len(texts) - len(unique_texts)} exact duplicates\")\n\n    # Show what was removed\n    logger.info(\"\\nExact duplicates removed:\")\n    for i, (original_idx, duplicate_indices) in enumerate(duplicate_info.items()):\n        if duplicate_indices:\n            logger.info(f\"  Group {i+1}: Original at index {original_idx}\")\n            logger.info(f\"    Text: {texts[original_idx][:100]}...\")\n            logger.info(f\"    Duplicates at indices: {duplicate_indices}\")\n\n    # Save results\n    output_file = sample_file.parent / \"exact_deduplicated.txt\"\n    output_file.write_text('\\n\\n'.join(unique_texts), encoding='utf-8')\n    logger.info(f\"Results saved to: {output_file}\")\n\ndef demonstrate_semantic_deduplication(sample_file):\n    \"\"\"Demonstrate semantic duplicate detection and removal.\"\"\"\n\n    logger.info(\"\\n\" + \"=\"*50)\n    logger.info(\"SEMANTIC DEDUPLICATION DEMO\")\n    logger.info(\"=\"*50)\n\n    try:\n        # Configure semantic deduplication\n        config = create_config_from_template(\"basic_config\", {\n            \"data\": {\n                \"deduplication\": {\n                    \"enable_exact_deduplication\": False,\n                    \"enable_semantic_deduplication\": True,\n                    \"similarity_threshold\": 0.8,  # 80% similarity threshold\n                    \"embedding_model\": \"all-MiniLM-L6-v2\",\n                    \"batch_size\": 32,\n                    \"use_gpu_for_embeddings\": False,  # Use CPU for demo\n                    \"similarity_metric\": \"cosine\"\n                }\n            }\n        })\n\n        # Create semantic deduplication detector\n        detector = SemanticDuplicateDetector(config.data.deduplication)\n\n        # Read sample data\n        text = sample_file.read_text(encoding='utf-8')\n        texts = [t.strip() for t in text.split('\\n\\n') if t.strip()]\n\n        logger.info(f\"Original texts: {len(texts)}\")\n        logger.info(f\"Similarity threshold: {config.data.deduplication.similarity_threshold}\")\n\n        # Detect semantic duplicates\n        unique_texts, similarity_info = detector.deduplicate(texts)\n\n        logger.info(f\"After semantic deduplication: {len(unique_texts)}\")\n        logger.info(f\"Removed {len(texts) - len(unique_texts)} semantic duplicates\")\n\n        # Show similarity information\n        logger.info(\"\\nSemantic duplicates found:\")\n        for i, (text1_idx, text2_idx, similarity) in enumerate(similarity_info[:5]):  # Show first 5\n            logger.info(f\"  Pair {i+1}: Similarity {similarity:.3f}\")\n            logger.info(f\"    Text 1: {texts[text1_idx][:80]}...\")\n            logger.info(f\"    Text 2: {texts[text2_idx][:80]}...\")\n\n        # Save results\n        output_file = sample_file.parent / \"semantic_deduplicated.txt\"\n        output_file.write_text('\\n\\n'.join(unique_texts), encoding='utf-8')\n        logger.info(f\"Results saved to: {output_file}\")\n\n    except ImportError as e:\n        logger.warning(f\"Semantic deduplication not available: {e}\")\n        logger.info(\"Install with: pip install sentence-transformers\")\n\ndef demonstrate_combined_deduplication(sample_file):\n    \"\"\"Demonstrate combined exact and semantic deduplication.\"\"\"\n\n    logger.info(\"\\n\" + \"=\"*50)\n    logger.info(\"COMBINED DEDUPLICATION DEMO\")\n    logger.info(\"=\"*50)\n\n    try:\n        # Configure combined deduplication\n        config = create_config_from_template(\"advanced_processing_config\", {\n            \"data\": {\n                \"deduplication\": {\n                    \"enable_exact_deduplication\": True,\n                    \"enable_semantic_deduplication\": True,\n                    \"similarity_threshold\": 0.85,\n                    \"embedding_model\": \"all-MiniLM-L6-v2\",\n                    \"batch_size\": 1000,\n                    \"chunk_size\": 512,\n                    \"min_text_length\": 20,\n                    \"normalize_text\": True,\n                    \"use_gpu_for_embeddings\": False,\n                    \"embedding_cache_size\": 5000,\n                    \"similarity_metric\": \"cosine\"\n                }\n            }\n        })\n\n        # Create deduplication pipeline\n        pipeline = DeduplicationPipeline(config.data.deduplication)\n\n        # Process file\n        output_file = sample_file.parent / \"combined_deduplicated.txt\"\n\n        stats = pipeline.process_file(str(sample_file), str(output_file))\n\n        logger.info(\"Combined deduplication completed!\")\n        logger.info(f\"Original texts: {stats['original_count']}\")\n        logger.info(f\"After exact deduplication: {stats['after_exact_dedup']}\")\n        logger.info(f\"After semantic deduplication: {stats['final_count']}\")\n        logger.info(f\"Total removed: {stats['total_removed']}\")\n        logger.info(f\"Exact duplicates removed: {stats['exact_duplicates_removed']}\")\n        logger.info(f\"Semantic duplicates removed: {stats['semantic_duplicates_removed']}\")\n        logger.info(f\"Processing time: {stats['processing_time']:.2f}s\")\n\n    except ImportError as e:\n        logger.warning(f\"Combined deduplication not available: {e}\")\n        logger.info(\"Install with: pip install sentence-transformers\")\n\ndef demonstrate_batch_deduplication():\n    \"\"\"Demonstrate batch processing of multiple files.\"\"\"\n\n    logger.info(\"\\n\" + \"=\"*50)\n    logger.info(\"BATCH DEDUPLICATION DEMO\")\n    logger.info(\"=\"*50)\n\n    # Create multiple sample files\n    batch_dir = Path(\"./batch_dedup\")\n    batch_dir.mkdir(exist_ok=True)\n\n    # Create sample files with different content\n    sample_files = []\n\n    for i in range(3):\n        content = [\n            f\"This is sample file {i+1} with unique content about machine learning.\",\n            f\"File {i+1} contains information about artificial intelligence and data science.\",\n            \"Machine learning is a subset of artificial intelligence.\",  # Common duplicate\n            f\"Deep learning is particularly useful for file {i+1} applications.\",\n            \"Natural language processing helps computers understand human language.\",  # Another common duplicate\n        ]\n\n        file_path = batch_dir / f\"sample_{i+1}.txt\"\n        file_path.write_text('\\n\\n'.join(content), encoding='utf-8')\n        sample_files.append(file_path)\n\n    logger.info(f\"Created {len(sample_files)} sample files for batch processing\")\n\n    try:\n        # Configure for batch processing\n        config = create_config_from_template(\"basic_config\", {\n            \"data\": {\n                \"deduplication\": {\n                    \"enable_exact_deduplication\": True,\n                    \"enable_semantic_deduplication\": True,\n                    \"similarity_threshold\": 0.8,\n                    \"batch_size\": 100,\n                    \"use_gpu_for_embeddings\": False\n                }\n            }\n        })\n\n        pipeline = DeduplicationPipeline(config.data.deduplication)\n\n        # Process each file\n        total_stats = {\n            'files_processed': 0,\n            'total_original': 0,\n            'total_final': 0,\n            'total_removed': 0\n        }\n\n        for file_path in sample_files:\n            output_path = batch_dir / f\"dedup_{file_path.name}\"\n\n            logger.info(f\"Processing {file_path.name}...\")\n            stats = pipeline.process_file(str(file_path), str(output_path))\n\n            total_stats['files_processed'] += 1\n            total_stats['total_original'] += stats['original_count']\n            total_stats['total_final'] += stats['final_count']\n            total_stats['total_removed'] += stats['total_removed']\n\n            logger.info(f\"  Original: {stats['original_count']}, Final: {stats['final_count']}, Removed: {stats['total_removed']}\")\n\n        logger.info(\"\\nBatch processing summary:\")\n        logger.info(f\"Files processed: {total_stats['files_processed']}\")\n        logger.info(f\"Total original texts: {total_stats['total_original']}\")\n        logger.info(f\"Total final texts: {total_stats['total_final']}\")\n        logger.info(f\"Total removed: {total_stats['total_removed']}\")\n        logger.info(f\"Overall reduction: {(total_stats['total_removed'] / total_stats['total_original'] * 100):.1f}%\")\n\n    except ImportError as e:\n        logger.warning(f\"Batch deduplication not available: {e}\")\n\nif __name__ == \"__main__\":\n    print(\"\ud83e\uddf9 Advanced Deduplication Example\")\n    print(\"=\" * 50)\n    print()\n    print(\"This example demonstrates:\")\n    print(\"\u2022 Exact duplicate detection and removal\")\n    print(\"\u2022 Semantic duplicate detection using embeddings\")\n    print(\"\u2022 Combined deduplication approaches\")\n    print(\"\u2022 Batch processing of multiple files\")\n    print(\"\u2022 Configurable similarity thresholds\")\n    print()\n\n    try:\n        main()\n        print(\"\\n\u2705 Example completed successfully!\")\n\n    except ImportError as e:\n        print(f\"\\n\u274c Missing dependencies: {e}\")\n        print(\"Install with: pip install llmbuilder[semantic]\")\n\n    except Exception as e:\n        print(f\"\\n\u274c Example failed: {e}\")\n        print(\"\\n\ud83d\udca1 Troubleshooting:\")\n        print(\"1. Install semantic dependencies: pip install sentence-transformers\")\n        print(\"2. Reduce batch size if memory issues occur\")\n        print(\"3. Disable GPU if CUDA issues: use_gpu_for_embeddings=False\")\n</code></pre>"},{"location":"examples/advanced-deduplication/#configuration-options","title":"Configuration Options","text":""},{"location":"examples/advanced-deduplication/#exact-deduplication-only","title":"Exact Deduplication Only","text":"<pre><code>config = {\n    \"data\": {\n        \"deduplication\": {\n            \"enable_exact_deduplication\": True,\n            \"enable_semantic_deduplication\": False,\n            \"normalize_text\": True,\n            \"min_text_length\": 50\n        }\n    }\n}\n</code></pre>"},{"location":"examples/advanced-deduplication/#semantic-deduplication-only","title":"Semantic Deduplication Only","text":"<pre><code>config = {\n    \"data\": {\n        \"deduplication\": {\n            \"enable_exact_deduplication\": False,\n            \"enable_semantic_deduplication\": True,\n            \"similarity_threshold\": 0.85,\n            \"embedding_model\": \"all-MiniLM-L6-v2\",\n            \"use_gpu_for_embeddings\": True,\n            \"batch_size\": 1000\n        }\n    }\n}\n</code></pre>"},{"location":"examples/advanced-deduplication/#combined-approach-recommended","title":"Combined Approach (Recommended)","text":"<pre><code>config = {\n    \"data\": {\n        \"deduplication\": {\n            \"enable_exact_deduplication\": True,\n            \"enable_semantic_deduplication\": True,\n            \"similarity_threshold\": 0.9,\n            \"embedding_model\": \"all-MiniLM-L6-v2\",\n            \"batch_size\": 2000,\n            \"chunk_size\": 1024,\n            \"min_text_length\": 100,\n            \"normalize_text\": True,\n            \"use_gpu_for_embeddings\": True,\n            \"embedding_cache_size\": 20000,\n            \"similarity_metric\": \"cosine\"\n        }\n    }\n}\n</code></pre>"},{"location":"examples/advanced-deduplication/#cli-usage","title":"CLI Usage","text":"<pre><code># Exact deduplication only\nllmbuilder data deduplicate -i dataset.txt -o clean.txt --method exact\n\n# Semantic deduplication with custom threshold\nllmbuilder data deduplicate -i dataset.txt -o clean.txt --method semantic --threshold 0.9\n\n# Combined approach (recommended)\nllmbuilder data deduplicate -i dataset.txt -o clean.txt --method both --threshold 0.85\n\n# GPU-accelerated semantic deduplication\nllmbuilder data deduplicate -i dataset.txt -o clean.txt --method semantic --use-gpu --batch-size 2000\n\n# Batch processing with custom settings\nllmbuilder data deduplicate -i dataset.txt -o clean.txt \\\n  --method both \\\n  --threshold 0.9 \\\n  --batch-size 1000 \\\n  --min-length 50 \\\n  --use-gpu\n</code></pre>"},{"location":"examples/advanced-deduplication/#similarity-thresholds","title":"Similarity Thresholds","text":"<p>Choose the right threshold for your use case:</p> Threshold Use Case Description 0.95-1.0 Conservative Only removes very similar content 0.85-0.95 Balanced Good balance between quality and quantity 0.7-0.85 Aggressive Removes more content, may lose some unique texts 0.5-0.7 Very Aggressive High risk of removing unique content"},{"location":"examples/advanced-deduplication/#performance-optimization","title":"Performance Optimization","text":""},{"location":"examples/advanced-deduplication/#memory-optimization","title":"Memory Optimization","text":"<pre><code>config = {\n    \"data\": {\n        \"deduplication\": {\n            \"batch_size\": 500,              # Smaller batches\n            \"embedding_cache_size\": 5000,   # Smaller cache\n            \"use_gpu_for_embeddings\": False, # Use CPU\n            \"chunk_size\": 256               # Smaller chunks\n        }\n    }\n}\n</code></pre>"},{"location":"examples/advanced-deduplication/#speed-optimization","title":"Speed Optimization","text":"<pre><code>config = {\n    \"data\": {\n        \"deduplication\": {\n            \"batch_size\": 5000,             # Larger batches\n            \"embedding_cache_size\": 50000,  # Larger cache\n            \"use_gpu_for_embeddings\": True, # Use GPU\n            \"similarity_metric\": \"cosine\"   # Fastest metric\n        }\n    }\n}\n</code></pre>"},{"location":"examples/advanced-deduplication/#embedding-models","title":"Embedding Models","text":"<p>Different models for different needs:</p> Model Size Speed Quality Use Case all-MiniLM-L6-v2 Small Fast Good General purpose all-mpnet-base-v2 Medium Medium Better Higher quality all-MiniLM-L12-v2 Medium Medium Better Balanced paraphrase-MiniLM-L6-v2 Small Fast Good Paraphrase detection"},{"location":"examples/advanced-deduplication/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/advanced-deduplication/#common-issues","title":"Common Issues","text":"<p>Out of Memory <pre><code># Reduce batch size and cache\nconfig = {\n    \"data\": {\n        \"deduplication\": {\n            \"batch_size\": 100,\n            \"embedding_cache_size\": 1000,\n            \"use_gpu_for_embeddings\": False\n        }\n    }\n}\n</code></pre></p> <p>Slow Processing <pre><code># Enable GPU acceleration\nllmbuilder data deduplicate -i dataset.txt -o clean.txt --use-gpu --batch-size 2000\n\n# Use faster embedding model\nllmbuilder config from-template basic_config -o config.json \\\n  --override data.deduplication.embedding_model=all-MiniLM-L6-v2\n</code></pre></p> <p>Too Many Duplicates Removed <pre><code># Increase similarity threshold\nconfig = {\n    \"data\": {\n        \"deduplication\": {\n            \"similarity_threshold\": 0.95,  # More conservative\n            \"min_text_length\": 100         # Longer minimum length\n        }\n    }\n}\n</code></pre></p> <p>CUDA Issues <pre><code># Disable GPU acceleration\nllmbuilder data deduplicate -i dataset.txt -o clean.txt --no-gpu\n\n# Or in configuration\nconfig[\"data\"][\"deduplication\"][\"use_gpu_for_embeddings\"] = False\n</code></pre></p>"},{"location":"examples/advanced-deduplication/#next-steps","title":"Next Steps","text":"<p>After mastering deduplication:</p> <ol> <li>Learn tokenizer training: Custom Tokenizer Training</li> <li>Explore batch processing: Batch Processing</li> <li>Try the complete pipeline: Complete Pipeline</li> </ol> <p>\ud83d\udca1 Pro Tip: Start with exact deduplication only on a small dataset to understand the impact, then add semantic deduplication with a conservative threshold (0.9+) before making it more aggressive.</p>"},{"location":"examples/basic-training/","title":"Basic Training Example","text":"<p>This comprehensive example demonstrates how to train a language model from scratch using LLMBuilder. We'll cover data preparation, tokenizer training, model training, and text generation.</p>"},{"location":"examples/basic-training/#overview","title":"\ud83c\udfaf Overview","text":"<p>In this example, we'll:</p> <ol> <li>Prepare training data from various document formats</li> <li>Train a tokenizer on our text corpus</li> <li>Configure and train a language model</li> <li>Generate text with the trained model</li> <li>Evaluate performance and iterate</li> </ol>"},{"location":"examples/basic-training/#project-structure","title":"\ud83d\udcc1 Project Structure","text":"<pre><code>basic_training_example/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/                    # Raw documents (PDF, DOCX, TXT)\n\u2502   \u2514\u2500\u2500 processed/              # Cleaned text files\n\u251c\u2500\u2500 config/\n\u2502   \u2514\u2500\u2500 training_config.json    # Training configuration\n\u251c\u2500\u2500 output/\n\u2502   \u251c\u2500\u2500 tokenizer/              # Trained tokenizer\n\u2502   \u251c\u2500\u2500 model/                  # Trained model\n\u2502   \u2514\u2500\u2500 logs/                   # Training logs\n\u2514\u2500\u2500 train_model.py              # Main training script\n</code></pre>"},{"location":"examples/basic-training/#complete-training-script","title":"\ud83d\ude80 Complete Training Script","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nBasic Training Example for LLMBuilder\n\nThis script demonstrates a complete training pipeline:\n1. Data loading and preprocessing\n2. Tokenizer training\n3. Model training\n4. Text generation and evaluation\n\"\"\"\n\nimport os\nimport sys\nfrom pathlib import Path\nimport logging\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef main():\n    \"\"\"Main training pipeline.\"\"\"\n\n    # Configuration\n    project_dir = Path(__file__).parent\n    raw_data_dir = project_dir / \"data\" / \"raw\"\n    processed_data_dir = project_dir / \"data\" / \"processed\"\n    output_dir = project_dir / \"output\"\n\n    # Create directories\n    for dir_path in [processed_data_dir, output_dir]:\n        dir_path.mkdir(parents=True, exist_ok=True)\n\n    logger.info(\"\ud83d\ude80 Starting LLMBuilder training pipeline\")\n\n    # Step 1: Data Preparation\n    logger.info(\"\ud83d\udcc1 Step 1: Preparing training data\")\n    training_data_path = prepare_training_data(raw_data_dir, processed_data_dir)\n\n    # Step 2: Tokenizer Training\n    logger.info(\"\ud83d\udd24 Step 2: Training tokenizer\")\n    tokenizer_dir = train_tokenizer(training_data_path, output_dir)\n\n    # Step 3: Model Training\n    logger.info(\"\ud83e\udde0 Step 3: Training language model\")\n    model_path = train_language_model(training_data_path, tokenizer_dir, output_dir)\n\n    # Step 4: Text Generation\n    logger.info(\"\ud83c\udfaf Step 4: Testing text generation\")\n    test_text_generation(model_path, tokenizer_dir)\n\n    # Step 5: Evaluation\n    logger.info(\"\ud83d\udcca Step 5: Evaluating model performance\")\n    evaluate_model(model_path, tokenizer_dir, training_data_path)\n\n    logger.info(\"\u2705 Training pipeline completed successfully!\")\n\ndef prepare_training_data(raw_data_dir, processed_data_dir):\n    \"\"\"Load and preprocess training data from various formats.\"\"\"\n    from llmbuilder.data import DataLoader, TextCleaner\n\n    # Initialize data loader\n    loader = DataLoader(\n        min_length=50,              # Filter short texts\n        clean_text=True,            # Apply basic cleaning\n        remove_duplicates=True      # Remove duplicate content\n    )\n\n    # Load all supported files\n    texts = []\n    supported_extensions = ['.txt', '.pdf', '.docx', '.md', '.html']\n\n    logger.info(f\"Loading documents from {raw_data_dir}\")\n\n    if raw_data_dir.exists():\n        for file_path in raw_data_dir.rglob(\"*\"):\n            if file_path.suffix.lower() in supported_extensions:\n                try:\n                    text = loader.load_file(file_path)\n                    if text:\n                        texts.append(text)\n                        logger.info(f\"  \u2705 Loaded {file_path.name}: {len(text):,} characters\")\n                except Exception as e:\n                    logger.warning(f\"  \u274c Failed to load {file_path.name}: {e}\")\n\n    # If no files found, create sample data\n    if not texts:\n        logger.info(\"No data files found, creating sample training data\")\n        sample_text = create_sample_data()\n        texts = [sample_text]\n\n    # Combine and clean texts\n    combined_text = \"\\n\\n\".join(texts)\n\n    # Advanced text cleaning\n    cleaner = TextCleaner(\n        normalize_whitespace=True,\n        remove_urls=True,\n        remove_emails=True,\n        min_sentence_length=20,\n        remove_duplicates=True,\n        language_filter=\"en\"        # Keep only English text\n    )\n\n    cleaned_text = cleaner.clean(combined_text)\n    stats = cleaner.get_stats()\n\n    logger.info(f\"Text cleaning results:\")\n    logger.info(f\"  Original: {stats.original_length:,} characters\")\n    logger.info(f\"  Cleaned: {stats.cleaned_length:,} characters\")\n    logger.info(f\"  Removed: {stats.removal_percentage:.1f}%\")\n\n    # Save processed data\n    training_data_path = processed_data_dir / \"training_data.txt\"\n    with open(training_data_path, 'w', encoding='utf-8') as f:\n        f.write(cleaned_text)\n\n    logger.info(f\"Training data saved to {training_data_path}\")\n    return training_data_path\n\ndef train_tokenizer(training_data_path, output_dir):\n    \"\"\"Train a BPE tokenizer on the training data.\"\"\"\n    from llmbuilder.tokenizer import TokenizerTrainer\n    from llmbuilder.config import TokenizerConfig\n\n    tokenizer_dir = output_dir / \"tokenizer\"\n    tokenizer_dir.mkdir(exist_ok=True)\n\n    # Configure tokenizer\n    config = TokenizerConfig(\n        vocab_size=16000,           # Vocabulary size\n        model_type=\"bpe\",           # Byte-Pair Encoding\n        character_coverage=1.0,     # Cover all characters\n        max_sentence_length=4096,   # Maximum sentence length\n        special_tokens=[            # Special tokens\n            \"&lt;pad&gt;\", \"&lt;unk&gt;\", \"&lt;s&gt;\", \"&lt;/s&gt;\", \"&lt;mask&gt;\"\n        ]\n    )\n\n    # Train tokenizer\n    trainer = TokenizerTrainer(config=config)\n    results = trainer.train(\n        input_file=str(training_data_path),\n        output_dir=str(tokenizer_dir),\n        model_prefix=\"tokenizer\"\n    )\n\n    logger.info(f\"Tokenizer training completed:\")\n    logger.info(f\"  Model: {results['model_file']}\")\n    logger.info(f\"  Vocab: {results['vocab_file']}\")\n    logger.info(f\"  Training time: {results['training_time']:.1f}s\")\n\n    # Test tokenizer\n    from llmbuilder.tokenizer import Tokenizer\n    tokenizer = Tokenizer.from_pretrained(str(tokenizer_dir))\n\n    test_text = \"Hello, world! This is a test of the tokenizer.\"\n    tokens = tokenizer.encode(test_text)\n    decoded = tokenizer.decode(tokens)\n\n    logger.info(f\"Tokenizer test:\")\n    logger.info(f\"  Original: {test_text}\")\n    logger.info(f\"  Tokens: {tokens}\")\n    logger.info(f\"  Decoded: {decoded}\")\n    logger.info(f\"  Perfect reconstruction: {test_text == decoded}\")\n\n    return tokenizer_dir\n\ndef train_language_model(training_data_path, tokenizer_dir, output_dir):\n    \"\"\"Train the language model.\"\"\"\n    import llmbuilder as lb\n    from llmbuilder.config import Config, ModelConfig, TrainingConfig\n    from llmbuilder.data import TextDataset\n\n    model_dir = output_dir / \"model\"\n    model_dir.mkdir(exist_ok=True)\n\n    # Create configuration\n    config = Config(\n        model=ModelConfig(\n            vocab_size=16000,           # Must match tokenizer\n            num_layers=8,               # Number of transformer layers\n            num_heads=8,                # Number of attention heads\n            embedding_dim=512,          # Embedding dimension\n            max_seq_length=1024,        # Maximum sequence length\n            dropout=0.1,                # Dropout rate\n            model_type=\"gpt\"            # Model architecture\n        ),\n        training=TrainingConfig(\n            batch_size=8,               # Batch size (adjust for your hardware)\n            num_epochs=10,              # Number of training epochs\n            learning_rate=3e-4,         # Learning rate\n            warmup_steps=1000,          # Warmup steps\n            weight_decay=0.01,          # Weight decay\n            max_grad_norm=1.0,          # Gradient clipping\n            save_every=1000,            # Save checkpoint every N steps\n            eval_every=500,             # Evaluate every N steps\n            log_every=100               # Log every N steps\n        )\n    )\n\n    # Save configuration\n    config_path = model_dir / \"config.json\"\n    config.save(str(config_path))\n    logger.info(f\"Configuration saved to {config_path}\")\n\n    # Build model\n    model = lb.build_model(config.model)\n    num_params = sum(p.numel() for p in model.parameters())\n    logger.info(f\"Model built with {num_params:,} parameters\")\n\n    # Prepare dataset\n    dataset = TextDataset(\n        data_path=str(training_data_path),\n        block_size=config.model.max_seq_length,\n        stride=config.model.max_seq_length // 2,  # 50% overlap\n        cache_in_memory=True\n    )\n\n    logger.info(f\"Dataset prepared: {len(dataset):,} samples\")\n\n    # Train model\n    results = lb.train_model(\n        model=model,\n        dataset=dataset,\n        config=config.training,\n        checkpoint_dir=str(model_dir)\n    )\n\n    logger.info(f\"Training completed:\")\n    logger.info(f\"  Final loss: {results.final_loss:.4f}\")\n    logger.info(f\"  Training time: {results.training_time}\")\n    logger.info(f\"  Model saved to: {results.model_path}\")\n\n    return results.model_path\n\ndef test_text_generation(model_path, tokenizer_dir):\n    \"\"\"Test text generation with the trained model.\"\"\"\n    import llmbuilder as lb\n\n    test_prompts = [\n        \"Artificial intelligence is\",\n        \"The future of technology\",\n        \"Machine learning can help us\",\n        \"In the world of programming\",\n        \"The benefits of renewable energy\"\n    ]\n\n    logger.info(\"Testing text generation:\")\n\n    for prompt in test_prompts:\n        try:\n            generated_text = lb.generate_text(\n                model_path=model_path,\n                tokenizer_path=str(tokenizer_dir),\n                prompt=prompt,\n                max_new_tokens=50,\n                temperature=0.8,\n                top_k=50,\n                top_p=0.9\n            )\n\n            logger.info(f\"  Prompt: {prompt}\")\n            logger.info(f\"  Generated: {generated_text}\")\n            logger.info(\"\")\n\n        except Exception as e:\n            logger.error(f\"  Generation failed for '{prompt}': {e}\")\n\ndef evaluate_model(model_path, tokenizer_dir, training_data_path):\n    \"\"\"Evaluate model performance.\"\"\"\n    from llmbuilder.model import load_model\n    from llmbuilder.tokenizer import Tokenizer\n    from llmbuilder.data import TextDataset\n    import torch\n\n    # Load model and tokenizer\n    model = load_model(model_path)\n    tokenizer = Tokenizer.from_pretrained(str(tokenizer_dir))\n\n    # Create evaluation dataset (small sample)\n    eval_dataset = TextDataset(\n        data_path=str(training_data_path),\n        block_size=512,\n        stride=256,\n        max_samples=100  # Small sample for quick evaluation\n    )\n\n    # Calculate perplexity\n    model.eval()\n    total_loss = 0\n    total_tokens = 0\n\n    with torch.no_grad():\n        for i, batch in enumerate(eval_dataset):\n            if i &gt;= 10:  # Limit evaluation for speed\n                break\n\n            input_ids = torch.tensor([batch], dtype=torch.long)\n\n            # Forward pass\n            outputs = model(input_ids[:, :-1])\n            logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n\n            # Calculate loss\n            targets = input_ids[:, 1:]\n            loss = torch.nn.functional.cross_entropy(\n                logits.view(-1, logits.size(-1)),\n                targets.view(-1),\n                ignore_index=tokenizer.pad_token_id\n            )\n\n            total_loss += loss.item()\n            total_tokens += targets.numel()\n\n    avg_loss = total_loss / min(10, len(eval_dataset))\n    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n\n    logger.info(f\"Model evaluation:\")\n    logger.info(f\"  Average loss: {avg_loss:.4f}\")\n    logger.info(f\"  Perplexity: {perplexity:.2f}\")\n    logger.info(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\ndef create_sample_data():\n    \"\"\"Create sample training data if no files are found.\"\"\"\n    return \"\"\"\n    Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.\n\n    Machine learning (ML) is a type of artificial intelligence that allows software applications to become more accurate at predicting outcomes without being explicitly programmed to do so. Machine learning algorithms use historical data as input to predict new output values.\n\n    Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.\n\n    Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n\n    Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do.\n\n    The future of artificial intelligence holds great promise for solving complex problems in healthcare, transportation, education, and many other fields. As AI systems become more sophisticated, they will continue to transform how we work, learn, and interact with technology.\n    \"\"\"\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/basic-training/#expected-output","title":"\ud83d\udcca Expected Output","text":"<p>When you run this script, you should see output similar to:</p> <pre><code>INFO:__main__:\ud83d\ude80 Starting LLMBuilder training pipeline\nINFO:__main__:\ud83d\udcc1 Step 1: Preparing training data\nINFO:__main__:No data files found, creating sample training data\nINFO:__main__:Text cleaning results:\nINFO:__main__:  Original: 1,234 characters\nINFO:__main__:  Cleaned: 1,180 characters\nINFO:__main__:  Removed: 4.4%\nINFO:__main__:Training data saved to basic_training_example/data/processed/training_data.txt\n\nINFO:__main__:\ud83d\udd24 Step 2: Training tokenizer\nINFO:__main__:Tokenizer training completed:\nINFO:__main__:  Model: basic_training_example/output/tokenizer/tokenizer.model\nINFO:__main__:  Vocab: basic_training_example/output/tokenizer/tokenizer.vocab\nINFO:__main__:  Training time: 5.2s\nINFO:__main__:Tokenizer test:\nINFO:__main__:  Original: Hello, world! This is a test of the tokenizer.\nINFO:__main__:  Tokens: [15496, 995, 0, 1188, 374, 264, 1296, 315, 279, 4037, 3213, 13]\nINFO:__main__:  Decoded: Hello, world! This is a test of the tokenizer.\nINFO:__main__:  Perfect reconstruction: True\n\nINFO:__main__:\ud83e\udde0 Step 3: Training language model\nINFO:__main__:Configuration saved to basic_training_example/output/model/config.json\nINFO:__main__:Model built with 42,123,456 parameters\nINFO:__main__:Dataset prepared: 156 samples\nINFO:__main__:Training completed:\nINFO:__main__:  Final loss: 2.45\nINFO:__main__:  Training time: 0:15:23\nINFO:__main__:  Model saved to: basic_training_example/output/model/model.pt\n\nINFO:__main__:\ud83c\udfaf Step 4: Testing text generation\nINFO:__main__:Testing text generation:\nINFO:__main__:  Prompt: Artificial intelligence is\nINFO:__main__:  Generated: Artificial intelligence is a rapidly evolving field that encompasses machine learning, deep learning, and neural networks...\n\nINFO:__main__:\ud83d\udcca Step 5: Evaluating model performance\nINFO:__main__:Model evaluation:\nINFO:__main__:  Average loss: 2.52\nINFO:__main__:  Perplexity: 12.4\nINFO:__main__:  Model parameters: 42,123,456\n\nINFO:__main__:\u2705 Training pipeline completed successfully!\n</code></pre>"},{"location":"examples/basic-training/#customization-options","title":"\ud83c\udfaf Customization Options","text":""},{"location":"examples/basic-training/#1-adjust-model-size","title":"1. Adjust Model Size","text":"<pre><code># Smaller model (faster training, less memory)\nmodel_config = ModelConfig(\n    vocab_size=8000,\n    num_layers=4,\n    num_heads=4,\n    embedding_dim=256,\n    max_seq_length=512\n)\n\n# Larger model (better quality, more resources)\nmodel_config = ModelConfig(\n    vocab_size=32000,\n    num_layers=16,\n    num_heads=16,\n    embedding_dim=1024,\n    max_seq_length=2048\n)\n</code></pre>"},{"location":"examples/basic-training/#2-modify-training-parameters","title":"2. Modify Training Parameters","text":"<pre><code># Fast training (for testing)\ntraining_config = TrainingConfig(\n    batch_size=16,\n    num_epochs=3,\n    learning_rate=1e-3,\n    save_every=100\n)\n\n# High-quality training (for production)\ntraining_config = TrainingConfig(\n    batch_size=8,\n    num_epochs=50,\n    learning_rate=1e-4,\n    warmup_steps=2000,\n    weight_decay=0.01\n)\n</code></pre>"},{"location":"examples/basic-training/#3-different-data-sources","title":"3. Different Data Sources","text":"<pre><code># Load from specific file types\nloader = DataLoader(\n    supported_formats=['.txt', '.md'],  # Only text and markdown\n    min_length=100,                     # Longer minimum length\n    max_length=10000,                   # Maximum length limit\n    clean_text=True\n)\n\n# Custom text cleaning\ncleaner = TextCleaner(\n    normalize_whitespace=True,\n    remove_urls=True,\n    remove_emails=True,\n    remove_phone_numbers=True,\n    min_sentence_length=30,\n    language_filter=\"en\",\n    custom_filters=[\n        lambda text: text.replace(\"specific_pattern\", \"replacement\")\n    ]\n)\n</code></pre>"},{"location":"examples/basic-training/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"examples/basic-training/#common-issues","title":"Common Issues","text":""},{"location":"examples/basic-training/#out-of-memory","title":"Out of Memory","text":"<pre><code># Reduce batch size\ntraining_config.batch_size = 4\n\n# Enable gradient checkpointing\nmodel_config.gradient_checkpointing = True\n\n# Use gradient accumulation\ntraining_config.gradient_accumulation_steps = 4\n</code></pre>"},{"location":"examples/basic-training/#poor-generation-quality","title":"Poor Generation Quality","text":"<pre><code># Train for more epochs\ntraining_config.num_epochs = 20\n\n# Use more training data\n# Add more text files to data/raw/\n\n# Adjust generation parameters\ngenerated_text = lb.generate_text(\n    model_path=model_path,\n    tokenizer_path=str(tokenizer_dir),\n    prompt=prompt,\n    max_new_tokens=100,\n    temperature=0.7,        # Lower temperature\n    top_k=40,              # More focused sampling\n    repetition_penalty=1.1  # Reduce repetition\n)\n</code></pre>"},{"location":"examples/basic-training/#slow-training","title":"Slow Training","text":"<pre><code># Use GPU if available\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Reduce sequence length\nmodel_config.max_seq_length = 512\n\n# Increase batch size (if memory allows)\ntraining_config.batch_size = 16\n</code></pre>"},{"location":"examples/basic-training/#next-steps","title":"\ud83d\udcda Next Steps","text":"<p>After running this basic example:</p> <ol> <li>Experiment with different model sizes and training parameters</li> <li>Add your own training data to the <code>data/raw/</code> directory</li> <li>Try fine-tuning the trained model on specific tasks</li> <li>Export the model for deployment using the export functionality</li> <li>Implement evaluation metrics specific to your use case</li> </ol> <p>Training Tips</p> <ul> <li>Start with small models and datasets to verify everything works</li> <li>Monitor training loss to ensure the model is learning</li> <li>Save checkpoints frequently during long training runs</li> <li>Test generation quality throughout training</li> <li>Keep track of what configurations work best for your data</li> </ul>"},{"location":"examples/multi-format-ingestion/","title":"Multi-Format Document Ingestion","text":"<p>This example demonstrates how to process documents in various formats (HTML, PDF, EPUB, Markdown) using LLMBuilder's advanced ingestion pipeline.</p>"},{"location":"examples/multi-format-ingestion/#overview","title":"Overview","text":"<p>LLMBuilder can intelligently process multiple document formats: - HTML: Clean text extraction with configurable parsers - PDF: Text extraction with OCR fallback for scanned documents - EPUB: Chapter-by-chapter extraction with metadata - Markdown: Syntax removal while preserving structure - Plain Text: Direct processing with encoding detection</p>"},{"location":"examples/multi-format-ingestion/#complete-example","title":"Complete Example","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nMulti-Format Document Ingestion Example\n\nDemonstrates processing various document formats with LLMBuilder's\nadvanced ingestion pipeline.\n\"\"\"\n\nimport logging\nfrom pathlib import Path\nfrom llmbuilder.data.ingest import IngestionPipeline\nfrom llmbuilder.config.manager import create_config_from_template\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef main():\n    \"\"\"Demonstrate multi-format document ingestion.\"\"\"\n\n    # Create configuration optimized for document processing\n    config = create_config_from_template(\"advanced_processing_config\", {\n        \"data\": {\n            \"ingestion\": {\n                \"supported_formats\": [\"html\", \"markdown\", \"epub\", \"pdf\", \"txt\"],\n                \"batch_size\": 100,\n                \"num_workers\": 4,\n                \"output_format\": \"jsonl\",  # Preserve metadata\n                \"preserve_structure\": True,\n                \"extract_metadata\": True,\n\n                # HTML processing options\n                \"html_parser\": \"lxml\",  # Fast and robust\n                \"remove_scripts\": True,\n                \"remove_styles\": True,\n\n                # PDF processing with OCR\n                \"enable_ocr\": True,\n                \"ocr_quality_threshold\": 0.6,\n                \"ocr_language\": \"eng\",\n                \"pdf_extraction_method\": \"auto\",\n\n                # EPUB processing\n                \"extract_toc\": True,\n                \"chapter_separator\": \"\\n\\n---\\n\\n\"\n            }\n        }\n    })\n\n    # Initialize ingestion pipeline\n    pipeline = IngestionPipeline(config.data.ingestion)\n\n    # Process documents from a directory\n    input_dir = Path(\"./documents\")\n    output_dir = Path(\"./processed\")\n    output_dir.mkdir(exist_ok=True)\n\n    logger.info(f\"Processing documents from {input_dir}\")\n\n    # Process all supported files in the directory\n    results = pipeline.process_directory(\n        input_dir=str(input_dir),\n        output_dir=str(output_dir)\n    )\n\n    # Display results\n    logger.info(\"Processing completed!\")\n    logger.info(f\"Files processed: {results['files_processed']}\")\n    logger.info(f\"Total text extracted: {results['total_characters']:,} characters\")\n    logger.info(f\"Processing time: {results['processing_time']:.1f}s\")\n\n    # Show per-format statistics\n    for format_type, stats in results['format_stats'].items():\n        logger.info(f\"{format_type.upper()}: {stats['files']} files, {stats['characters']:,} chars\")\n\n    # Process individual files with specific settings\n    process_individual_files(pipeline, output_dir)\n\ndef process_individual_files(pipeline, output_dir):\n    \"\"\"Demonstrate processing individual files with custom settings.\"\"\"\n\n    logger.info(\"\\nProcessing individual files:\")\n\n    # Example files (create sample files for demonstration)\n    sample_files = create_sample_files()\n\n    for file_path in sample_files:\n        try:\n            logger.info(f\"Processing {file_path.name}...\")\n\n            # Process single file\n            result = pipeline.process_file(\n                input_file=str(file_path),\n                output_file=str(output_dir / f\"processed_{file_path.stem}.txt\")\n            )\n\n            if result['success']:\n                logger.info(f\"  \u2705 Extracted {result['characters']:,} characters\")\n                if result.get('metadata'):\n                    logger.info(f\"  \ud83d\udcca Metadata: {result['metadata']}\")\n            else:\n                logger.error(f\"  \u274c Failed: {result['error']}\")\n\n        except Exception as e:\n            logger.error(f\"  \u274c Error processing {file_path.name}: {e}\")\n\ndef create_sample_files():\n    \"\"\"Create sample files for demonstration.\"\"\"\n\n    sample_dir = Path(\"./sample_documents\")\n    sample_dir.mkdir(exist_ok=True)\n\n    # Create sample HTML file\n    html_content = \"\"\"\n    &lt;!DOCTYPE html&gt;\n    &lt;html&gt;\n    &lt;head&gt;\n        &lt;title&gt;Machine Learning Basics&lt;/title&gt;\n        &lt;style&gt;body { font-family: Arial; }&lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;h1&gt;Introduction to Machine Learning&lt;/h1&gt;\n        &lt;p&gt;Machine learning is a subset of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed.&lt;/p&gt;\n\n        &lt;h2&gt;Types of Machine Learning&lt;/h2&gt;\n        &lt;ul&gt;\n            &lt;li&gt;&lt;strong&gt;Supervised Learning&lt;/strong&gt;: Learning with labeled examples&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;Unsupervised Learning&lt;/strong&gt;: Finding patterns in unlabeled data&lt;/li&gt;\n            &lt;li&gt;&lt;strong&gt;Reinforcement Learning&lt;/strong&gt;: Learning through interaction and feedback&lt;/li&gt;\n        &lt;/ul&gt;\n\n        &lt;script&gt;console.log(\"This script will be removed\");&lt;/script&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n    \"\"\"\n\n    # Create sample Markdown file\n    markdown_content = \"\"\"\n    # Deep Learning Guide\n\n    Deep learning is a subset of machine learning that uses neural networks with multiple layers.\n\n    ## Key Concepts\n\n    ### Neural Networks\n    - **Neurons**: Basic processing units\n    - **Layers**: Collections of neurons\n    - **Weights**: Connection strengths between neurons\n\n    ### Training Process\n    1. Forward propagation\n    2. Loss calculation\n    3. Backpropagation\n    4. Weight updates\n\n    ## Applications\n\n    Deep learning has revolutionized:\n    - Computer vision\n    - Natural language processing\n    - Speech recognition\n    - Game playing (AlphaGo, etc.)\n\n    &gt; \"Deep learning is not just a trend, it's a fundamental shift in how we approach AI problems.\"\n    \"\"\"\n\n    # Create sample plain text file\n    text_content = \"\"\"\n    Natural Language Processing (NLP) Fundamentals\n\n    Natural Language Processing is a branch of artificial intelligence that helps computers understand, interpret, and manipulate human language. NLP draws from many disciplines, including computer science and computational linguistics.\n\n    Key NLP Tasks:\n    - Tokenization: Breaking text into individual words or tokens\n    - Part-of-speech tagging: Identifying grammatical roles of words\n    - Named entity recognition: Identifying people, places, organizations\n    - Sentiment analysis: Determining emotional tone of text\n    - Machine translation: Converting text from one language to another\n    - Text summarization: Creating concise summaries of longer texts\n\n    Modern NLP relies heavily on machine learning and deep learning techniques, particularly transformer models like BERT and GPT.\n    \"\"\"\n\n    # Write sample files\n    files = []\n\n    html_file = sample_dir / \"ml_basics.html\"\n    html_file.write_text(html_content, encoding='utf-8')\n    files.append(html_file)\n\n    md_file = sample_dir / \"deep_learning.md\"\n    md_file.write_text(markdown_content, encoding='utf-8')\n    files.append(md_file)\n\n    txt_file = sample_dir / \"nlp_fundamentals.txt\"\n    txt_file.write_text(text_content, encoding='utf-8')\n    files.append(txt_file)\n\n    return files\n\ndef demonstrate_format_specific_processing():\n    \"\"\"Show format-specific processing options.\"\"\"\n\n    logger.info(\"\\nDemonstrating format-specific processing:\")\n\n    # HTML-specific processing\n    from llmbuilder.data.ingest import HTMLProcessor\n\n    html_processor = HTMLProcessor(\n        parser='lxml',           # Use lxml for speed\n        remove_scripts=True,     # Remove JavaScript\n        remove_styles=True,      # Remove CSS\n        extract_links=True,      # Extract hyperlinks\n        preserve_formatting=False # Clean formatting\n    )\n\n    # PDF-specific processing\n    from llmbuilder.data.ingest import PDFProcessor\n\n    pdf_processor = PDFProcessor(\n        enable_ocr=True,         # Enable OCR fallback\n        ocr_quality_threshold=0.7, # Higher threshold for better quality\n        ocr_language='eng',      # English OCR\n        extract_images=False,    # Don't extract images\n        preserve_layout=True     # Maintain document layout\n    )\n\n    # EPUB-specific processing\n    from llmbuilder.data.ingest import EPUBProcessor\n\n    epub_processor = EPUBProcessor(\n        extract_toc=True,        # Extract table of contents\n        chapter_separator='\\n\\n---\\n\\n', # Chapter separator\n        include_metadata=True,   # Include book metadata\n        extract_images=False     # Don't extract images\n    )\n\n    logger.info(\"Format-specific processors configured\")\n\nif __name__ == \"__main__\":\n    print(\"\ud83d\udd04 Multi-Format Document Ingestion Example\")\n    print(\"=\" * 50)\n    print()\n    print(\"This example demonstrates:\")\n    print(\"\u2022 Processing HTML, PDF, EPUB, Markdown, and text files\")\n    print(\"\u2022 Configurable extraction options for each format\")\n    print(\"\u2022 Batch processing with progress reporting\")\n    print(\"\u2022 Metadata extraction and preservation\")\n    print(\"\u2022 OCR fallback for scanned PDFs\")\n    print()\n\n    try:\n        main()\n        demonstrate_format_specific_processing()\n        print(\"\\n\u2705 Example completed successfully!\")\n\n    except ImportError as e:\n        print(f\"\\n\u274c Missing dependencies: {e}\")\n        print(\"Install with: pip install llmbuilder[all]\")\n\n    except Exception as e:\n        print(f\"\\n\u274c Example failed: {e}\")\n        print(\"\\n\ud83d\udca1 Troubleshooting:\")\n        print(\"1. Ensure input documents exist\")\n        print(\"2. Check file permissions\")\n        print(\"3. Install system dependencies (Tesseract for OCR)\")\n</code></pre>"},{"location":"examples/multi-format-ingestion/#configuration-options","title":"Configuration Options","text":""},{"location":"examples/multi-format-ingestion/#basic-configuration","title":"Basic Configuration","text":"<pre><code>config = {\n    \"data\": {\n        \"ingestion\": {\n            \"supported_formats\": [\"html\", \"pdf\", \"txt\"],\n            \"batch_size\": 50,\n            \"num_workers\": 2,\n            \"output_format\": \"txt\"\n        }\n    }\n}\n</code></pre>"},{"location":"examples/multi-format-ingestion/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>config = {\n    \"data\": {\n        \"ingestion\": {\n            \"supported_formats\": [\"html\", \"markdown\", \"epub\", \"pdf\", \"txt\"],\n            \"batch_size\": 200,\n            \"num_workers\": 8,\n            \"output_format\": \"jsonl\",\n            \"preserve_structure\": True,\n            \"extract_metadata\": True,\n\n            # HTML processing\n            \"html_parser\": \"lxml\",\n            \"remove_scripts\": True,\n            \"remove_styles\": True,\n\n            # PDF processing\n            \"enable_ocr\": True,\n            \"ocr_quality_threshold\": 0.7,\n            \"ocr_language\": \"eng\",\n            \"pdf_extraction_method\": \"auto\",\n\n            # EPUB processing\n            \"extract_toc\": True,\n            \"chapter_separator\": \"\\n\\n---\\n\\n\"\n        }\n    }\n}\n</code></pre>"},{"location":"examples/multi-format-ingestion/#cli-usage","title":"CLI Usage","text":"<pre><code># Process all supported files in a directory\nllmbuilder data load -i ./documents -o ./processed.txt --format all --clean\n\n# Process specific formats only\nllmbuilder data load -i ./documents -o ./processed.txt --format pdf,html --enable-ocr\n\n# Batch processing with custom settings\nllmbuilder data load -i ./documents -o ./processed.txt \\\n  --format all \\\n  --batch-size 100 \\\n  --workers 8 \\\n  --enable-ocr \\\n  --ocr-threshold 0.7\n</code></pre>"},{"location":"examples/multi-format-ingestion/#format-specific-features","title":"Format-Specific Features","text":""},{"location":"examples/multi-format-ingestion/#html-processing","title":"HTML Processing","text":"<ul> <li>Parser options: html.parser, lxml, html5lib</li> <li>Content filtering: Remove scripts, styles, comments</li> <li>Link extraction: Extract and preserve hyperlinks</li> <li>Table handling: Convert tables to structured text</li> </ul>"},{"location":"examples/multi-format-ingestion/#pdf-processing","title":"PDF Processing","text":"<ul> <li>Text extraction: Direct text extraction from searchable PDFs</li> <li>OCR fallback: Automatic OCR for scanned documents</li> <li>Quality assessment: Intelligent switching between text and OCR</li> <li>Layout preservation: Maintain document structure</li> </ul>"},{"location":"examples/multi-format-ingestion/#epub-processing","title":"EPUB Processing","text":"<ul> <li>Chapter extraction: Process books chapter by chapter</li> <li>Metadata handling: Extract title, author, publication info</li> <li>Table of contents: Extract and use book structure</li> <li>Image handling: Optional image extraction</li> </ul>"},{"location":"examples/multi-format-ingestion/#markdown-processing","title":"Markdown Processing","text":"<ul> <li>Syntax removal: Clean markdown syntax while preserving content</li> <li>Structure preservation: Maintain headings and lists</li> <li>Link extraction: Extract and preserve links</li> <li>Code block handling: Special handling for code sections</li> </ul>"},{"location":"examples/multi-format-ingestion/#performance-optimization","title":"Performance Optimization","text":""},{"location":"examples/multi-format-ingestion/#memory-optimization","title":"Memory Optimization","text":"<pre><code>config = {\n    \"data\": {\n        \"ingestion\": {\n            \"batch_size\": 25,        # Smaller batches\n            \"num_workers\": 2,        # Fewer workers\n            \"stream_processing\": True # Process files one at a time\n        }\n    }\n}\n</code></pre>"},{"location":"examples/multi-format-ingestion/#speed-optimization","title":"Speed Optimization","text":"<pre><code>config = {\n    \"data\": {\n        \"ingestion\": {\n            \"batch_size\": 200,       # Larger batches\n            \"num_workers\": 8,        # More workers\n            \"html_parser\": \"lxml\",   # Fastest HTML parser\n            \"enable_ocr\": False      # Disable OCR if not needed\n        }\n    }\n}\n</code></pre>"},{"location":"examples/multi-format-ingestion/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/multi-format-ingestion/#common-issues","title":"Common Issues","text":"<p>OCR Not Working <pre><code># Install Tesseract\n# Ubuntu: sudo apt-get install tesseract-ocr\n# macOS: brew install tesseract\n# Windows: Download from GitHub\n\n# Verify installation\ntesseract --version\npython -c \"import pytesseract; print('OCR available')\"\n</code></pre></p> <p>HTML Parser Issues <pre><code># Install lxml for better HTML parsing\npip install lxml\n\n# Or use built-in parser\nconfig[\"data\"][\"ingestion\"][\"html_parser\"] = \"html.parser\"\n</code></pre></p> <p>Memory Issues with Large Files <pre><code># Reduce batch size and workers\nconfig = {\n    \"data\": {\n        \"ingestion\": {\n            \"batch_size\": 10,\n            \"num_workers\": 1,\n            \"stream_processing\": True\n        }\n    }\n}\n</code></pre></p> <p>Encoding Issues <pre><code># Enable encoding detection\nconfig = {\n    \"data\": {\n        \"ingestion\": {\n            \"detect_encoding\": True,\n            \"fallback_encoding\": \"utf-8\"\n        }\n    }\n}\n</code></pre></p>"},{"location":"examples/multi-format-ingestion/#next-steps","title":"Next Steps","text":"<p>After mastering multi-format ingestion:</p> <ol> <li>Learn deduplication: Advanced Deduplication</li> <li>Explore batch processing: Batch Processing</li> <li>Try the complete pipeline: Complete Pipeline</li> </ol> <p>\ud83d\udca1 Pro Tip: Start with a small set of documents to test your configuration, then scale up to larger datasets once everything works correctly.</p>"},{"location":"examples/model-cards/code-generation-model/","title":"Model Card: CodeAssist Pro v2.1","text":""},{"location":"examples/model-cards/code-generation-model/#model-details","title":"Model Details","text":""},{"location":"examples/model-cards/code-generation-model/#basic-information","title":"Basic Information","text":"<ul> <li>Model Name: CodeAssist Pro</li> <li>Model Version: v2.1.0</li> <li>Model Type: GPT-style Transformer (Decoder-only) optimized for code</li> <li>Architecture: Transformer with 16 layers, 1024 hidden dimensions</li> <li>Training Framework: LLMBuilder v1.3.0</li> <li>Model Size: 350M parameters</li> <li>License: Apache 2.0</li> <li>Contact: codeassist@devtools.com</li> <li>Date: April 22, 2024</li> </ul>"},{"location":"examples/model-cards/code-generation-model/#model-architecture","title":"Model Architecture","text":"<pre><code>Model Configuration:\n- Vocabulary Size: 50,000 tokens (code-optimized)\n- Hidden Dimensions: 1024\n- Number of Layers: 16\n- Number of Attention Heads: 16\n- Maximum Sequence Length: 4096 tokens\n- Activation Function: GELU\n- Dropout Rate: 0.05 (reduced for code precision)\n- Layer Normalization: Pre-norm\n- Position Encoding: Rotary Position Embedding (RoPE)\n</code></pre>"},{"location":"examples/model-cards/code-generation-model/#training-configuration","title":"Training Configuration","text":"<pre><code>{\n  \"model\": {\n    \"vocab_size\": 50000,\n    \"num_layers\": 16,\n    \"num_heads\": 16,\n    \"embedding_dim\": 1024,\n    \"max_seq_length\": 4096,\n    \"dropout\": 0.05\n  },\n  \"training\": {\n    \"batch_size\": 16,\n    \"learning_rate\": 1e-4,\n    \"num_epochs\": 25,\n    \"optimizer\": \"AdamW\",\n    \"weight_decay\": 0.01,\n    \"warmup_steps\": 5000\n  },\n  \"tokenizer\": {\n    \"algorithm\": \"bpe\",\n    \"vocab_size\": 50000,\n    \"special_tokens\": [\"&lt;pad&gt;\", \"&lt;unk&gt;\", \"&lt;s&gt;\", \"&lt;/s&gt;\", \"&lt;mask&gt;\", \"&lt;code&gt;\", \"&lt;/code&gt;\", \"&lt;comment&gt;\", \"&lt;/comment&gt;\"]\n  }\n}\n</code></pre>"},{"location":"examples/model-cards/code-generation-model/#intended-use","title":"Intended Use","text":""},{"location":"examples/model-cards/code-generation-model/#primary-use-cases","title":"Primary Use Cases","text":"<ul> <li>Code completion and suggestion</li> <li>Function and class generation from docstrings</li> <li>Code explanation and documentation</li> <li>Bug detection and fixing suggestions</li> <li>Code refactoring recommendations</li> <li>Unit test generation</li> <li>API usage examples</li> </ul>"},{"location":"examples/model-cards/code-generation-model/#intended-users","title":"Intended Users","text":"<ul> <li>Software developers and engineers</li> <li>Computer science students</li> <li>Code reviewers and maintainers</li> <li>DevOps engineers</li> <li>Technical writers documenting code</li> </ul>"},{"location":"examples/model-cards/code-generation-model/#supported-programming-languages","title":"Supported Programming Languages","text":"<ul> <li>Primary: Python, JavaScript, TypeScript, Java, C++</li> <li>Secondary: Go, Rust, C#, PHP, Ruby</li> <li>Limited: SQL, HTML, CSS, Shell scripts</li> </ul>"},{"location":"examples/model-cards/code-generation-model/#out-of-scope-uses","title":"Out-of-Scope Uses","text":"<ul> <li>Production-critical code without human review</li> <li>Security-sensitive implementations</li> <li>Code for safety-critical systems (medical, automotive, aerospace)</li> <li>Legal or compliance-related code generation</li> <li>Proprietary algorithm implementation</li> </ul>"},{"location":"examples/model-cards/code-generation-model/#training-data","title":"Training Data","text":""},{"location":"examples/model-cards/code-generation-model/#data-sources","title":"Data Sources","text":"<ul> <li>Primary Dataset: Multi-language Code Corpus</li> <li>Size: 180GB of source code, approximately 45 billion tokens</li> <li>Languages: 15+ programming languages</li> <li>Time Period: Open source repositories from 2015-2024</li> <li>Domains: Web development, data science, system programming, mobile apps</li> </ul>"},{"location":"examples/model-cards/code-generation-model/#data-processing","title":"Data Processing","text":"<ul> <li>Ingestion: Advanced code-aware processing</li> <li>Formats processed: .py, .js, .ts, .java, .cpp, .go, .rs, .cs, .php, .rb</li> <li>Repository structure preservation</li> <li> <p>Metadata extraction: Language, license, stars, commits</p> </li> <li> <p>Deduplication: Code-specific deduplication</p> </li> <li>Exact deduplication: Yes (function-level)</li> <li>Semantic deduplication: Yes (threshold: 0.90)</li> <li>Similarity metric: Code structure similarity with tree-sitter</li> <li> <p>Duplicates removed: 25% of original data</p> </li> <li> <p>Filtering and Cleaning: Code quality filtering</p> </li> <li>Syntax validation for all supported languages</li> <li>License filtering (permissive licenses only)</li> <li>Quality filtering based on repository metrics</li> <li>Length filtering (minimum 10 lines, maximum 1000 lines per function)</li> <li>Removal of generated/minified code</li> </ul>"},{"location":"examples/model-cards/code-generation-model/#data-quality","title":"Data Quality","text":"<ul> <li>Quality Metrics: High-star repositories, active maintenance, good documentation</li> <li>Known Issues: Potential bias toward popular frameworks and libraries</li> <li>Bias Considerations: Over-representation of English comments and variable names</li> </ul>"},{"location":"examples/model-cards/code-generation-model/#performance","title":"Performance","text":""},{"location":"examples/model-cards/code-generation-model/#evaluation-metrics","title":"Evaluation Metrics","text":"Metric Value Dataset Notes Pass@1 42.3% HumanEval Code correctness Pass@10 68.7% HumanEval Top-10 suggestions BLEU Score 0.71 Code completion Syntactic similarity CodeBLEU 0.65 Multi-language Code-specific metric Compilation Rate 89.2% Generated functions Syntactically correct"},{"location":"examples/model-cards/code-generation-model/#benchmark-results","title":"Benchmark Results","text":"<pre><code>Evaluation on Code Generation Benchmarks:\n- HumanEval (Python): 42.3% Pass@1, 68.7% Pass@10\n- MBPP (Python): 38.9% Pass@1, 62.1% Pass@10\n- CodeContests: 15.2% Pass@1 (competitive programming)\n- MultiPL-E (avg): 35.7% Pass@1 across languages\n</code></pre>"},{"location":"examples/model-cards/code-generation-model/#performance-by-language","title":"Performance by Language","text":"Language Pass@1 Pass@10 Notes Python 42.3% 68.7% Best performance JavaScript 38.1% 64.2% Strong web dev support Java 35.7% 59.8% Good enterprise patterns C++ 28.4% 51.3% Complex syntax challenges TypeScript 36.9% 62.1% Type inference support"},{"location":"examples/model-cards/code-generation-model/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>Inference Speed: 85 tokens/second on RTX 4090</li> <li>Memory Requirements: 12GB VRAM for inference, 4GB for GGUF Q4_0</li> <li>Quantization Support: GGUF Q8_0, Q4_0, Q4_1 available</li> <li>Hardware Requirements: </li> <li>Minimum: 16GB RAM, modern CPU</li> <li>Recommended: 32GB RAM, RTX 4070 or better</li> </ul>"},{"location":"examples/model-cards/code-generation-model/#limitations-and-risks","title":"Limitations and Risks","text":""},{"location":"examples/model-cards/code-generation-model/#known-limitations","title":"Known Limitations","text":"<ul> <li>Limited context window may truncate large codebases</li> <li>May generate syntactically correct but logically flawed code</li> <li>Performance varies significantly across programming languages</li> <li>Limited understanding of complex business logic</li> <li>May not follow latest language features or best practices</li> <li>Struggles with highly domain-specific APIs</li> </ul>"},{"location":"examples/model-cards/code-generation-model/#potential-risks","title":"Potential Risks","text":"<ul> <li>Security Vulnerabilities: May generate insecure code patterns</li> <li>Copyright Issues: Potential reproduction of copyrighted code</li> <li>Dependency Issues: May suggest outdated or vulnerable dependencies</li> <li>Logic Errors: Code may compile but contain subtle bugs</li> <li>Performance Issues: Generated code may be inefficient</li> </ul>"},{"location":"examples/model-cards/code-generation-model/#mitigation-strategies","title":"Mitigation Strategies","text":"<ul> <li>Mandatory code review for all generated code</li> <li>Security scanning and vulnerability assessment</li> <li>Unit testing requirements for generated functions</li> <li>Documentation of code generation assistance</li> <li>Regular model updates with security patches</li> </ul>"},{"location":"examples/model-cards/code-generation-model/#ethical-considerations","title":"Ethical Considerations","text":""},{"location":"examples/model-cards/code-generation-model/#responsible-use","title":"Responsible Use","text":"<ul> <li>Always review and test generated code before deployment</li> <li>Disclose AI assistance in code contributions</li> <li>Respect open source licenses and attribution</li> <li>Consider impact on junior developer learning</li> <li>Maintain human oversight for critical systems</li> </ul>"},{"location":"examples/model-cards/code-generation-model/#environmental-impact","title":"Environmental Impact","text":"<ul> <li>Training Compute: 800 GPU-hours on A100 GPUs</li> <li>Carbon Footprint: Approximately 240 kg CO2 equivalent</li> <li>Energy Consumption: 12,800 kWh total training energy</li> </ul>"},{"location":"examples/model-cards/code-generation-model/#technical-specifications","title":"Technical Specifications","text":""},{"location":"examples/model-cards/code-generation-model/#model-files","title":"Model Files","text":"<ul> <li>Model Weights: <code>codeassist_pro_v2.1.pt</code> (1.4GB)</li> <li>Configuration: <code>config.json</code></li> <li>Tokenizer: <code>tokenizer/</code> directory (45MB)</li> <li>GGUF Versions: </li> <li><code>codeassist_pro_q8_0.gguf</code> (1.35GB) - High quality</li> <li><code>codeassist_pro_q4_0.gguf</code> (680MB) - Balanced</li> <li><code>codeassist_pro_q4_1.gguf</code> (750MB) - Compact</li> </ul>"},{"location":"examples/model-cards/code-generation-model/#dependencies","title":"Dependencies","text":"<pre><code>Required:\n- Python &gt;= 3.8\n- PyTorch &gt;= 1.13.0\n- LLMBuilder &gt;= 1.3.0\n- transformers &gt;= 4.25.0\n\nOptional:\n- CUDA &gt;= 11.8 (for GPU inference)\n- llama.cpp (for GGUF inference)\n- tree-sitter (for code parsing)\n</code></pre>"},{"location":"examples/model-cards/code-generation-model/#usage-example","title":"Usage Example","text":"<pre><code>import llmbuilder as lb\n\n# Load the model\nmodel = lb.load_model(\"codeassist_pro_v2.1.pt\")\ntokenizer = lb.load_tokenizer(\"tokenizer/\")\n\n# Generate code completion\ncode = lb.generate_text(\n    model=model,\n    tokenizer=tokenizer,\n    prompt='''def fibonacci(n):\n    \"\"\"\n    Calculate the nth Fibonacci number using dynamic programming.\n\n    Args:\n        n (int): The position in the Fibonacci sequence\n\n    Returns:\n        int: The nth Fibonacci number\n    \"\"\"''',\n    max_new_tokens=150,\n    temperature=0.2,  # Lower temperature for code\n    top_p=0.95,\n    stop_tokens=[\"def \", \"class \", \"\\n\\n\"]\n)\nprint(code)\n</code></pre>"},{"location":"examples/model-cards/code-generation-model/#cli-usage","title":"CLI Usage","text":"<pre><code># Code completion\nllmbuilder generate text \\\n  --model codeassist_pro_v2.1.pt \\\n  --tokenizer tokenizer/ \\\n  --prompt \"def binary_search(arr, target):\" \\\n  --max-tokens 100 \\\n  --temperature 0.2 \\\n  --stop-tokens \"def,class\"\n\n# Interactive coding session\nllmbuilder generate text --interactive \\\n  --model codeassist_pro_v2.1.pt \\\n  --tokenizer tokenizer/ \\\n  --temperature 0.1\n</code></pre>"},{"location":"examples/model-cards/code-generation-model/#training-details","title":"Training Details","text":""},{"location":"examples/model-cards/code-generation-model/#training-process","title":"Training Process","text":"<ul> <li>Training Duration: 8 days</li> <li>Hardware Used: 8x NVIDIA A100 GPUs (80GB)</li> <li>Training Framework: LLMBuilder v1.3.0</li> <li>Distributed Training: Yes, using PyTorch FSDP</li> </ul>"},{"location":"examples/model-cards/code-generation-model/#hyperparameters","title":"Hyperparameters","text":"<pre><code>{\n  \"optimizer\": \"AdamW\",\n  \"learning_rate\": 1e-4,\n  \"weight_decay\": 0.01,\n  \"beta1\": 0.9,\n  \"beta2\": 0.999,\n  \"epsilon\": 1e-8,\n  \"gradient_clipping\": 1.0,\n  \"warmup_steps\": 5000,\n  \"lr_scheduler\": \"cosine\",\n  \"batch_size\": 16,\n  \"gradient_accumulation_steps\": 8,\n  \"effective_batch_size\": 128\n}\n</code></pre>"},{"location":"examples/model-cards/code-generation-model/#training-curves","title":"Training Curves","text":"<pre><code>Training Progress:\n- Initial Loss: 3.9\n- Final Training Loss: 1.8\n- Final Validation Loss: 1.9\n- Best Validation Loss: 1.85 (at epoch 22)\n- Code compilation rate improved from 45% to 89% during training\n</code></pre>"},{"location":"examples/model-cards/code-generation-model/#reproducibility","title":"Reproducibility","text":""},{"location":"examples/model-cards/code-generation-model/#reproducibility-information","title":"Reproducibility Information","text":"<ul> <li>Random Seed: 12345</li> <li>LLMBuilder Version: 1.3.0</li> <li>PyTorch Version: 2.1.0</li> <li>CUDA Version: 11.8</li> <li>Training Configuration: Available at <code>config/codeassist_config.json</code></li> </ul>"},{"location":"examples/model-cards/code-generation-model/#reproduction-steps","title":"Reproduction Steps","text":"<ol> <li>Install LLMBuilder: <code>pip install llmbuilder==1.3.0</code></li> <li>Download training data: Access via GitHub API (see data collection script)</li> <li>Run data processing: <code>llmbuilder data load --config code_data_config.json</code></li> <li>Train tokenizer: <code>llmbuilder data tokenizer --config code_tokenizer_config.json</code></li> <li>Train model: <code>llmbuilder train model --config codeassist_config.json</code></li> </ol>"},{"location":"examples/model-cards/code-generation-model/#citation","title":"Citation","text":"<p>If you use this model in your research or development, please cite:</p> <pre><code>@misc{codeassist_pro_v2,\n  title={CodeAssist Pro: A Large Language Model for Code Generation and Completion},\n  author={DevTools AI Research Team},\n  year={2024},\n  url={https://github.com/devtools/codeassist-pro},\n  note={Trained using LLMBuilder}\n}\n</code></pre>"},{"location":"examples/model-cards/code-generation-model/#changelog","title":"Changelog","text":""},{"location":"examples/model-cards/code-generation-model/#version-history","title":"Version History","text":"<ul> <li>v2.1.0 (April 22, 2024): Major update</li> <li>Increased model size to 350M parameters</li> <li>Added support for 5 additional programming languages</li> <li>Improved code completion accuracy by 15%</li> <li> <p>Enhanced security vulnerability detection</p> </li> <li> <p>v2.0.0 (February 10, 2024): Architecture update</p> </li> <li>Switched to RoPE position encoding</li> <li>Extended context length to 4096 tokens</li> <li> <p>Added code-specific special tokens</p> </li> <li> <p>v1.0.0 (November 5, 2023): Initial release</p> </li> <li>125M parameter model</li> <li>Support for Python, JavaScript, Java</li> <li>Basic code completion functionality</li> </ul>"},{"location":"examples/model-cards/code-generation-model/#contact-and-support","title":"Contact and Support","text":"<ul> <li>Primary Contact: Dr. Alex Chen (alex.chen@devtools.com)</li> <li>Organization: DevTools AI Research Division</li> <li>Repository: https://github.com/devtools/codeassist-pro</li> <li>Issues: https://github.com/devtools/codeassist-pro/issues</li> <li>Documentation: https://docs.devtools.com/codeassist-pro</li> <li>Community: https://discord.gg/devtools-ai</li> </ul> <p>This model card was created using the LLMBuilder Model Card Template v1.0</p>"},{"location":"examples/model-cards/text-generation-model/","title":"Model Card: Creative Writing Assistant v1.0","text":""},{"location":"examples/model-cards/text-generation-model/#model-details","title":"Model Details","text":""},{"location":"examples/model-cards/text-generation-model/#basic-information","title":"Basic Information","text":"<ul> <li>Model Name: Creative Writing Assistant</li> <li>Model Version: v1.0.0</li> <li>Model Type: GPT-style Transformer (Decoder-only)</li> <li>Architecture: Transformer with 12 layers, 768 hidden dimensions</li> <li>Training Framework: LLMBuilder v1.2.0</li> <li>Model Size: 125M parameters</li> <li>License: MIT License</li> <li>Contact: research@example.com</li> <li>Date: March 15, 2024</li> </ul>"},{"location":"examples/model-cards/text-generation-model/#model-architecture","title":"Model Architecture","text":"<pre><code>Model Configuration:\n- Vocabulary Size: 32,000 tokens\n- Hidden Dimensions: 768\n- Number of Layers: 12\n- Number of Attention Heads: 12\n- Maximum Sequence Length: 2048 tokens\n- Activation Function: GELU\n- Dropout Rate: 0.1\n- Layer Normalization: Pre-norm\n</code></pre>"},{"location":"examples/model-cards/text-generation-model/#training-configuration","title":"Training Configuration","text":"<pre><code>{\n  \"model\": {\n    \"vocab_size\": 32000,\n    \"num_layers\": 12,\n    \"num_heads\": 12,\n    \"embedding_dim\": 768,\n    \"max_seq_length\": 2048,\n    \"dropout\": 0.1\n  },\n  \"training\": {\n    \"batch_size\": 32,\n    \"learning_rate\": 3e-4,\n    \"num_epochs\": 15,\n    \"optimizer\": \"AdamW\",\n    \"weight_decay\": 0.01,\n    \"warmup_steps\": 2000\n  },\n  \"tokenizer\": {\n    \"algorithm\": \"sentencepiece\",\n    \"vocab_size\": 32000,\n    \"special_tokens\": [\"&lt;pad&gt;\", \"&lt;unk&gt;\", \"&lt;s&gt;\", \"&lt;/s&gt;\", \"&lt;mask&gt;\"]\n  }\n}\n</code></pre>"},{"location":"examples/model-cards/text-generation-model/#intended-use","title":"Intended Use","text":""},{"location":"examples/model-cards/text-generation-model/#primary-use-cases","title":"Primary Use Cases","text":"<ul> <li>Creative writing assistance and story generation</li> <li>Poetry and creative text composition</li> <li>Writing prompt expansion and development</li> <li>Character dialogue generation</li> <li>Narrative structure assistance</li> </ul>"},{"location":"examples/model-cards/text-generation-model/#intended-users","title":"Intended Users","text":"<ul> <li>Creative writers and authors</li> <li>Content creators and bloggers</li> <li>Writing educators and students</li> <li>Game developers creating narrative content</li> <li>Screenwriters and storytellers</li> </ul>"},{"location":"examples/model-cards/text-generation-model/#out-of-scope-uses","title":"Out-of-Scope Uses","text":"<ul> <li>Medical, legal, or financial advice</li> <li>Factual information retrieval (model may hallucinate)</li> <li>Academic research or citation generation</li> <li>Real-time news or current events</li> <li>Content requiring high factual accuracy</li> </ul>"},{"location":"examples/model-cards/text-generation-model/#training-data","title":"Training Data","text":""},{"location":"examples/model-cards/text-generation-model/#data-sources","title":"Data Sources","text":"<ul> <li>Primary Dataset: Curated Creative Writing Corpus</li> <li>Size: 25GB of text, approximately 6 billion tokens</li> <li>Languages: English</li> <li>Time Period: Literature and creative works from 1900-2023</li> <li>Domains: Fiction books, short stories, poetry, creative writing samples, screenplays</li> </ul>"},{"location":"examples/model-cards/text-generation-model/#data-processing","title":"Data Processing","text":"<ul> <li>Ingestion: Multi-format processing using LLMBuilder</li> <li>Formats processed: EPUB, PDF, HTML, Markdown, plain text</li> <li>OCR used for: Scanned literary works and manuscripts</li> <li> <p>Metadata extraction: Yes (author, genre, publication date)</p> </li> <li> <p>Deduplication: Advanced deduplication pipeline</p> </li> <li>Exact deduplication: Yes</li> <li>Semantic deduplication: Yes (threshold: 0.85)</li> <li>Similarity metric: Cosine similarity with sentence-transformers</li> <li> <p>Duplicates removed: 12% of original data</p> </li> <li> <p>Filtering and Cleaning: Comprehensive quality filtering</p> </li> <li>Language detection and English-only filtering</li> <li>Quality filtering based on literary merit</li> <li>Content filtering for inappropriate material</li> <li>Length filtering (minimum 100 characters, maximum 50,000 characters per document)</li> </ul>"},{"location":"examples/model-cards/text-generation-model/#data-quality","title":"Data Quality","text":"<ul> <li>Quality Metrics: High-quality literary works with professional editing</li> <li>Known Issues: Some older texts may contain outdated language or perspectives</li> <li>Bias Considerations: Potential bias toward Western literature and established authors</li> </ul>"},{"location":"examples/model-cards/text-generation-model/#performance","title":"Performance","text":""},{"location":"examples/model-cards/text-generation-model/#evaluation-metrics","title":"Evaluation Metrics","text":"Metric Value Dataset Notes Perplexity 18.4 Creative Writing Validation Set Lower is better BLEU Score 0.38 Story Continuation Task For generation quality Human Evaluation 7.2/10 Creative Quality Assessment Rated by writers Coherence Score 0.82 Narrative Coherence Test Automated evaluation"},{"location":"examples/model-cards/text-generation-model/#benchmark-results","title":"Benchmark Results","text":"<pre><code>Evaluation on Creative Writing Benchmarks:\n- WritingPrompts: 7.1/10 (human evaluation)\n- Story Coherence: 0.78 (automated metric)\n- Character Consistency: 0.71 (automated metric)\n- Genre Adherence: 0.85 (classification accuracy)\n</code></pre>"},{"location":"examples/model-cards/text-generation-model/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>Inference Speed: 45 tokens/second on RTX 4090</li> <li>Memory Requirements: 6GB VRAM for inference, 2GB for GGUF Q4_0</li> <li>Quantization Support: GGUF Q8_0, Q4_0, Q4_1 available</li> <li>Hardware Requirements: </li> <li>Minimum: 8GB RAM, modern CPU</li> <li>Recommended: 16GB RAM, RTX 3060 or better</li> </ul>"},{"location":"examples/model-cards/text-generation-model/#limitations-and-risks","title":"Limitations and Risks","text":""},{"location":"examples/model-cards/text-generation-model/#known-limitations","title":"Known Limitations","text":"<ul> <li>Limited context window of 2048 tokens may truncate longer narratives</li> <li>May generate repetitive phrases in very long sequences</li> <li>Performance varies significantly across different creative genres</li> <li>Limited knowledge of events after training data cutoff</li> <li>May struggle with highly technical or specialized creative writing</li> </ul>"},{"location":"examples/model-cards/text-generation-model/#potential-risks","title":"Potential Risks","text":"<ul> <li>Bias and Fairness: May reflect biases present in literary canon</li> <li>Harmful Content: Potential to generate inappropriate or offensive content</li> <li>Misinformation: May generate fictional \"facts\" that seem plausible</li> <li>Copyright: May inadvertently reproduce copyrighted text patterns</li> </ul>"},{"location":"examples/model-cards/text-generation-model/#mitigation-strategies","title":"Mitigation Strategies","text":"<ul> <li>Content filtering during training to remove harmful material</li> <li>Recommended human oversight for all generated content</li> <li>Clear labeling as AI-generated content</li> <li>Regular bias auditing and evaluation</li> </ul>"},{"location":"examples/model-cards/text-generation-model/#ethical-considerations","title":"Ethical Considerations","text":""},{"location":"examples/model-cards/text-generation-model/#responsible-use","title":"Responsible Use","text":"<ul> <li>Always disclose AI assistance in creative works</li> <li>Use human judgment to review and edit generated content</li> <li>Respect copyright and intellectual property rights</li> <li>Consider the impact on human creative professionals</li> </ul>"},{"location":"examples/model-cards/text-generation-model/#environmental-impact","title":"Environmental Impact","text":"<ul> <li>Training Compute: 150 GPU-hours on A100 GPUs</li> <li>Carbon Footprint: Approximately 45 kg CO2 equivalent</li> <li>Energy Consumption: 2,400 kWh total training energy</li> </ul>"},{"location":"examples/model-cards/text-generation-model/#technical-specifications","title":"Technical Specifications","text":""},{"location":"examples/model-cards/text-generation-model/#model-files","title":"Model Files","text":"<ul> <li>Model Weights: <code>creative_writer_v1.pt</code> (500MB)</li> <li>Configuration: <code>config.json</code></li> <li>Tokenizer: <code>tokenizer/</code> directory (15MB)</li> <li>GGUF Versions: </li> <li><code>creative_writer_q8_0.gguf</code> (485MB) - High quality</li> <li><code>creative_writer_q4_0.gguf</code> (245MB) - Balanced</li> <li><code>creative_writer_q4_1.gguf</code> (270MB) - Compact</li> </ul>"},{"location":"examples/model-cards/text-generation-model/#dependencies","title":"Dependencies","text":"<pre><code>Required:\n- Python &gt;= 3.8\n- PyTorch &gt;= 1.12.0\n- LLMBuilder &gt;= 1.2.0\n\nOptional:\n- CUDA &gt;= 11.0 (for GPU inference)\n- llama.cpp (for GGUF inference)\n</code></pre>"},{"location":"examples/model-cards/text-generation-model/#usage-example","title":"Usage Example","text":"<pre><code>import llmbuilder as lb\n\n# Load the model\nmodel = lb.load_model(\"creative_writer_v1.pt\")\ntokenizer = lb.load_tokenizer(\"tokenizer/\")\n\n# Generate creative text\nstory = lb.generate_text(\n    model=model,\n    tokenizer=tokenizer,\n    prompt=\"In a world where dreams become reality, Sarah discovered that her nightmares\",\n    max_new_tokens=200,\n    temperature=0.8,\n    top_p=0.9,\n    repetition_penalty=1.1\n)\nprint(story)\n</code></pre>"},{"location":"examples/model-cards/text-generation-model/#cli-usage","title":"CLI Usage","text":"<pre><code># Generate creative text\nllmbuilder generate text \\\n  --model creative_writer_v1.pt \\\n  --tokenizer tokenizer/ \\\n  --prompt \"Once upon a time in a distant galaxy\" \\\n  --max-tokens 150 \\\n  --temperature 0.8 \\\n  --top-p 0.9\n\n# Interactive creative writing session\nllmbuilder generate text --interactive \\\n  --model creative_writer_v1.pt \\\n  --tokenizer tokenizer/ \\\n  --temperature 0.8\n</code></pre>"},{"location":"examples/model-cards/text-generation-model/#training-details","title":"Training Details","text":""},{"location":"examples/model-cards/text-generation-model/#training-process","title":"Training Process","text":"<ul> <li>Training Duration: 3 days</li> <li>Hardware Used: 4x NVIDIA A100 GPUs (40GB)</li> <li>Training Framework: LLMBuilder v1.2.0</li> <li>Distributed Training: Yes, using PyTorch DDP</li> </ul>"},{"location":"examples/model-cards/text-generation-model/#hyperparameters","title":"Hyperparameters","text":"<pre><code>{\n  \"optimizer\": \"AdamW\",\n  \"learning_rate\": 3e-4,\n  \"weight_decay\": 0.01,\n  \"beta1\": 0.9,\n  \"beta2\": 0.999,\n  \"epsilon\": 1e-8,\n  \"gradient_clipping\": 1.0,\n  \"warmup_steps\": 2000,\n  \"lr_scheduler\": \"cosine\",\n  \"batch_size\": 32,\n  \"gradient_accumulation_steps\": 2,\n  \"effective_batch_size\": 64\n}\n</code></pre>"},{"location":"examples/model-cards/text-generation-model/#training-curves","title":"Training Curves","text":"<pre><code>Training Progress:\n- Initial Loss: 4.8\n- Final Training Loss: 2.2\n- Final Validation Loss: 2.4\n- Best Validation Loss: 2.3 (at epoch 12)\n- Training completed without overfitting\n</code></pre>"},{"location":"examples/model-cards/text-generation-model/#reproducibility","title":"Reproducibility","text":""},{"location":"examples/model-cards/text-generation-model/#reproducibility-information","title":"Reproducibility Information","text":"<ul> <li>Random Seed: 42</li> <li>LLMBuilder Version: 1.2.0</li> <li>PyTorch Version: 2.0.1</li> <li>CUDA Version: 11.8</li> <li>Training Configuration: Available at <code>config/creative_writer_config.json</code></li> </ul>"},{"location":"examples/model-cards/text-generation-model/#reproduction-steps","title":"Reproduction Steps","text":"<ol> <li>Install LLMBuilder: <code>pip install llmbuilder==1.2.0</code></li> <li>Download training data: Contact research@example.com for access</li> <li>Run data processing: <code>llmbuilder data load --config data_config.json</code></li> <li>Train tokenizer: <code>llmbuilder data tokenizer --config tokenizer_config.json</code></li> <li>Train model: <code>llmbuilder train model --config creative_writer_config.json</code></li> </ol>"},{"location":"examples/model-cards/text-generation-model/#citation","title":"Citation","text":"<p>If you use this model in your research or creative work, please cite:</p> <pre><code>@misc{creative_writer_v1,\n  title={Creative Writing Assistant: A Language Model for Creative Text Generation},\n  author={Example Research Team},\n  year={2024},\n  url={https://github.com/example/creative-writer-model},\n  note={Trained using LLMBuilder}\n}\n</code></pre>"},{"location":"examples/model-cards/text-generation-model/#changelog","title":"Changelog","text":""},{"location":"examples/model-cards/text-generation-model/#version-history","title":"Version History","text":"<ul> <li>v1.0.0 (March 15, 2024): Initial release</li> <li>125M parameter model trained on curated creative writing corpus</li> <li>Support for story generation, poetry, and creative writing assistance</li> <li>Available in PyTorch and GGUF formats</li> </ul>"},{"location":"examples/model-cards/text-generation-model/#contact-and-support","title":"Contact and Support","text":"<ul> <li>Primary Contact: Dr. Jane Smith (jane.smith@example.com)</li> <li>Organization: Example AI Research Lab</li> <li>Repository: https://github.com/example/creative-writer-model</li> <li>Issues: https://github.com/example/creative-writer-model/issues</li> <li>Documentation: https://example.com/creative-writer-docs</li> </ul> <p>This model card was created using the LLMBuilder Model Card Template v1.0</p>"},{"location":"getting-started/CLI/","title":"LLMBuilder CLI Guide","text":"<p>This document explains how to use the LLMBuilder command-line interface. It lists all available commands, their inputs/outputs, and Windows-friendly examples.</p> <ul> <li>Executable: <code>llmbuilder</code></li> <li>Show help: <code>llmbuilder --help</code></li> <li>Version: <code>llmbuilder --version</code></li> </ul> <p>Tip for Windows: wrap prompts in double quotes, and use full paths when in doubt.</p>"},{"location":"getting-started/CLI/#global","title":"Global","text":"<ul> <li><code>llmbuilder --help</code> \u2014 Show top-level help.</li> <li><code>llmbuilder --version</code> \u2014 Show package version.</li> <li><code>llmbuilder -v</code> \u2014 Enable verbose mode for top-level command (shows extra output in some subcommands).</li> <li><code>llmbuilder welcome</code> \u2014 Friendly welcome and quick actions.</li> <li><code>llmbuilder info</code> \u2014 Display package information and module overview.</li> </ul> <p>Examples:</p> <ul> <li><code>llmbuilder info</code></li> <li><code>llmbuilder welcome</code></li> </ul>"},{"location":"getting-started/CLI/#data-commands","title":"Data Commands","text":"<p>Group: <code>llmbuilder data</code></p>"},{"location":"getting-started/CLI/#1-load-and-preprocess-text","title":"1) Load and preprocess text","text":"<p><code>llmbuilder data load</code> \u2014 Load text from files/directories, optionally clean, and save to a single file.</p> <p>Inputs:</p> <ul> <li><code>--input, -i</code> (file or directory)</li> <li><code>--output, -o</code> (output .txt file)</li> <li><code>--format</code> [txt|pdf|docx|all] (default: all)</li> <li><code>--clean</code> (flag to clean text)</li> <li><code>--min-length</code> (minimum text length to keep; default: 50)</li> <li><code>--interactive</code> (guided mode)</li> </ul> <p>Outputs:</p> <ul> <li>A single text file with processed content; progress messages.</li> </ul> <p>Examples:</p> <ul> <li>Directory to single file:   <code>llmbuilder data load -i D:\\data -o D:\\out\\combined.txt --clean --min-length 80</code></li> <li>Single file:   <code>llmbuilder data load -i D:\\docs\\paper.pdf -o D:\\out\\paper.txt</code></li> <li>Interactive:   <code>llmbuilder data load --interactive</code></li> </ul>"},{"location":"getting-started/CLI/#2-ingest-multi-format-documents","title":"2) Ingest multi-format documents","text":"<p><code>llmbuilder data ingest</code> \u2014 Batch-process multiple formats with optional OCR fallback.</p> <p>Inputs:</p> <ul> <li><code>--input, -i</code> (file or directory)</li> <li><code>--output, -o</code> (output directory)</li> <li><code>--formats</code> (multiple) [html|markdown|epub|pdf|all] (default: all)</li> <li><code>--batch-size</code> (default: 100)</li> <li><code>--workers</code> (default: 4)</li> <li><code>--ocr-fallback</code> (flag for PDFs)</li> <li><code>--verbose, -v</code></li> </ul> <p>Outputs:</p> <ul> <li>Processed files written into the output directory and a summary (processed/succeeded/failed).</li> </ul> <p>Example:</p> <ul> <li><code>llmbuilder data ingest -i D:\\raw_docs -o D:\\processed --formats html --formats pdf --ocr-fallback -v</code></li> </ul>"},{"location":"getting-started/CLI/#3-deduplicate-text","title":"3) Deduplicate text","text":"<p><code>llmbuilder data deduplicate</code> \u2014 Remove exact/semantic duplicates from text.</p> <p>Inputs:</p> <ul> <li><code>--input, -i</code> (file or directory containing .txt files)</li> <li><code>--output, -o</code> (output file for deduplicated lines)</li> <li><code>--method</code> [exact|semantic|both] (default: both)</li> <li><code>--similarity-threshold</code> (0.0\u20131.0, default: 0.85)</li> <li><code>--batch-size</code> (default: 1000)</li> <li><code>--embedding-model</code> (default: all-MiniLM-L6-v2)</li> <li><code>--verbose, -v</code></li> </ul> <p>Outputs:</p> <ul> <li>Deduplicated text file, with stats printed.</li> </ul> <p>Example:</p> <ul> <li><code>llmbuilder data deduplicate -i D:\\text_corpus -o D:\\out\\dedup.txt --method both --similarity-threshold 0.9 -v</code></li> </ul>"},{"location":"getting-started/CLI/#tokenizer-commands","title":"Tokenizer Commands","text":"<p>Group: <code>llmbuilder tokenizer</code></p>"},{"location":"getting-started/CLI/#1-train-tokenizer","title":"1) Train tokenizer","text":"<p><code>llmbuilder tokenizer train</code></p> <p>Inputs:</p> <ul> <li><code>--input, -i</code> (text file or directory)</li> <li><code>--output, -o</code> (output directory)</li> <li><code>--vocab-size</code> (default: 16000)</li> <li><code>--algorithm</code> [bpe|unigram|wordpiece|sentencepiece] (default: bpe)</li> <li><code>--special-tokens</code> (multiple) e.g. <code>&lt;pad&gt;</code> <code>&lt;unk&gt;</code></li> <li><code>--min-frequency</code> (default: 2)</li> <li><code>--coverage</code> (SentencePiece only, default: 0.9995)</li> <li><code>--validate</code> (flag)</li> <li><code>--verbose, -v</code></li> </ul> <p>Outputs:</p> <ul> <li>Tokenizer model and vocab files saved under the output directory with training stats.</li> </ul> <p>Examples:</p> <ul> <li>BPE:   <code>llmbuilder tokenizer train -i D:\\out\\combined.txt -o D:\\out\\tokenizer --vocab-size 8000 --algorithm bpe --validate</code></li> <li>SentencePiece:   <code>llmbuilder tokenizer train -i D:\\out\\combined.txt -o D:\\out\\sp_tokenizer --algorithm sentencepiece --coverage 0.999</code></li> </ul>"},{"location":"getting-started/CLI/#2-test-tokenizer","title":"2) Test tokenizer","text":"<p><code>llmbuilder tokenizer test</code></p> <p>Inputs:</p> <ul> <li><code>tokenizer_path</code> (positional; directory containing tokenizer)</li> <li>One of:</li> <li><code>--text, -t</code> (quick test string)</li> <li><code>--file, -f</code> (file to tokenize)</li> <li><code>--interactive, -i</code> (interactive loop)</li> </ul> <p>Outputs:</p> <ul> <li>Encoded tokens, decoded text, and token counts.</li> </ul> <p>Examples:</p> <ul> <li>Single text:   <code>llmbuilder tokenizer test D:\\out\\tokenizer -t \"Hello world\"</code></li> <li>Interactive:   <code>llmbuilder tokenizer test D:\\out\\tokenizer -i</code></li> </ul>"},{"location":"getting-started/CLI/#training-commands","title":"Training Commands","text":"<p>Group: <code>llmbuilder train</code></p>"},{"location":"getting-started/CLI/#1-train-a-model-from-scratch","title":"1) Train a model from scratch","text":"<p><code>llmbuilder train model</code></p> <p>Inputs:</p> <ul> <li><code>--data, -d</code> (path to tokenized dataset or compatible data file)</li> <li><code>--tokenizer, -t</code> (tokenizer directory)</li> <li><code>--output, -o</code> (checkpoint output directory)</li> <li><code>--epochs</code> (default: 10)</li> <li><code>--batch-size</code> (default: 32)</li> <li><code>--lr</code> (default: 3e-4)</li> <li><code>--vocab-size</code> (default: 16000)</li> <li><code>--layers</code> (default: 8)</li> <li><code>--heads</code> (default: 8)</li> <li><code>--dim</code> (embedding dim; default: 512)</li> <li><code>--config, -c</code> (optional config file, if supported)</li> </ul> <p>Outputs:</p> <ul> <li>Training progress, checkpoints in output directory.</li> </ul> <p>Example:</p> <ul> <li><code>llmbuilder train model -d D:\\LLM\\Model_Test\\output\\tokenized_data.pt -t D:\\LLM\\Model_Test\\output\\tokenizer -o D:\\LLM\\Model_Test\\output\\checkpoints --epochs 5 --batch-size 1 --lr 6e-4 --vocab-size 8000 --layers 4 --heads 8 --dim 256</code></li> </ul>"},{"location":"getting-started/CLI/#2-resume-training","title":"2) Resume training","text":"<p><code>llmbuilder train resume</code></p> <p>Inputs:</p> <ul> <li><code>--checkpoint, -c</code> (path to existing checkpoint)</li> <li><code>--data, -d</code> (dataset path)</li> <li><code>--output, -o</code> (optional; defaults to checkpoint directory)</li> </ul> <p>Outputs:</p> <ul> <li>Training resumed; new checkpoints written.</li> </ul> <p>Example:</p> <ul> <li><code>llmbuilder train resume -c D:\\LLM\\Model_Test\\output\\checkpoints\\checkpoint_epoch_2.pt -d D:\\LLM\\Model_Test\\output\\tokenized_data.pt</code></li> </ul>"},{"location":"getting-started/CLI/#fine-tuning-commands","title":"Fine-tuning Commands","text":"<p>Group: <code>llmbuilder finetune</code></p>"},{"location":"getting-started/CLI/#fine-tune-a-pre-trained-model","title":"Fine-tune a pre-trained model","text":"<p><code>llmbuilder finetune model</code></p> <p>Inputs:</p> <ul> <li><code>--model, -m</code> (path to pre-trained model)</li> <li><code>--dataset, -d</code> (dataset path)</li> <li><code>--output, -o</code> (output directory)</li> <li><code>--epochs</code> (default: 3)</li> <li><code>--lr</code> (default: 5e-5)</li> <li><code>--batch-size</code> (default: 4)</li> <li><code>--use-lora</code> (flag)</li> <li><code>--lora-rank</code> (default: 4)</li> </ul> <p>Outputs:</p> <ul> <li>Fine-tuned model checkpoints and summary (best validation loss etc.).</li> </ul> <p>Example:</p> <ul> <li><code>llmbuilder finetune model -m D:\\models\\base.pt -d D:\\data\\fine_tune.pt -o D:\\out\\ft --epochs 3 --use-lora</code></li> </ul>"},{"location":"getting-started/CLI/#generation-commands","title":"Generation Commands","text":"<p>Group: <code>llmbuilder generate</code></p>"},{"location":"getting-started/CLI/#generate-text","title":"Generate text","text":"<p><code>llmbuilder generate text</code></p> <p>Inputs:</p> <ul> <li><code>--model, -m</code> (checkpoint path). Use <code>latest_checkpoint.pt</code> or a specific <code>checkpoint_epoch_X.pt</code>.</li> <li><code>--tokenizer, -t</code> (tokenizer directory)</li> <li><code>--prompt, -p</code> (text prompt)</li> <li><code>--interactive, -i</code> (interactive chat-like mode)</li> <li><code>--max-tokens</code> (default: 100)</li> <li><code>--temperature</code> (default: 0.8)</li> <li><code>--top-k</code> (default: 50)</li> <li><code>--top-p</code> (default: 0.9)</li> <li><code>--device</code> (cpu|cuda; optional)</li> <li><code>--setup</code> (guided setup)</li> </ul> <p>Outputs:</p> <ul> <li>Generated text printed to console; in interactive mode, a live session.</li> </ul> <p>Examples:</p> <ul> <li>One-shot:   <code>llmbuilder generate text -m D:\\LLM\\Model_Test\\output\\checkpoints\\latest_checkpoint.pt -t D:\\LLM\\Model_Test\\output\\tokenizer -p \"Cybersecurity is important because\" --max-tokens 120 --temperature 0.8 --top-k 50 --top-p 0.9</code></li> <li>Interactive:   <code>llmbuilder generate text -m D:\\LLM\\Model_Test\\output\\checkpoints\\latest_checkpoint.pt -t D:\\LLM\\Model_Test\\output\\tokenizer --interactive</code></li> <li>Guided setup:   <code>llmbuilder generate text --setup</code></li> </ul> <p>Notes:</p> <ul> <li>If <code>best_checkpoint.pt</code> is missing, use <code>latest_checkpoint.pt</code>.</li> <li>Ensure your tokenizer matches the model used for training.</li> </ul>"},{"location":"getting-started/CLI/#model-management-commands","title":"Model Management Commands","text":"<p>Group: <code>llmbuilder model</code></p>"},{"location":"getting-started/CLI/#1-create-a-new-empty-model","title":"1) Create a new empty model","text":"<p><code>llmbuilder model create</code></p> <p>Inputs:</p> <ul> <li><code>--output, -o</code> (path to save)</li> <li><code>--vocab-size</code> (default: 16000)</li> <li><code>--layers</code> (default: 8)</li> <li><code>--heads</code> (default: 8)</li> <li><code>--dim</code> (default: 512)</li> <li><code>--config, -c</code> (optional)</li> </ul> <p>Outputs:</p> <ul> <li>Saved model file and parameter counts printed.</li> </ul> <p>Example:</p> <ul> <li><code>llmbuilder model create -o D:\\models\\new.pt --vocab-size 8000 --layers 4 --heads 8 --dim 256</code></li> </ul>"},{"location":"getting-started/CLI/#2-show-model-info","title":"2) Show model info","text":"<p><code>llmbuilder model info &lt;model_path&gt;</code></p> <p>Outputs:</p> <ul> <li>Total/trainable parameters and architecture details (if available).</li> </ul> <p>Example:</p> <ul> <li><code>llmbuilder model info D:\\LLM\\Model_Test\\output\\checkpoints\\latest_checkpoint.pt</code></li> </ul>"},{"location":"getting-started/CLI/#3-evaluate-a-model","title":"3) Evaluate a model","text":"<p><code>llmbuilder model evaluate</code></p> <p>Inputs:</p> <ul> <li><code>--dataset, -d</code> (evaluation dataset path)</li> <li><code>--batch-size</code> (default: 32)</li> </ul> <p>Outputs:</p> <ul> <li>Evaluation metrics (loss, perplexity if available).</li> </ul> <p>Example:</p> <ul> <li><code>llmbuilder model evaluate D:\\LLM\\Model_Test\\output\\checkpoints\\latest_checkpoint.pt -d D:\\LLM\\Model_Test\\output\\tokenized_data.pt</code></li> </ul>"},{"location":"getting-started/CLI/#export-commands","title":"Export Commands","text":"<p>Group: <code>llmbuilder export</code></p>"},{"location":"getting-started/CLI/#export-to-gguf-llamacpp-compatibility","title":"Export to GGUF (llama.cpp compatibility)","text":"<p><code>llmbuilder export gguf</code></p> <p>Inputs:</p> <ul> <li><code>model_path</code> (positional; path to source model/checkpoint)</li> <li><code>--output, -o</code> (output GGUF file path)</li> <li><code>--quantization</code> [Q8_0|Q4_0|Q4_1|Q5_0|Q5_1|F16|F32] (default: Q8_0)</li> <li><code>--validate</code> (flag; run post-conversion validation if available)</li> <li><code>--verbose, -v</code> (flag; print extra diagnostics and script discovery)</li> </ul> <p>Outputs:</p> <ul> <li>A <code>.gguf</code> model file written to the output path. Summary includes size, time, and quantization details. On <code>--validate</code>, prints validation result.</li> </ul> <p>Examples:</p> <ul> <li>Convert with default Q8_0:   <code>llmbuilder export gguf D:\\LLM\\Model_Test\\output\\checkpoints\\latest_checkpoint.pt -o D:\\LLM\\Model_Test\\output\\models\\latest_q8.gguf</code></li> <li>Convert with Q4_0 and validate:   <code>llmbuilder export gguf D:\\models\\my_model.pt -o D:\\models\\my_model_q4.gguf --quantization Q4_0 --validate -v</code></li> </ul> <p>Notes:</p> <ul> <li>Ensure sufficient disk space for the converted file.</li> <li>Quantization level affects size and speed/quality tradeoffs.</li> </ul>"},{"location":"getting-started/CLI/#tips-troubleshooting","title":"Tips &amp; Troubleshooting","text":"<ul> <li>No best checkpoint: If validation is disabled or failed to split, only <code>latest_checkpoint.pt</code> is produced. Use that for generation. After upgrading to a version with robust splits and retraining, <code>best_checkpoint.pt</code> will appear.</li> <li>Colors on Windows: Colors are enabled automatically via colorama. If you don\u2019t see colors, ensure <code>llmbuilder&gt;=0.4.5</code> is installed.</li> <li>Progress bars: Data loading, training, and validation show tqdm bars when console output is enabled.</li> <li>Paths: Prefer absolute paths on Windows to avoid confusion.</li> <li>Help: Every command supports <code>--help</code>.</li> </ul>"},{"location":"getting-started/CLI/#quick-start-workflow","title":"Quick Start Workflow","text":"<p>1) Prepare data</p> <pre><code>llmbuilder data load -i D:\\data -o D:\\LLM\\Model_Test\\output\\processed_data.txt --clean\n</code></pre> <p>2) Train tokenizer</p> <pre><code>llmbuilder tokenizer train -i D:\\LLM\\Model_Test\\output\\processed_data.txt -o D:\\LLM\\Model_Test\\output\\tokenizer --vocab-size 8000 --algorithm bpe --validate\n</code></pre> <p>3) Train model</p> <pre><code>llmbuilder train model -d D:\\LLM\\Model_Test\\output\\tokenized_data.pt -t D:\\LLM\\Model_Test\\output\\tokenizer -o D:\\LLM\\Model_Test\\output\\checkpoints --epochs 5 --batch-size 1 --lr 6e-4 --vocab-size 8000 --layers 4 --heads 8 --dim 256\n</code></pre> <p>4) Generate text</p> <pre><code>llmbuilder generate text -m D:\\LLM\\Model_Test\\output\\checkpoints\\latest_checkpoint.pt -t D:\\LLM\\Model_Test\\output\\tokenizer -p \"Cybersecurity is important because\" --max-tokens 120\n</code></pre>"},{"location":"getting-started/CLI/#crossplatform-quick-commands","title":"Cross\u2011platform Quick Commands","text":"<p>Below are ready-to-run snippets for Windows PowerShell and Linux/macOS Bash/Zsh.</p>"},{"location":"getting-started/CLI/#data-load","title":"Data: Load","text":"<p>PowerShell</p> <pre><code>llmbuilder data load -i \"D:\\data\" -o \"D:\\LLM\\Model_Test\\output\\processed_data.txt\" --clean --min-length 80\n</code></pre> <p>Bash/Zsh</p> <pre><code>llmbuilder data load -i \"/data\" -o \"/mnt/llm/Model_Test/output/processed_data.txt\" --clean --min-length 80\n</code></pre>"},{"location":"getting-started/CLI/#data-ingest","title":"Data: Ingest","text":"<p>PowerShell</p> <pre><code>llmbuilder data ingest -i \"D:\\raw_docs\" -o \"D:\\processed\" --formats html --formats pdf --ocr-fallback -v\n</code></pre> <p>Bash/Zsh</p> <pre><code>llmbuilder data ingest -i \"/data/raw_docs\" -o \"/data/processed\" --formats html --formats pdf --ocr-fallback -v\n</code></pre>"},{"location":"getting-started/CLI/#data-deduplicate","title":"Data: Deduplicate","text":"<p>PowerShell</p> <pre><code>llmbuilder data deduplicate -i \"D:\\text_corpus\" -o \"D:\\out\\dedup.txt\" --method both --similarity-threshold 0.9 -v\n</code></pre> <p>Bash/Zsh</p> <pre><code>llmbuilder data deduplicate -i \"/data/text_corpus\" -o \"/data/out/dedup.txt\" --method both --similarity-threshold 0.9 -v\n</code></pre>"},{"location":"getting-started/CLI/#tokenizer-train-bpe","title":"Tokenizer: Train (BPE)","text":"<p>PowerShell</p> <pre><code>llmbuilder tokenizer train -i \"D:\\LLM\\Model_Test\\output\\processed_data.txt\" -o \"D:\\LLM\\Model_Test\\output\\tokenizer\" --vocab-size 8000 --algorithm bpe --validate\n</code></pre> <p>Bash/Zsh</p> <pre><code>llmbuilder tokenizer train -i \"/mnt/llm/Model_Test/output/processed_data.txt\" -o \"/mnt/llm/Model_Test/output/tokenizer\" --vocab-size 8000 --algorithm bpe --validate\n</code></pre>"},{"location":"getting-started/CLI/#tokenizer-test-interactive","title":"Tokenizer: Test (interactive)","text":"<p>PowerShell</p> <pre><code>llmbuilder tokenizer test \"D:\\LLM\\Model_Test\\output\\tokenizer\" -i\n</code></pre> <p>Bash/Zsh</p> <pre><code>llmbuilder tokenizer test \"/mnt/llm/Model_Test/output/tokenizer\" -i\n</code></pre>"},{"location":"getting-started/CLI/#train-from-scratch","title":"Train: From scratch","text":"<p>PowerShell</p> <pre><code>llmbuilder train model -d \"D:\\LLM\\Model_Test\\output\\tokenized_data.pt\" -t \"D:\\LLM\\Model_Test\\output\\tokenizer\" -o \"D:\\LLM\\Model_Test\\output\\checkpoints\" --epochs 5 --batch-size 1 --lr 6e-4 --vocab-size 8000 --layers 4 --heads 8 --dim 256\n</code></pre> <p>Bash/Zsh</p> <pre><code>llmbuilder train model -d \"/mnt/llm/Model_Test/output/tokenized_data.pt\" -t \"/mnt/llm/Model_Test/output/tokenizer\" -o \"/mnt/llm/Model_Test/output/checkpoints\" --epochs 5 --batch-size 1 --lr 6e-4 --vocab-size 8000 --layers 4 --heads 8 --dim 256\n</code></pre>"},{"location":"getting-started/CLI/#train-resume","title":"Train: Resume","text":"<p>PowerShell</p> <pre><code>llmbuilder train resume -c \"D:\\LLM\\Model_Test\\output\\checkpoints\\checkpoint_epoch_2.pt\" -d \"D:\\LLM\\Model_Test\\output\\tokenized_data.pt\"\n</code></pre> <p>Bash/Zsh</p> <pre><code>llmbuilder train resume -c \"/mnt/llm/Model_Test/output/checkpoints/checkpoint_epoch_2.pt\" -d \"/mnt/llm/Model_Test/output/tokenized_data.pt\"\n</code></pre>"},{"location":"getting-started/CLI/#generate-one-shot","title":"Generate: One-shot","text":"<p>PowerShell</p> <pre><code>llmbuilder generate text -m \"D:\\LLM\\Model_Test\\output\\checkpoints\\latest_checkpoint.pt\" -t \"D:\\LLM\\Model_Test\\output\\tokenizer\" -p \"Cybersecurity is important because\" --max-tokens 120 --temperature 0.8 --top-k 50 --top-p 0.9\n</code></pre> <p>Bash/Zsh</p> <pre><code>llmbuilder generate text -m \"/mnt/llm/Model_Test/output/checkpoints/latest_checkpoint.pt\" -t \"/mnt/llm/Model_Test/output/tokenizer\" -p \"Cybersecurity is important because\" --max-tokens 120 --temperature 0.8 --top-k 50 --top-p 0.9\n</code></pre>"},{"location":"getting-started/CLI/#generate-interactive","title":"Generate: Interactive","text":"<p>PowerShell</p> <pre><code>llmbuilder generate text -m \"D:\\LLM\\Model_Test\\output\\checkpoints\\latest_checkpoint.pt\" -t \"D:\\LLM\\Model_Test\\output\\tokenizer\" --interactive\n</code></pre> <p>Bash/Zsh</p> <pre><code>llmbuilder generate text -m \"/mnt/llm/Model_Test/output/checkpoints/latest_checkpoint.pt\" -t \"/mnt/llm/Model_Test/output/tokenizer\" --interactive\n</code></pre>"},{"location":"getting-started/CLI/#model-create","title":"Model: Create","text":"<p>PowerShell</p> <pre><code>llmbuilder model create -o \"D:\\models\\new.pt\" --vocab-size 8000 --layers 4 --heads 8 --dim 256\n</code></pre> <p>Bash/Zsh</p> <pre><code>llmbuilder model create -o \"/models/new.pt\" --vocab-size 8000 --layers 4 --heads 8 --dim 256\n</code></pre>"},{"location":"getting-started/CLI/#model-info","title":"Model: Info","text":"<p>PowerShell</p> <pre><code>llmbuilder model info \"D:\\LLM\\Model_Test\\output\\checkpoints\\latest_checkpoint.pt\"\n</code></pre> <p>Bash/Zsh</p> <pre><code>llmbuilder model info \"/mnt/llm/Model_Test/output/checkpoints/latest_checkpoint.pt\"\n</code></pre>"},{"location":"getting-started/CLI/#model-evaluate","title":"Model: Evaluate","text":"<p>PowerShell</p> <pre><code>llmbuilder model evaluate \"D:\\LLM\\Model_Test\\output\\checkpoints\\latest_checkpoint.pt\" -d \"D:\\LLM\\Model_Test\\output\\tokenized_data.pt\" --batch-size 8\n</code></pre> <p>Bash/Zsh</p> <pre><code>llmbuilder model evaluate \"/mnt/llm/Model_Test/output/checkpoints/latest_checkpoint.pt\" -d \"/mnt/llm/Model_Test/output/tokenized_data.pt\" --batch-size 8\n</code></pre>"},{"location":"getting-started/CLI/#export-gguf-q8_0","title":"Export: GGUF (Q8_0)","text":"<p>PowerShell</p> <pre><code>llmbuilder export gguf \"D:\\LLM\\Model_Test\\output\\checkpoints\\latest_checkpoint.pt\" -o \"D:\\LLM\\Model_Test\\output\\models\\latest_q8.gguf\"\n</code></pre> <p>Bash/Zsh</p> <pre><code>llmbuilder export gguf \"/mnt/llm/Model_Test/output/checkpoints/latest_checkpoint.pt\" -o \"/mnt/llm/Model_Test/output/models/latest_q8.gguf\"\n</code></pre>"},{"location":"getting-started/CLI/#export-gguf-q4_0-validate","title":"Export: GGUF (Q4_0, validate)","text":"<p>PowerShell</p> <pre><code>llmbuilder export gguf \"D:\\models\\my_model.pt\" -o \"D:\\models\\my_model_q4.gguf\" --quantization Q4_0 --validate -v\n</code></pre> <p>Bash/Zsh</p> <pre><code>llmbuilder export gguf \"/models/my_model.pt\" -o \"/models/my_model_q4.gguf\" --quantization Q4_0 --validate -v\n</code></pre>"},{"location":"getting-started/CLI/#screenshots","title":"Screenshots","text":"<p>Add images under <code>docs/images/</code> and reference them here or in the README.</p> <ul> <li>Welcome screen: <code>![Welcome](docs/images/cli-welcome.png)</code></li> <li>Data load progress: <code>![Data Load](docs/images/cli-data-load.png)</code></li> <li>Training progress bars: <code>![Training](docs/images/cli-training.png)</code></li> <li>Validation loop: <code>![Validation](docs/images/cli-validation.png)</code></li> <li>Generate (interactive): <code>![Generate Interactive](docs/images/cli-generate-interactive.png)</code></li> <li>Export GGUF: <code>![Export GGUF](docs/images/cli-export-gguf.png)</code></li> </ul> <p>Tip (Windows): Alt+PrintScreen captures the active window; save PNG to <code>docs/images/</code>.</p> <p>Maintained by Qub\u25b3se. For more, see the repository wiki and <code>llmbuilder info</code>.</p>"},{"location":"getting-started/first-model/","title":"Your First Model","text":"<p>This comprehensive tutorial will guide you through training your first language model with LLMBuilder, from data preparation to text generation. By the end, you'll have a working model and understand the entire process.</p>"},{"location":"getting-started/first-model/#what-well-build","title":"\ud83c\udfaf What We'll Build","text":"<p>We'll create a small GPT-style language model that can:</p> <ul> <li>Generate coherent text based on prompts</li> <li>Complete sentences and paragraphs</li> <li>Demonstrate understanding of the training data</li> </ul> <p>Estimated time: 30-60 minutes Requirements: 4GB RAM, Python 3.8+</p>"},{"location":"getting-started/first-model/#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<p>Make sure you have LLMBuilder installed:</p> <pre><code>pip install llmbuilder\n</code></pre> <p>Verify the installation:</p> <pre><code>llmbuilder --version\n</code></pre>"},{"location":"getting-started/first-model/#step-1-prepare-training-data","title":"\ud83d\udcda Step 1: Prepare Training Data","text":""},{"location":"getting-started/first-model/#option-a-use-sample-data","title":"Option A: Use Sample Data","text":"<p>Create a sample dataset for testing:</p> <pre><code># create_sample_data.py\nsample_text = \"\"\"\nArtificial intelligence is a rapidly evolving field that encompasses machine learning, deep learning, and neural networks. Machine learning algorithms enable computers to learn patterns from data without explicit programming. Deep learning, a subset of machine learning, uses multi-layered neural networks to process complex information.\n\nNatural language processing is a branch of AI that focuses on the interaction between computers and human language. It enables machines to understand, interpret, and generate human language in a valuable way. Applications include chatbots, translation services, and text analysis.\n\nComputer vision is another important area of AI that enables machines to interpret and understand visual information from the world. It combines techniques from machine learning, image processing, and pattern recognition to analyze and understand images and videos.\n\nThe future of artificial intelligence holds great promise for solving complex problems in healthcare, transportation, education, and many other fields. As AI systems become more sophisticated, they will continue to transform how we work, learn, and interact with technology.\n\"\"\"\n\nwith open(\"training_data.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(sample_text)\n\nprint(\"Sample data created: training_data.txt\")\n</code></pre> <p>Run the script:</p> <pre><code>python create_sample_data.py\n</code></pre>"},{"location":"getting-started/first-model/#option-b-use-your-own-data","title":"Option B: Use Your Own Data","text":"<p>If you have your own text data:</p> <pre><code># Process various document formats\nllmbuilder data load \\\n  --input ./documents \\\n  --output training_data.txt \\\n  --format all \\\n  --clean \\\n  --min-length 100\n</code></pre> <p>This will:</p> <ul> <li>Load PDF, DOCX, TXT files from <code>./documents</code></li> <li>Clean and normalize the text</li> <li>Filter out short passages</li> <li>Save everything to <code>training_data.txt</code></li> </ul>"},{"location":"getting-started/first-model/#step-2-configure-your-model","title":"\ud83d\udd27 Step 2: Configure Your Model","text":"<p>Create a configuration file for your model:</p> <pre><code>llmbuilder config create \\\n  --preset cpu_small \\\n  --output model_config.json \\\n  --interactive\n</code></pre> <p>This will create a configuration optimized for CPU training. The interactive mode will ask you questions like:</p> <pre><code>\u2699\ufe0f LLMBuilder Configuration Creator\nChoose a preset: cpu_small\nOutput file path: model_config.json\n\ud83e\udde0 Model layers: 6\n\ud83d\udccf Embedding dimension: 384\n\ud83d\udd24 Vocabulary size: 8000\n\ud83d\udce6 Batch size: 8\n\ud83d\udcc8 Learning rate: 0.0003\n</code></pre>"},{"location":"getting-started/first-model/#understanding-the-configuration","title":"Understanding the Configuration","text":"<p>Let's examine what was created:</p> <pre><code>cat model_config.json\n</code></pre> <pre><code>{\n  \"model\": {\n    \"vocab_size\": 8000,\n    \"num_layers\": 6,\n    \"num_heads\": 6,\n    \"embedding_dim\": 384,\n    \"max_seq_length\": 512,\n    \"dropout\": 0.1\n  },\n  \"training\": {\n    \"batch_size\": 8,\n    \"num_epochs\": 10,\n    \"learning_rate\": 0.0003,\n    \"warmup_steps\": 100,\n    \"save_every\": 1000,\n    \"eval_every\": 500\n  },\n  \"system\": {\n    \"device\": \"cpu\"\n  }\n}\n</code></pre>"},{"location":"getting-started/first-model/#step-3-train-the-tokenizer","title":"\ud83d\udd24 Step 3: Train the Tokenizer","text":"<p>Before training the model, we need to create a tokenizer:</p> <pre><code>llmbuilder data tokenizer \\\n  --input training_data.txt \\\n  --output ./tokenizer \\\n  --vocab-size 8000 \\\n  --model-type bpe\n</code></pre> <p>This creates a Byte-Pair Encoding (BPE) tokenizer with 8,000 vocabulary items. You'll see output like:</p> <pre><code>\ud83d\udd24 Training BPE tokenizer with vocab size 8000...\n\ud83d\udcca Processing text data...\n\u2699\ufe0f Training tokenizer model...\n\u2705 Tokenizer training completed!\n  Model: ./tokenizer/tokenizer.model\n  Vocab: ./tokenizer/tokenizer.vocab\n  Training time: 12.3s\n</code></pre>"},{"location":"getting-started/first-model/#test-your-tokenizer","title":"Test Your Tokenizer","text":"<p>Let's verify the tokenizer works:</p> <pre><code># test_tokenizer.py\nfrom llmbuilder.tokenizer import Tokenizer\n\n# Load the tokenizer\ntokenizer = Tokenizer.from_pretrained(\"./tokenizer\")\n\n# Test encoding and decoding\ntext = \"Artificial intelligence is amazing!\"\ntokens = tokenizer.encode(text)\ndecoded = tokenizer.decode(tokens)\n\nprint(f\"Original: {text}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Decoded: {decoded}\")\nprint(f\"Vocabulary size: {len(tokenizer)}\")\n</code></pre>"},{"location":"getting-started/first-model/#step-4-train-the-model","title":"\ud83e\udde0 Step 4: Train the Model","text":"<p>Now for the main event - training your language model:</p> <pre><code>llmbuilder train model \\\n  --config model_config.json \\\n  --data training_data.txt \\\n  --tokenizer ./tokenizer \\\n  --output ./my_first_model \\\n  --verbose\n</code></pre>"},{"location":"getting-started/first-model/#what-happens-during-training","title":"What Happens During Training","text":"<p>You'll see output like this:</p> <pre><code>\ud83d\ude80 Starting model training...\n\ud83d\udcca Dataset: 1,234 tokens, 156 samples\n\ud83e\udde0 Model: 2.1M parameters\n\ud83d\udcc8 Training configuration:\n  \u2022 Epochs: 10\n  \u2022 Batch size: 8\n  \u2022 Learning rate: 0.0003\n  \u2022 Device: cpu\n\nEpoch 1/10:\n  Step 10/20: loss=4.23, lr=0.00015, time=2.1s\n  Step 20/20: loss=3.87, lr=0.00030, time=4.2s\n  Validation: loss=3.92, perplexity=50.4\n\nEpoch 2/10:\n  Step 10/20: loss=3.45, lr=0.00030, time=2.0s\n  Step 20/20: loss=3.21, lr=0.00030, time=4.1s\n  Validation: loss=3.28, perplexity=26.7\n\n...\n\n\u2705 Training completed successfully!\n\ud83d\udcca Final Results:\n  \u2022 Training Loss: 2.45\n  \u2022 Validation Loss: 2.52\n  \u2022 Training Time: 8m 34s\n  \u2022 Model saved to: ./my_first_model/model.pt\n</code></pre>"},{"location":"getting-started/first-model/#understanding-training-metrics","title":"Understanding Training Metrics","text":"<ul> <li>Loss: How well the model predicts the next token (lower is better)</li> <li>Perplexity: Model confidence (lower means more confident)</li> <li>Learning Rate: How fast the model learns (adjusted automatically)</li> </ul>"},{"location":"getting-started/first-model/#step-5-generate-text","title":"\ud83c\udfaf Step 5: Generate Text","text":"<p>Time to test your model! Let's generate some text:</p> <pre><code>llmbuilder generate text \\\n  --model ./my_first_model/model.pt \\\n  --tokenizer ./tokenizer \\\n  --prompt \"Artificial intelligence\" \\\n  --max-tokens 100 \\\n  --temperature 0.8\n</code></pre> <p>You should see output like:</p> <pre><code>\ud83d\udcad Prompt: Artificial intelligence\n\ud83e\udd14 Generating...\n\n\ud83c\udfaf Generated Text:\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nArtificial intelligence is a rapidly evolving field that encompasses machine learning and deep learning technologies. These systems can process vast amounts of data to identify patterns and make predictions. Natural language processing enables computers to understand and generate human language, while computer vision allows machines to interpret visual information.\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ud83d\udcca Settings: temp=0.8, max_tokens=100\n</code></pre>"},{"location":"getting-started/first-model/#interactive-generation","title":"Interactive Generation","text":"<p>For a more interactive experience:</p> <pre><code>llmbuilder generate text \\\n  --model ./my_first_model/model.pt \\\n  --tokenizer ./tokenizer \\\n  --interactive\n</code></pre> <p>This starts an interactive session where you can try different prompts:</p> <pre><code>\ud83c\udfae Interactive Text Generation\n\ud83d\udca1 Type your prompts and watch the AI respond!\n\n&gt; Prompt: Machine learning is\nGenerated: Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed...\n\n&gt; Prompt: The future of AI\nGenerated: The future of AI holds tremendous potential for transforming industries and solving complex global challenges...\n\n&gt; Prompt: /quit\nGoodbye! \ud83d\udc4b\n</code></pre>"},{"location":"getting-started/first-model/#step-6-evaluate-your-model","title":"\ud83d\udcca Step 6: Evaluate Your Model","text":"<p>Let's assess how well your model performed:</p> <pre><code>llmbuilder model evaluate \\\n  ./my_first_model/model.pt \\\n  --dataset training_data.txt \\\n  --batch-size 16\n</code></pre> <p>This will output metrics like:</p> <pre><code>\ud83d\udcca Model Evaluation Results:\n  \u2022 Perplexity: 15.2 (lower is better)\n  \u2022 Loss: 2.72\n  \u2022 Tokens per second: 1,234\n  \u2022 Memory usage: 1.2GB\n  \u2022 Model size: 8.4MB\n</code></pre>"},{"location":"getting-started/first-model/#model-information","title":"Model Information","text":"<p>Get detailed information about your model:</p> <pre><code>llmbuilder model info ./my_first_model/model.pt\n</code></pre> <pre><code>\u2705 Model loaded successfully\n  Total parameters: 2,123,456\n  Trainable parameters: 2,123,456\n  Architecture: 6 layers, 6 heads\n  Embedding dim: 384\n  Vocab size: 8,000\n  Max sequence length: 512\n</code></pre>"},{"location":"getting-started/first-model/#step-7-experiment-and-improve","title":"\ud83d\ude80 Step 7: Experiment and Improve","text":"<p>Now that you have a working model, try these experiments:</p>"},{"location":"getting-started/first-model/#experiment-1-different-generation-parameters","title":"Experiment 1: Different Generation Parameters","text":"<pre><code># experiment_generation.py\nimport llmbuilder as lb\n\nmodel_path = \"./my_first_model/model.pt\"\ntokenizer_path = \"./tokenizer\"\nprompt = \"The future of technology\"\n\n# Conservative generation (more predictable)\nconservative = lb.generate_text(\n    model_path, tokenizer_path, prompt,\n    temperature=0.3, top_k=10, max_new_tokens=50\n)\n\n# Creative generation (more diverse)\ncreative = lb.generate_text(\n    model_path, tokenizer_path, prompt,\n    temperature=1.2, top_k=100, max_new_tokens=50\n)\n\nprint(\"Conservative:\", conservative)\nprint(\"Creative:\", creative)\n</code></pre>"},{"location":"getting-started/first-model/#experiment-2-fine-tuning","title":"Experiment 2: Fine-tuning","text":"<p>If you have domain-specific data, try fine-tuning:</p> <pre><code># Create domain-specific data\necho \"Your domain-specific text here...\" &gt; domain_data.txt\n\n# Fine-tune the model\nllmbuilder finetune model \\\n  --model ./my_first_model/model.pt \\\n  --dataset domain_data.txt \\\n  --output ./fine_tuned_model \\\n  --epochs 5 \\\n  --lr 1e-5\n</code></pre>"},{"location":"getting-started/first-model/#experiment-3-export-for-production","title":"Experiment 3: Export for Production","text":"<p>Export your model for different deployment scenarios:</p> <pre><code># Export to GGUF for llama.cpp\nllmbuilder export gguf \\\n  ./my_first_model/model.pt \\\n  --output my_model.gguf \\\n  --quantization q4_0\n\n# Export to ONNX for mobile/edge\nllmbuilder export onnx \\\n  ./my_first_model/model.pt \\\n  --output my_model.onnx\n</code></pre>"},{"location":"getting-started/first-model/#troubleshooting","title":"\ud83d\udd0d Troubleshooting","text":""},{"location":"getting-started/first-model/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"getting-started/first-model/#1-out-of-memory-error","title":"1. Out of Memory Error","text":"<pre><code># Reduce batch size\nllmbuilder train model --config model_config.json --batch-size 2\n\n# Or use gradient accumulation\nllmbuilder train model --config model_config.json --gradient-accumulation-steps 4\n</code></pre>"},{"location":"getting-started/first-model/#2-poor-generation-quality","title":"2. Poor Generation Quality","text":"<pre><code># Train for more epochs\nllmbuilder train model --config model_config.json --epochs 20\n\n# Use more training data\n# Add more text files to your dataset\n\n# Adjust model size\nllmbuilder config create --preset gpu_medium --output larger_config.json\n</code></pre>"},{"location":"getting-started/first-model/#3-training-too-slow","title":"3. Training Too Slow","text":"<pre><code># Use GPU if available\nllmbuilder train model --config model_config.json --device cuda\n\n# Reduce sequence length\n# Edit model_config.json: \"max_seq_length\": 256\n</code></pre>"},{"location":"getting-started/first-model/#understanding-your-results","title":"\ud83d\udcc8 Understanding Your Results","text":""},{"location":"getting-started/first-model/#what-makes-a-good-model","title":"What Makes a Good Model?","text":"<ul> <li>Perplexity &lt; 20: Good for small datasets</li> <li>Coherent text: Generated text should make sense</li> <li>Diverse outputs: Different prompts should produce varied responses</li> <li>Fast inference: Generation should be reasonably quick</li> </ul>"},{"location":"getting-started/first-model/#improving-your-model","title":"Improving Your Model","text":"<ol> <li>More data: The most important factor</li> <li>Longer training: More epochs often help</li> <li>Better data quality: Clean, relevant text</li> <li>Hyperparameter tuning: Adjust learning rate, batch size</li> <li>Model architecture: Try different sizes</li> </ol>"},{"location":"getting-started/first-model/#congratulations","title":"\ud83c\udf89 Congratulations","text":"<p>You've successfully:</p> <p>\u2705 Prepared training data \u2705 Configured a model \u2705 Trained a tokenizer \u2705 Trained a language model \u2705 Generated text \u2705 Evaluated performance</p>"},{"location":"getting-started/first-model/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<p>Now that you have the basics down, explore:</p> <ul> <li>Advanced Training - Learn advanced techniques</li> <li>Fine-tuning Guide - Adapt models to specific domains</li> <li>Model Export - Deploy your models</li> <li>CLI Reference - Complete CLI documentation</li> </ul> <p>Keep Experimenting!</p> <p>The model you just trained is small and trained on limited data, but you've learned the complete workflow. Try training on larger datasets, experimenting with different architectures, and fine-tuning for specific tasks. The possibilities are endless!</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you install LLMBuilder and set up your environment for training and deploying language models.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":""},{"location":"getting-started/installation/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>Python: 3.8 or higher</li> <li>RAM: 4GB (8GB+ recommended)</li> <li>Storage: 2GB free space</li> <li>OS: Windows, macOS, or Linux</li> </ul>"},{"location":"getting-started/installation/#recommended-requirements","title":"Recommended Requirements","text":"<ul> <li>Python: 3.9 or higher</li> <li>RAM: 16GB or more</li> <li>GPU: NVIDIA GPU with 8GB+ VRAM (optional but recommended)</li> <li>Storage: 10GB+ free space for models and data</li> </ul>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#method-1-pypi-installation-recommended","title":"Method 1: PyPI Installation (Recommended)","text":"<p>The easiest way to install LLMBuilder is via PyPI:</p> <pre><code>pip install llmbuilder\n</code></pre>"},{"location":"getting-started/installation/#method-2-development-installation","title":"Method 2: Development Installation","text":"<p>For the latest features or if you want to contribute:</p> <pre><code># Clone the repository\ngit clone https://github.com/Qubasehq/llmbuilder-package.git\ncd llmbuilder-package\n\n# Install in development mode\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#method-3-cpu-only-installation","title":"Method 3: CPU-Only Installation","text":"<p>If you don't have a GPU or want to use CPU-only PyTorch:</p> <pre><code># Install CPU-only PyTorch first\npip install torch --index-url https://download.pytorch.org/whl/cpu\n\n# Then install LLMBuilder\npip install llmbuilder\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Test your installation by running:</p> <pre><code># Check if LLMBuilder is installed\npython -c \"import llmbuilder; print(f'LLMBuilder {llmbuilder.__version__} installed successfully!')\"\n\n# Test the CLI\nllmbuilder --version\nllmbuilder info\n</code></pre> <p>You should see output similar to:</p> <pre><code>LLMBuilder installed successfully!\n\ud83e\udd16 LLMBuilder version\nA comprehensive toolkit for building, training, and deploying language models.\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation/#for-gpu-training","title":"For GPU Training","text":"<p>If you have an NVIDIA GPU and want to use CUDA:</p> <pre><code># Install CUDA-enabled PyTorch (adjust version as needed)\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n</code></pre>"},{"location":"getting-started/installation/#for-advanced-data-processing","title":"For Advanced Data Processing","text":"<p>For processing various document formats:</p> <pre><code>pip install llmbuilder[data]\n</code></pre> <p>This includes:</p> <ul> <li><code>pandas</code> - For data manipulation</li> <li><code>pymupdf</code> - For PDF processing</li> <li><code>docx2txt</code> - For DOCX files</li> <li><code>python-pptx</code> - For PowerPoint files</li> <li><code>beautifulsoup4</code> - For HTML processing</li> </ul>"},{"location":"getting-started/installation/#for-development","title":"For Development","text":"<p>If you're contributing to LLMBuilder:</p> <pre><code>pip install llmbuilder[dev]\n</code></pre> <p>This includes:</p> <ul> <li><code>pytest</code> - For running tests</li> <li><code>black</code> - For code formatting</li> <li><code>ruff</code> - For linting</li> <li><code>mypy</code> - For type checking</li> </ul>"},{"location":"getting-started/installation/#for-model-export","title":"For Model Export","text":"<p>For exporting models to different formats:</p> <pre><code>pip install llmbuilder[export]\n</code></pre> <p>This includes:</p> <ul> <li><code>onnx</code> - For ONNX export</li> <li><code>onnxruntime</code> - For ONNX inference</li> </ul>"},{"location":"getting-started/installation/#environment-setup","title":"Environment Setup","text":""},{"location":"getting-started/installation/#virtual-environment-recommended","title":"Virtual Environment (Recommended)","text":"<p>Create a dedicated virtual environment for LLMBuilder:</p> Using venvUsing conda <pre><code># Create virtual environment\npython -m venv llmbuilder-env\n\n# Activate it\n# On Windows:\nllmbuilder-env\\Scripts\\activate\n# On macOS/Linux:\nsource llmbuilder-env/bin/activate\n\n# Install LLMBuilder\npip install llmbuilder\n</code></pre> <pre><code># Create conda environment\nconda create -n llmbuilder python=3.9\nconda activate llmbuilder\n\n# Install LLMBuilder\npip install llmbuilder\n</code></pre>"},{"location":"getting-started/installation/#environment-variables","title":"Environment Variables","text":"<p>You can set these optional environment variables:</p> <pre><code># Enable slow tests (for development)\nexport RUN_SLOW=1\n\n# Enable performance tests (for development)\nexport RUN_PERF=1\n\n# Set default device\nexport LLMBUILDER_DEVICE=cuda  # or 'cpu'\n\n# Set cache directory\nexport LLMBUILDER_CACHE_DIR=/path/to/cache\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/installation/#importerror-no-module-named-torch","title":"ImportError: No module named 'torch'","text":"<p>Solution: Install PyTorch first:</p> <pre><code>pip install torch\n</code></pre>"},{"location":"getting-started/installation/#cuda-out-of-memory","title":"CUDA out of memory","text":"<p>Solution: Use CPU-only installation or reduce batch size:</p> <pre><code>pip install torch --index-url https://download.pytorch.org/whl/cpu\n</code></pre>"},{"location":"getting-started/installation/#permission-denied-errors","title":"Permission denied errors","text":"<p>Solution: Use <code>--user</code> flag or virtual environment:</p> <pre><code>pip install --user llmbuilder\n</code></pre>"},{"location":"getting-started/installation/#package-conflicts","title":"Package conflicts","text":"<p>Solution: Create a fresh virtual environment:</p> <pre><code>python -m venv fresh-env\nsource fresh-env/bin/activate  # or fresh-env\\Scripts\\activate on Windows\npip install llmbuilder\n</code></pre>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the logs: LLMBuilder provides detailed error messages</li> <li>Search existing issues: GitHub Issues</li> <li>Create a new issue: Include your system info and error messages</li> <li>Join discussions: GitHub Discussions</li> </ol>"},{"location":"getting-started/installation/#system-information","title":"System Information","text":"<p>To help with troubleshooting, you can gather system information:</p> <pre><code>import llmbuilder\nimport torch\nimport sys\nimport platform\n\nprint(f\"LLMBuilder version: {llmbuilder.__version__}\")\nprint(f\"Python version: {sys.version}\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"GPU count: {torch.cuda.device_count()}\")\nprint(f\"Platform: {platform.platform()}\")\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once you have LLMBuilder installed:</p> <ol> <li>Quick Start - Get up and running in 5 minutes</li> <li>First Model - Train your first language model</li> <li>User Guide - Learn about all features</li> </ol> <p>Pro Tip</p> <p>For the best experience, we recommend using a virtual environment and installing the GPU version of PyTorch if you have a compatible NVIDIA GPU.</p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with LLMBuilder in just 5 minutes! This guide will walk you through training your first language model.</p>"},{"location":"getting-started/quickstart/#5-minute-setup","title":"\ud83d\ude80 5-Minute Setup","text":""},{"location":"getting-started/quickstart/#step-1-install-llmbuilder","title":"Step 1: Install LLMBuilder","text":"<pre><code>pip install llmbuilder\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-prepare-your-data","title":"Step 2: Prepare Your Data","text":"<p>Create a simple text file with some training data:</p> <pre><code># Create a sample data file\necho \"Artificial intelligence is transforming the world. Machine learning enables computers to learn from data. Deep learning uses neural networks to solve complex problems.\" &gt; sample_data.txt\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-train-your-first-model","title":"Step 3: Train Your First Model","text":"<p>Use the interactive CLI to train a model:</p> <pre><code>llmbuilder welcome\n</code></pre> <p>Or use the direct command:</p> <pre><code># Train a small model (perfect for testing)\nllmbuilder train model \\\n  --data sample_data.txt \\\n  --tokenizer ./tokenizer \\\n  --output ./my_first_model \\\n  --epochs 5 \\\n  --batch-size 2\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-alternative-use-the-bundled-example-model_test","title":"Step 3 (Alternative): Use the bundled example (Model_Test)","text":"<p>If you cloned the repo, we include a tiny cybersecurity dataset under <code>Model_Test/Data/</code> and a ready-to-run script in <code>docs/train_model.py</code>. Note: the script auto-detects a nested <code>Data/</code> folder if you pass <code>--data_dir ./Model_Test</code>.</p> <pre><code># Train using the example dataset (explicit Data/ path)\npython docs/train_model.py --data_dir ./Model_Test/Data --output_dir ./Model_Test/output \\\n  --epochs 5 --batch_size 1 --block_size 64 --embed_dim 256 --layers 4 --heads 8 \\\n  --prompt \"Cybersecurity is important because\"\n\n# After training, generate again any time without retraining\npython -c \"import llmbuilder; print(llmbuilder.generate_text(\\\n  model_path=r'.\\\\Model_Test\\\\output\\\\checkpoints\\\\latest_checkpoint.pt', \\\n  tokenizer_path=r'.\\\\Model_Test\\\\output\\\\tokenizer', \\\n  prompt='what is Cybersecurity', max_new_tokens=80, temperature=0.8, top_p=0.9))\"\n</code></pre> <p>Outputs are created in <code>Model_Test/output/</code>:</p> <ul> <li><code>Model_Test/output/tokenizer/</code> \u2013 trained tokenizer</li> <li><code>Model_Test/output/checkpoints/</code> \u2013 model checkpoints (latest, epoch_*.pt)</li> </ul>"},{"location":"getting-started/quickstart/#step-4-generate-text","title":"Step 4: Generate Text","text":"<pre><code># Generate text with your trained model\nllmbuilder generate text \\\n  --model ./my_first_model/model.pt \\\n  --tokenizer ./tokenizer \\\n  --prompt \"Artificial intelligence\" \\\n  --max-tokens 50\n</code></pre> <p>\ud83c\udf89 Congratulations! You've just trained and used your first language model with LLMBuilder!</p>"},{"location":"getting-started/quickstart/#python-api-quick-start","title":"\ud83d\udc0d Python API Quick Start","text":"<p>Prefer Python code? Here's the same workflow using the Python API:</p> <pre><code>import llmbuilder as lb\n\n# 1. Load configuration\ncfg = lb.load_config(preset=\"cpu_small\")\n\n# 2. Build model\nmodel = lb.build_model(cfg.model)\n\n# 3. Prepare data\nfrom llmbuilder.data import TextDataset\ndataset = TextDataset(\"sample_data.txt\", block_size=cfg.model.max_seq_length)\n\n# 4. Train model\nresults = lb.train_model(model, dataset, cfg.training)\n\n# 5. Generate text\ntext = lb.generate_text(\n    model_path=\"./checkpoints/model.pt\",\n    tokenizer_path=\"./tokenizers\",\n    prompt=\"Artificial intelligence\",\n    max_new_tokens=50\n)\nprint(text)\n</code></pre>"},{"location":"getting-started/quickstart/#understanding-the-output","title":"\ud83d\udcca Understanding the Output","text":"<p>When training completes, you'll see output like:</p> <pre><code>\u2705 Training completed successfully!\n\ud83d\udcca Final Results:\n  \u2022 Training Loss: 2.45\n  \u2022 Validation Loss: 2.52\n  \u2022 Training Time: 3m 24s\n  \u2022 Model Parameters: 2.1M\n  \u2022 Model Size: 8.4MB\n\n\ud83d\udcbe Outputs saved to:\n  \u2022 Model: ./my_first_model/model.pt\n  \u2022 Tokenizer: ./tokenizer/\n  \u2022 Logs: ./my_first_model/training.log\n</code></pre>"},{"location":"getting-started/quickstart/#what-just-happened","title":"\ud83c\udfaf What Just Happened?","text":"<p>Let's break down what LLMBuilder did:</p> <ol> <li>Data Processing: Loaded and cleaned your text data</li> <li>Tokenization: Created a vocabulary and tokenized the text</li> <li>Model Creation: Built a GPT-style transformer model</li> <li>Training: Trained the model to predict the next token</li> <li>Saving: Saved the model and tokenizer for later use</li> </ol>"},{"location":"getting-started/quickstart/#customization-options","title":"\ud83d\udd27 Customization Options","text":""},{"location":"getting-started/quickstart/#different-model-sizes","title":"Different Model Sizes","text":"<pre><code># Tiny model (fastest, least memory)\nllmbuilder train model --config-preset tiny --data data.txt --output model/\n\n# Small model (balanced)\nllmbuilder train model --config-preset cpu_small --data data.txt --output model/\n\n# Medium model (better quality, needs more resources)\nllmbuilder train model --config-preset gpu_medium --data data.txt --output model/\n</code></pre>"},{"location":"getting-started/quickstart/#different-data-formats","title":"Different Data Formats","text":"<p>LLMBuilder supports multiple input formats:</p> <pre><code># Process PDF files\nllmbuilder data load --input documents/ --output clean_text.txt --format pdf\n\n# Process DOCX files\nllmbuilder data load --input documents/ --output clean_text.txt --format docx\n\n# Process all supported formats\nllmbuilder data load --input documents/ --output clean_text.txt --format all\n</code></pre>"},{"location":"getting-started/quickstart/#interactive-mode","title":"Interactive Mode","text":"<p>For a guided experience:</p> <pre><code># Interactive training setup\nllmbuilder train model --interactive\n\n# Interactive text generation\nllmbuilder generate text --setup\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"\ud83d\udcc8 Next Steps","text":"<p>Now that you have a working model, here are some things to try:</p>"},{"location":"getting-started/quickstart/#1-improve-your-model","title":"1. Improve Your Model","text":"<pre><code># Train for more epochs\nllmbuilder train model --data data.txt --output model/ --epochs 20\n\n# Use a larger model\nllmbuilder train model --data data.txt --output model/ --layers 12 --dim 768\n\n# Add more training data\nllmbuilder data load --input more_documents/ --output bigger_dataset.txt\n</code></pre>"},{"location":"getting-started/quickstart/#2-fine-tune-on-specific-data","title":"2. Fine-tune on Specific Data","text":"<pre><code># Fine-tune your model on domain-specific data\nllmbuilder finetune model \\\n  --model ./my_first_model/model.pt \\\n  --dataset domain_specific_data.txt \\\n  --output ./fine_tuned_model\n</code></pre>"},{"location":"getting-started/quickstart/#3-export-for-production","title":"3. Export for Production","text":"<pre><code># Export to GGUF format for llama.cpp\nllmbuilder export gguf ./my_first_model/model.pt --output model.gguf\n\n# Export to ONNX for mobile/edge deployment\nllmbuilder export onnx ./my_first_model/model.pt --output model.onnx\n</code></pre>"},{"location":"getting-started/quickstart/#4-advanced-generation","title":"4. Advanced Generation","text":"<pre><code>import llmbuilder as lb\n\n# Interactive chat-like generation\nlb.interactive_cli(\n    model_path=\"./my_first_model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    temperature=0.8,\n    top_k=50\n)\n</code></pre>"},{"location":"getting-started/quickstart/#configuration-presets","title":"\ud83d\udee0\ufe0f Configuration Presets","text":"<p>LLMBuilder comes with several built-in presets:</p> Preset Use Case Memory Training Time <code>tiny</code> Testing, debugging ~1GB Minutes <code>cpu_small</code> CPU training, learning ~2GB Hours <code>gpu_medium</code> Single GPU training ~8GB Hours <code>gpu_large</code> High-end GPU training ~16GB+ Days"},{"location":"getting-started/quickstart/#monitoring-training","title":"\ud83d\udd0d Monitoring Training","text":""},{"location":"getting-started/quickstart/#real-time-monitoring","title":"Real-time Monitoring","text":"<pre><code># Monitor training progress\ntail -f ./my_first_model/training.log\n\n# Or use the built-in progress display\nllmbuilder train model --data data.txt --output model/ --verbose\n</code></pre>"},{"location":"getting-started/quickstart/#training-metrics","title":"Training Metrics","text":"<p>LLMBuilder tracks important metrics:</p> <ul> <li>Loss: How well the model is learning</li> <li>Perplexity: Model confidence (lower is better)</li> <li>Learning Rate: Training speed</li> <li>Memory Usage: Resource consumption</li> </ul>"},{"location":"getting-started/quickstart/#common-issues-solutions","title":"\ud83d\udea8 Common Issues &amp; Solutions","text":""},{"location":"getting-started/quickstart/#out-of-memory","title":"Out of Memory","text":"<pre><code># Reduce batch size\nllmbuilder train model --data data.txt --output model/ --batch-size 1\n\n# Use CPU-only mode\nllmbuilder train model --data data.txt --output model/ --device cpu\n</code></pre>"},{"location":"getting-started/quickstart/#slow-training","title":"Slow Training","text":"<pre><code># Use GPU if available\nllmbuilder train model --data data.txt --output model/ --device cuda\n\n# Reduce model size\nllmbuilder train model --data data.txt --output model/ --layers 4 --dim 256\n</code></pre>"},{"location":"getting-started/quickstart/#poor-generation-quality","title":"Poor Generation Quality","text":"<pre><code># Train for more epochs\nllmbuilder train model --data data.txt --output model/ --epochs 50\n\n# Use more training data\n# Add more text files to your dataset\n\n# Adjust generation parameters\nllmbuilder generate text --model model.pt --tokenizer tokenizer/ \\\n  --prompt \"Your prompt\" --temperature 0.7 --top-k 40\n</code></pre>"},{"location":"getting-started/quickstart/#learn-more","title":"\ud83d\udcda Learn More","text":"<p>Ready to dive deeper? Check out these resources:</p> <ul> <li>First Model Tutorial - Detailed step-by-step guide</li> <li>Configuration Guide - Customize your setup</li> <li>Training Guide - Advanced training techniques</li> <li>Model Export - Deploy your models</li> <li>CLI Reference - Complete CLI documentation</li> </ul> <p>You're Ready!</p> <p>You now have a working LLMBuilder setup! The model you just trained might be small, but you've learned the complete workflow. Try experimenting with different data, model sizes, and generation parameters to see what works best for your use case.</p>"},{"location":"reference/faq/","title":"Frequently Asked Questions","text":"<p>Common questions and answers about LLMBuilder usage, troubleshooting, and best practices.</p>"},{"location":"reference/faq/#getting-started","title":"\ud83d\ude80 Getting Started","text":""},{"location":"reference/faq/#q-what-are-the-minimum-system-requirements","title":"Q: What are the minimum system requirements?","text":"<p>A: LLMBuilder requires:</p> <ul> <li>Python 3.8+ (3.9+ recommended)</li> <li>4GB RAM minimum (8GB+ recommended)</li> <li>2GB free disk space for installation and basic models</li> <li>Optional: NVIDIA GPU with 4GB+ VRAM for faster training</li> </ul>"},{"location":"reference/faq/#q-should-i-use-cpu-or-gpu-for-training","title":"Q: Should I use CPU or GPU for training?","text":"<p>A:</p> <ul> <li>CPU: Good for learning, small models, and development. Use <code>preset=\"cpu_small\"</code></li> <li>GPU: Recommended for production training and larger models. Use <code>preset=\"gpu_medium\"</code> or <code>preset=\"gpu_large\"</code></li> <li>Mixed: Start with CPU for prototyping, then move to GPU for final training</li> </ul>"},{"location":"reference/faq/#q-how-long-does-it-take-to-train-a-model","title":"Q: How long does it take to train a model?","text":"<p>A: Training time depends on several factors:</p> <ul> <li>Small model (10M params): 30 minutes - 2 hours on CPU, 5-15 minutes on GPU</li> <li>Medium model (50M params): 2-8 hours on CPU, 30 minutes - 2 hours on GPU</li> <li>Large model (200M+ params): Days on CPU, 2-12 hours on GPU</li> </ul>"},{"location":"reference/faq/#configuration","title":"\ud83d\udd27 Configuration","text":""},{"location":"reference/faq/#q-which-configuration-preset-should-i-use","title":"Q: Which configuration preset should I use?","text":"<p>A: Choose based on your hardware and use case:</p> Preset Use Case Hardware Model Size Training Time <code>tiny</code> Testing, debugging Any ~1M params Minutes <code>cpu_small</code> Learning, development CPU ~10M params Hours <code>gpu_medium</code> Production training Single GPU ~50M params Hours <code>gpu_large</code> High-quality models High-end GPU ~200M+ params Days"},{"location":"reference/faq/#q-how-do-i-customize-model-architecture","title":"Q: How do I customize model architecture?","text":"<p>A: Modify the model configuration:</p> <pre><code>from llmbuilder.config import ModelConfig\n\nconfig = ModelConfig(\n    vocab_size=16000,      # Match your tokenizer\n    num_layers=12,         # More layers = more capacity\n    num_heads=12,          # Should divide embedding_dim evenly\n    embedding_dim=768,     # Larger = more capacity\n    max_seq_length=1024,   # Longer sequences = more memory\n    dropout=0.1            # Higher = more regularization\n)\n</code></pre>"},{"location":"reference/faq/#q-what-vocabulary-size-should-i-use","title":"Q: What vocabulary size should I use?","text":"<p>A: Vocabulary size depends on your data and use case:</p> <ul> <li>8K-16K: Small datasets, specific domains</li> <li>16K-32K: General purpose, balanced size</li> <li>32K-64K: Large datasets, multilingual models</li> <li>64K+: Very large datasets, maximum coverage</li> </ul>"},{"location":"reference/faq/#data-and-training","title":"\ud83d\udcca Data and Training","text":""},{"location":"reference/faq/#q-how-much-training-data-do-i-need","title":"Q: How much training data do I need?","text":"<p>A: Data requirements vary by model size and quality goals:</p> <ul> <li>Minimum: 1MB of text (~200K words) for basic functionality</li> <li>Recommended: 10MB+ of text (~2M words) for good quality</li> <li>Optimal: 100MB+ of text (~20M words) for high quality</li> <li>Production: 1GB+ of text (~200M words) for best results</li> </ul>"},{"location":"reference/faq/#q-what-file-formats-are-supported-for-training-data","title":"Q: What file formats are supported for training data?","text":"<p>A: LLMBuilder supports:</p> <ul> <li>Text files: <code>.txt</code>, <code>.md</code> (best quality)</li> <li>Documents: <code>.pdf</code>, <code>.docx</code> (good quality)</li> <li>Web content: <code>.html</code>, <code>.htm</code> (moderate quality)</li> <li>Presentations: <code>.pptx</code> (basic support)</li> <li>Data files: <code>.csv</code>, <code>.json</code> (with proper formatting)</li> </ul>"},{"location":"reference/faq/#q-how-do-i-handle-out-of-memory-errors","title":"Q: How do I handle out-of-memory errors?","text":"<p>A: Try these solutions in order:</p> <ol> <li>Reduce batch size:</li> </ol> <pre><code>config.training.batch_size = 4  # or even 2\n</code></pre> <ol> <li>Enable gradient checkpointing:</li> </ol> <pre><code>config.model.gradient_checkpointing = True\n</code></pre> <ol> <li>Use gradient accumulation:</li> </ol> <pre><code>config.training.gradient_accumulation_steps = 4\n</code></pre> <ol> <li>Reduce sequence length:</li> </ol> <pre><code>config.model.max_seq_length = 512\n</code></pre> <ol> <li>Use CPU training:</li> </ol> <pre><code>config.system.device = \"cpu\"\n</code></pre>"},{"location":"reference/faq/#q-my-model-isnt-learning-loss-not-decreasing-whats-wrong","title":"Q: My model isn't learning (loss not decreasing). What's wrong?","text":"<p>A: Common causes and solutions:</p> <ol> <li>Learning rate too high: Reduce to 1e-4 or 1e-5</li> <li>Learning rate too low: Increase to 3e-4 or 5e-4</li> <li>Bad data: Check for corrupted or repetitive text</li> <li>Wrong tokenizer: Ensure vocab_size matches tokenizer</li> <li>Insufficient warmup: Increase warmup_steps to 1000+</li> </ol>"},{"location":"reference/faq/#q-how-do-i-know-if-my-model-is-overfitting","title":"Q: How do I know if my model is overfitting?","text":"<p>A: Signs of overfitting:</p> <ul> <li>Training loss decreases but validation loss increases</li> <li>Generated text is repetitive or memorized</li> <li>Model performs poorly on new data</li> </ul> <p>Solutions:</p> <ul> <li>Increase dropout rate (0.1 \u2192 0.2)</li> <li>Add weight decay (0.01)</li> <li>Use early stopping</li> <li>Get more training data</li> <li>Reduce model size</li> </ul>"},{"location":"reference/faq/#text-generation","title":"\ud83c\udfaf Text Generation","text":""},{"location":"reference/faq/#q-how-do-i-improve-generation-quality","title":"Q: How do I improve generation quality?","text":"<p>A: Try these techniques:</p> <ol> <li>Adjust temperature:</li> <li>Lower (0.3-0.7): More focused, predictable</li> <li> <p>Higher (0.8-1.2): More creative, diverse</p> </li> <li> <p>Use nucleus sampling:</p> </li> </ol> <pre><code>config = GenerationConfig(\n    temperature=0.8,\n    top_p=0.9,      # Nucleus sampling\n    top_k=50        # Top-k sampling\n)\n</code></pre> <ol> <li>Add repetition penalty:</li> </ol> <pre><code>config.repetition_penalty = 1.1\n</code></pre> <ol> <li>Better prompts:</li> <li>Be specific and clear</li> <li>Provide context and examples</li> <li>Use consistent formatting</li> </ol>"},{"location":"reference/faq/#q-why-is-my-generated-text-repetitive","title":"Q: Why is my generated text repetitive?","text":"<p>A: Common causes and fixes:</p> <ol> <li>Insufficient training: Train for more epochs</li> <li>Poor sampling: Use top-p/top-k sampling instead of greedy</li> <li>Low temperature: Increase temperature to 0.8+</li> <li>Add repetition penalty: Set to 1.1-1.3</li> <li>Prevent n-gram repetition: Set <code>no_repeat_ngram_size=3</code></li> </ol>"},{"location":"reference/faq/#q-how-do-i-make-generation-faster","title":"Q: How do I make generation faster?","text":"<p>A: Speed optimization techniques:</p> <ol> <li>Use GPU: Much faster than CPU</li> <li>Reduce max_tokens: Generate shorter responses</li> <li>Use greedy decoding: Set <code>do_sample=False</code></li> <li>Enable model compilation: Set <code>compile=True</code> (PyTorch 2.0+)</li> <li>Quantize model: Use 8-bit or 16-bit precision</li> </ol>"},{"location":"reference/faq/#fine-tuning","title":"\ud83d\udd04 Fine-tuning","text":""},{"location":"reference/faq/#q-when-should-i-fine-tune-vs-train-from-scratch","title":"Q: When should I fine-tune vs. train from scratch?","text":"<p>A:</p> <ul> <li>Fine-tune when: You have a pre-trained model and domain-specific data</li> <li>Train from scratch when: You have lots of data and need full control</li> <li>Fine-tuning advantages: Faster, less data needed, preserves general knowledge</li> <li>Training advantages: Full customization, no dependency on base model</li> </ul>"},{"location":"reference/faq/#q-whats-the-difference-between-lora-and-full-fine-tuning","title":"Q: What's the difference between LoRA and full fine-tuning?","text":"<p>A:</p> Aspect LoRA Full Fine-tuning Memory Low (~1% of params) High (all params) Speed Fast Slower Quality Good for most tasks Best possible Flexibility Limited adaptation Full adaptation Use case Domain adaptation Major architecture changes"},{"location":"reference/faq/#q-how-do-i-prevent-catastrophic-forgetting-during-fine-tuning","title":"Q: How do I prevent catastrophic forgetting during fine-tuning?","text":"<p>A: Use these techniques:</p> <ol> <li>Lower learning rate: 1e-5 to 5e-5</li> <li>Fewer epochs: 3-5 epochs usually sufficient</li> <li>Regularization: Add weight decay (0.01)</li> <li>LoRA: Preserves base model weights</li> <li>Mixed training: Include general data with domain data</li> </ol>"},{"location":"reference/faq/#deployment","title":"\ud83d\ude80 Deployment","text":""},{"location":"reference/faq/#q-how-do-i-deploy-my-trained-model","title":"Q: How do I deploy my trained model?","text":"<p>A: LLMBuilder supports multiple deployment options:</p> <ol> <li>GGUF format (for llama.cpp):</li> </ol> <pre><code>llmbuilder export gguf model.pt --output model.gguf --quantization q4_0\n</code></pre> <ol> <li>ONNX format (for cross-platform):</li> </ol> <pre><code>llmbuilder export onnx model.pt --output model.onnx\n</code></pre> <ol> <li>Quantized PyTorch (for production):</li> </ol> <pre><code>llmbuilder export quantize model.pt --output model_int8.pt --bits 8\n</code></pre>"},{"location":"reference/faq/#q-which-export-format-should-i-choose","title":"Q: Which export format should I choose?","text":"<p>A: Choose based on your deployment target:</p> <ul> <li>GGUF: CPU inference, llama.cpp compatibility, edge devices</li> <li>ONNX: Cross-platform, mobile apps, cloud services</li> <li>Quantized PyTorch: PyTorch ecosystem, balanced performance</li> <li>HuggingFace: Easy sharing, transformers compatibility</li> </ul>"},{"location":"reference/faq/#q-how-do-i-reduce-model-size-for-deployment","title":"Q: How do I reduce model size for deployment?","text":"<p>A: Size reduction techniques:</p> <ol> <li>Quantization: 8-bit (50% smaller) or 4-bit (75% smaller)</li> <li>Pruning: Remove least important weights</li> <li>Distillation: Train smaller model to mimic larger one</li> <li>Architecture optimization: Use efficient attention mechanisms</li> </ol>"},{"location":"reference/faq/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"reference/faq/#q-i-get-cuda-out-of-memory-errors-what-should-i-do","title":"Q: I get \"CUDA out of memory\" errors. What should I do?","text":"<p>A: Try these solutions:</p> <ol> <li>Reduce batch size: Start with batch_size=1</li> <li>Enable gradient checkpointing: Trades compute for memory</li> <li>Use gradient accumulation: Simulate larger batches</li> <li>Reduce sequence length: Shorter sequences use less memory</li> <li>Use CPU: Slower but no memory limits</li> <li>Clear GPU cache: <code>torch.cuda.empty_cache()</code></li> </ol>"},{"location":"reference/faq/#q-training-is-very-slow-how-can-i-speed-it-up","title":"Q: Training is very slow. How can I speed it up?","text":"<p>A: Speed optimization:</p> <ol> <li>Use GPU: 10-100x faster than CPU</li> <li>Increase batch size: Better GPU utilization</li> <li>Enable mixed precision: <code>fp16</code> or <code>bf16</code></li> <li>Use multiple GPUs: Distributed training</li> <li>Optimize data loading: More workers, pin memory</li> <li>Compile model: PyTorch 2.0 compilation</li> </ol>"},{"location":"reference/faq/#q-my-tokenizer-produces-weird-results-whats-wrong","title":"Q: My tokenizer produces weird results. What's wrong?","text":"<p>A: Common tokenizer issues:</p> <ol> <li>Wrong vocabulary size: Must match model config</li> <li>Insufficient training data: Need diverse text corpus</li> <li>Character coverage too low: Increase to 0.9999</li> <li>Wrong model type: BPE usually works best</li> <li>Missing special tokens: Include <code>&lt;pad&gt;</code>, <code>&lt;unk&gt;</code>, etc.</li> </ol>"},{"location":"reference/faq/#q-generated-text-contains-strange-characters-or-formatting","title":"Q: Generated text contains strange characters or formatting","text":"<p>A: Text cleaning solutions:</p> <ol> <li>Improve data cleaning: Remove unwanted characters</li> <li>Filter by language: Keep only desired languages</li> <li>Normalize text: Fix encoding issues</li> <li>Add text filters: Remove specific patterns</li> <li>Better tokenizer training: Use cleaner training data</li> </ol>"},{"location":"reference/faq/#best-practices","title":"\ud83d\udca1 Best Practices","text":""},{"location":"reference/faq/#q-what-are-the-most-important-best-practices","title":"Q: What are the most important best practices?","text":"<p>A: Key recommendations:</p> <ol> <li>Start small: Begin with tiny models and scale up</li> <li>Clean your data: Quality over quantity</li> <li>Monitor training: Watch loss curves and generation quality</li> <li>Save checkpoints: Protect against failures</li> <li>Validate everything: Test configurations before long training</li> <li>Document experiments: Keep track of what works</li> </ol>"},{"location":"reference/faq/#q-how-do-i-choose-hyperparameters","title":"Q: How do I choose hyperparameters?","text":"<p>A: Hyperparameter selection guide:</p> <ol> <li>Learning rate: Start with 3e-4, adjust based on loss curves</li> <li>Batch size: Largest that fits in memory</li> <li>Model size: Balance quality needs with resources</li> <li>Sequence length: Match your use case requirements</li> <li>Dropout: 0.1 is usually good, increase if overfitting</li> </ol>"},{"location":"reference/faq/#q-how-do-i-evaluate-model-quality","title":"Q: How do I evaluate model quality?","text":"<p>A: Evaluation methods:</p> <ol> <li>Perplexity: Lower is better (&lt; 20 is good)</li> <li>Generation quality: Manual inspection of outputs</li> <li>Task-specific metrics: BLEU, ROUGE for specific tasks</li> <li>Human evaluation: Best but most expensive</li> <li>Automated metrics: Coherence, fluency scores</li> </ol>"},{"location":"reference/faq/#getting-help","title":"\ud83c\udd98 Getting Help","text":""},{"location":"reference/faq/#q-where-can-i-get-help-if-im-stuck","title":"Q: Where can I get help if I'm stuck?","text":"<p>A: Support resources:</p> <ol> <li>Documentation: Complete guides and examples</li> <li>GitHub Issues: Report bugs and request features</li> <li>GitHub Discussions: Community Q&amp;A</li> <li>Examples: Working code samples</li> <li>Stack Overflow: Tag questions with <code>llmbuilder</code></li> </ol>"},{"location":"reference/faq/#q-how-do-i-report-a-bug","title":"Q: How do I report a bug?","text":"<p>A: When reporting bugs, include:</p> <ol> <li>LLMBuilder version: <code>llmbuilder --version</code></li> <li>Python version: <code>python --version</code></li> <li>Operating system: Windows/macOS/Linux</li> <li>Hardware: CPU/GPU specifications</li> <li>Error message: Full traceback</li> <li>Minimal example: Code to reproduce the issue</li> <li>Configuration: Model and training configs used</li> </ol>"},{"location":"reference/faq/#q-how-can-i-contribute-to-llmbuilder","title":"Q: How can I contribute to LLMBuilder?","text":"<p>A: Ways to contribute:</p> <ol> <li>Report bugs: Help improve stability</li> <li>Request features: Suggest improvements</li> <li>Submit PRs: Code contributions welcome</li> <li>Improve docs: Fix typos, add examples</li> <li>Share examples: Help other users</li> <li>Test releases: Try beta versions</li> </ol> <p>Still have questions?</p> <p>If you can't find the answer here, check our GitHub Discussions or create a new issue. The community is always happy to help!</p>"},{"location":"reference/migration/","title":"Migration Guide","text":"<p>This guide helps you migrate from legacy LLMBuilder scripts and configurations to the new package structure. Whether you're upgrading from older versions or transitioning from standalone scripts, this guide covers all the changes you need to know.</p>"},{"location":"reference/migration/#overview","title":"\ud83c\udfaf Overview","text":"<p>The LLMBuilder package represents a major evolution from the original standalone scripts. Key improvements include:</p> <ul> <li>Unified API: Single import for all functionality</li> <li>Better Configuration: Structured, validated configurations</li> <li>CLI Interface: Comprehensive command-line tools</li> <li>Modular Design: Use only what you need</li> <li>Better Documentation: Complete guides and examples</li> </ul>"},{"location":"reference/migration/#migration-checklist","title":"\ud83d\udccb Migration Checklist","text":"<ul> <li>[ ] Update Python version to 3.8+</li> <li>[ ] Install new LLMBuilder package</li> <li>[ ] Convert configuration files</li> <li>[ ] Update import statements</li> <li>[ ] Migrate training scripts</li> <li>[ ] Update CLI commands</li> <li>[ ] Test functionality</li> <li>[ ] Update deployment scripts</li> </ul>"},{"location":"reference/migration/#configuration-migration","title":"\ud83d\udd04 Configuration Migration","text":""},{"location":"reference/migration/#legacy-configuration-format","title":"Legacy Configuration Format","text":"<p>Old format (config.json):</p> <pre><code>{\n  \"n_layer\": 12,\n  \"n_head\": 12,\n  \"n_embd\": 768,\n  \"block_size\": 1024,\n  \"dropout\": 0.1,\n  \"bias\": true,\n  \"vocab_size\": 16000,\n  \"device\": \"cuda\",\n  \"batch_size\": 16,\n  \"learning_rate\": 3e-4,\n  \"max_iters\": 10000,\n  \"eval_interval\": 500,\n  \"eval_iters\": 100\n}\n</code></pre>"},{"location":"reference/migration/#new-configuration-format","title":"New Configuration Format","text":"<p>New format (structured):</p> <pre><code>{\n  \"model\": {\n    \"vocab_size\": 16000,\n    \"num_layers\": 12,\n    \"num_heads\": 12,\n    \"embedding_dim\": 768,\n    \"max_seq_length\": 1024,\n    \"dropout\": 0.1,\n    \"bias\": true,\n    \"model_type\": \"gpt\"\n  },\n  \"training\": {\n    \"batch_size\": 16,\n    \"learning_rate\": 3e-4,\n    \"num_epochs\": 10,\n    \"warmup_steps\": 1000,\n    \"weight_decay\": 0.01,\n    \"max_grad_norm\": 1.0,\n    \"save_every\": 1000,\n    \"eval_every\": 500,\n    \"log_every\": 100\n  },\n  \"system\": {\n    \"device\": \"cuda\",\n    \"seed\": 42,\n    \"deterministic\": false\n  }\n}\n</code></pre>"},{"location":"reference/migration/#automatic-migration","title":"Automatic Migration","text":"<p>LLMBuilder provides automatic migration for legacy configurations:</p> <pre><code>import llmbuilder as lb\n\n# Load legacy configuration (automatically migrated)\nconfig = lb.load_config(\"legacy_config.json\")\n\n# Or migrate explicitly\nfrom llmbuilder.config import migrate_legacy_config\nnew_config = migrate_legacy_config(\"legacy_config.json\")\nnew_config.save(\"new_config.json\")\n</code></pre>"},{"location":"reference/migration/#configuration-key-mapping","title":"Configuration Key Mapping","text":"Legacy Key New Key Notes <code>n_layer</code> <code>model.num_layers</code> Number of transformer layers <code>n_head</code> <code>model.num_heads</code> Number of attention heads <code>n_embd</code> <code>model.embedding_dim</code> Embedding dimension <code>block_size</code> <code>model.max_seq_length</code> Maximum sequence length <code>dropout</code> <code>model.dropout</code> Dropout rate <code>bias</code> <code>model.bias</code> Use bias in linear layers <code>vocab_size</code> <code>model.vocab_size</code> Vocabulary size <code>device</code> <code>system.device</code> Training device <code>batch_size</code> <code>training.batch_size</code> Batch size <code>learning_rate</code> <code>training.learning_rate</code> Learning rate <code>max_iters</code> <code>training.max_steps</code> Maximum training steps <code>eval_interval</code> <code>training.eval_every</code> Evaluation frequency"},{"location":"reference/migration/#code-migration","title":"\ud83d\udc0d Code Migration","text":""},{"location":"reference/migration/#legacy-import-pattern","title":"Legacy Import Pattern","text":"<p>Old way:</p> <pre><code># Legacy imports\nfrom model import GPTConfig, GPT\nfrom train import train\nfrom sample import sample\nimport tiktoken\n\n# Legacy usage\nconfig = GPTConfig(\n    n_layer=12,\n    n_head=12,\n    n_embd=768,\n    block_size=1024,\n    vocab_size=50257\n)\n\nmodel = GPT(config)\n</code></pre>"},{"location":"reference/migration/#new-import-pattern","title":"New Import Pattern","text":"<p>New way:</p> <pre><code># New unified import\nimport llmbuilder as lb\n\n# New usage\nconfig = lb.load_config(preset=\"gpu_medium\")\nmodel = lb.build_model(config.model)\n</code></pre>"},{"location":"reference/migration/#training-script-migration","title":"Training Script Migration","text":"<p>Legacy training script:</p> <pre><code># train.py (legacy)\nimport torch\nfrom model import GPTConfig, GPT\nfrom dataset import get_batch\n\n# Configuration\nconfig = GPTConfig(...)\nmodel = GPT(config)\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\n# Training loop\nfor iter in range(max_iters):\n    X, Y = get_batch('train')\n    logits, loss = model(X, Y)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if iter % eval_interval == 0:\n        evaluate()\n</code></pre> <p>New training script:</p> <pre><code># train.py (new)\nimport llmbuilder as lb\nfrom llmbuilder.data import TextDataset\n\n# Configuration\nconfig = lb.load_config(preset=\"gpu_medium\")\nmodel = lb.build_model(config.model)\n\n# Dataset\ndataset = TextDataset(\"data.txt\", block_size=config.model.max_seq_length)\n\n# Training (handled automatically)\nresults = lb.train_model(model, dataset, config.training)\n</code></pre>"},{"location":"reference/migration/#generation-script-migration","title":"Generation Script Migration","text":"<p>Legacy generation:</p> <pre><code># sample.py (legacy)\nimport torch\nfrom model import GPT\nimport tiktoken\n\n# Load model\nmodel = GPT.from_pretrained('model.pt')\nenc = tiktoken.get_encoding(\"gpt2\")\n\n# Generate\ncontext = enc.encode(\"Hello\")\ngenerated = model.generate(context, max_new_tokens=100)\ntext = enc.decode(generated)\n</code></pre> <p>New generation:</p> <pre><code># generate.py (new)\nimport llmbuilder as lb\n\n# Generate (much simpler)\ntext = lb.generate_text(\n    model_path=\"model.pt\",\n    tokenizer_path=\"tokenizer/\",\n    prompt=\"Hello\",\n    max_new_tokens=100\n)\n</code></pre>"},{"location":"reference/migration/#cli-migration","title":"\ud83d\udda5\ufe0f CLI Migration","text":""},{"location":"reference/migration/#legacy-cli-commands","title":"Legacy CLI Commands","text":"<p>Old commands:</p> <pre><code># Legacy training\npython train.py --config config.json --data data.txt\n\n# Legacy generation\npython sample.py --model model.pt --prompt \"Hello\" --num_samples 5\n\n# Legacy data preparation\npython prepare_data.py --input raw/ --output data.txt\n</code></pre>"},{"location":"reference/migration/#new-cli-commands","title":"New CLI Commands","text":"<p>New commands:</p> <pre><code># New training\nllmbuilder train model --config config.json --data data.txt --output model/\n\n# New generation\nllmbuilder generate text --model model.pt --tokenizer tokenizer/ --prompt \"Hello\"\n\n# New data preparation\nllmbuilder data load --input raw/ --output data.txt --clean\n</code></pre>"},{"location":"reference/migration/#cli-command-mapping","title":"CLI Command Mapping","text":"Legacy Command New Command Notes <code>python train.py</code> <code>llmbuilder train model</code> Unified training interface <code>python sample.py</code> <code>llmbuilder generate text</code> Enhanced generation options <code>python prepare_data.py</code> <code>llmbuilder data load</code> Better data processing <code>python eval.py</code> <code>llmbuilder model evaluate</code> Built-in evaluation"},{"location":"reference/migration/#data-format-migration","title":"\ud83d\udcca Data Format Migration","text":""},{"location":"reference/migration/#legacy-data-format","title":"Legacy Data Format","text":"<p>Old format:</p> <ul> <li>Single text file with raw concatenated text</li> <li>Manual tokenization required</li> <li>No metadata or structure</li> </ul>"},{"location":"reference/migration/#new-data-format","title":"New Data Format","text":"<p>New format:</p> <ul> <li>Multiple input formats supported (PDF, DOCX, etc.)</li> <li>Automatic cleaning and preprocessing</li> <li>Structured dataset with metadata</li> <li>Built-in train/validation splitting</li> </ul> <p>Migration example:</p> <pre><code># Legacy data preparation\nwith open('data.txt', 'r') as f:\n    text = f.read()\n\n# Tokenize manually\nimport tiktoken\nenc = tiktoken.get_encoding(\"gpt2\")\ntokens = enc.encode(text)\n\n# New data preparation\nfrom llmbuilder.data import DataLoader, TextDataset\n\n# Load and clean automatically\nloader = DataLoader(clean_text=True, remove_duplicates=True)\ntexts = loader.load_directory(\"raw_data/\")\n\n# Create structured dataset\ndataset = TextDataset(\n    texts,\n    block_size=1024,\n    stride=512,\n    train_split=0.9\n)\n</code></pre>"},{"location":"reference/migration/#model-architecture-migration","title":"\ud83d\udd27 Model Architecture Migration","text":""},{"location":"reference/migration/#legacy-model-definition","title":"Legacy Model Definition","text":"<p>Old way:</p> <pre><code>class GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        # Manual layer definition...\n\n    def forward(self, idx, targets=None):\n        # Manual forward pass...\n</code></pre>"},{"location":"reference/migration/#new-model-definition","title":"New Model Definition","text":"<p>New way:</p> <pre><code># Use built-in model builder\nimport llmbuilder as lb\n\nconfig = lb.load_config(preset=\"gpu_medium\")\nmodel = lb.build_model(config.model)\n\n# Or customize if needed\nfrom llmbuilder.model import GPTModel\nfrom llmbuilder.config import ModelConfig\n\ncustom_config = ModelConfig(\n    vocab_size=32000,\n    num_layers=16,\n    num_heads=16,\n    embedding_dim=1024\n)\n\nmodel = GPTModel(custom_config)\n</code></pre>"},{"location":"reference/migration/#deployment-migration","title":"\ud83d\ude80 Deployment Migration","text":""},{"location":"reference/migration/#legacy-deployment","title":"Legacy Deployment","text":"<p>Old way:</p> <pre><code># Manual model saving/loading\ntorch.save({\n    'model': model.state_dict(),\n    'config': config,\n    'iter_num': iter_num,\n}, 'model.pt')\n\n# Manual inference setup\ncheckpoint = torch.load('model.pt')\nmodel.load_state_dict(checkpoint['model'])\n</code></pre>"},{"location":"reference/migration/#new-deployment","title":"New Deployment","text":"<p>New way:</p> <pre><code># Automatic model management\nfrom llmbuilder.model import save_model, load_model\n\n# Save with metadata\nsave_model(model, \"model.pt\", metadata={\n    \"training_config\": config,\n    \"performance_metrics\": results\n})\n\n# Load with validation\nmodel = load_model(\"model.pt\")\n\n# Export for deployment\nfrom llmbuilder.export import export_gguf\nexport_gguf(\"model.pt\", \"model.gguf\", quantization=\"q4_0\")\n</code></pre>"},{"location":"reference/migration/#testing-migration","title":"\ud83d\udd0d Testing Migration","text":""},{"location":"reference/migration/#verify-migration-success","title":"Verify Migration Success","text":"<p>1. Configuration Test:</p> <pre><code>import llmbuilder as lb\n\n# Test configuration loading\nconfig = lb.load_config(\"migrated_config.json\")\nprint(f\"Config loaded: {config.model.num_layers} layers\")\n\n# Test model building\nmodel = lb.build_model(config.model)\nprint(f\"Model built: {sum(p.numel() for p in model.parameters()):,} parameters\")\n</code></pre> <p>2. Training Test:</p> <pre><code># Test training pipeline\nfrom llmbuilder.data import TextDataset\n\ndataset = TextDataset(\"test_data.txt\", block_size=512)\nresults = lb.train_model(model, dataset, config.training)\nprint(f\"Training completed: {results.final_loss:.4f} final loss\")\n</code></pre> <p>3. Generation Test:</p> <pre><code># Test generation\ntext = lb.generate_text(\n    model_path=\"model.pt\",\n    tokenizer_path=\"tokenizer/\",\n    prompt=\"Test prompt\",\n    max_new_tokens=50\n)\nprint(f\"Generated: {text}\")\n</code></pre>"},{"location":"reference/migration/#common-migration-issues","title":"\ud83d\udea8 Common Migration Issues","text":""},{"location":"reference/migration/#issue-1-configuration-validation-errors","title":"Issue 1: Configuration Validation Errors","text":"<p>Problem: Legacy configs fail validation Solution:</p> <pre><code>from llmbuilder.config import validate_config, fix_config\n\n# Validate and fix automatically\nconfig = lb.load_config(\"legacy_config.json\")\nis_valid, errors = validate_config(config)\n\nif not is_valid:\n    fixed_config = fix_config(config, errors)\n    fixed_config.save(\"fixed_config.json\")\n</code></pre>"},{"location":"reference/migration/#issue-2-model-size-mismatch","title":"Issue 2: Model Size Mismatch","text":"<p>Problem: Tokenizer vocab size doesn't match model Solution:</p> <pre><code># Check and fix vocab size mismatch\nfrom llmbuilder.tokenizer import Tokenizer\n\ntokenizer = Tokenizer.from_pretrained(\"tokenizer/\")\nconfig.model.vocab_size = len(tokenizer)\n</code></pre>"},{"location":"reference/migration/#issue-3-performance-regression","title":"Issue 3: Performance Regression","text":"<p>Problem: New version is slower than legacy Solution:</p> <pre><code># Enable performance optimizations\nconfig.system.compile = True  # PyTorch 2.0 compilation\nconfig.training.mixed_precision = \"fp16\"  # Mixed precision\nconfig.training.dataloader_num_workers = 4  # Parallel data loading\n</code></pre>"},{"location":"reference/migration/#issue-4-generation-quality-differences","title":"Issue 4: Generation Quality Differences","text":"<p>Problem: Generated text quality differs from legacy Solution:</p> <pre><code># Match legacy generation parameters\nfrom llmbuilder.inference import GenerationConfig\n\nlegacy_config = GenerationConfig(\n    temperature=0.8,\n    top_k=200,  # Legacy often used higher top_k\n    do_sample=True,\n    repetition_penalty=1.0  # Legacy default\n)\n\ntext = lb.generate_text(\n    model_path=\"model.pt\",\n    tokenizer_path=\"tokenizer/\",\n    prompt=prompt,\n    config=legacy_config\n)\n</code></pre>"},{"location":"reference/migration/#migration-examples","title":"\ud83d\udcda Migration Examples","text":""},{"location":"reference/migration/#complete-migration-script","title":"Complete Migration Script","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nComplete migration script from legacy LLMBuilder to new package.\n\"\"\"\n\nimport json\nimport shutil\nfrom pathlib import Path\nimport llmbuilder as lb\n\ndef migrate_project(legacy_dir, new_dir):\n    \"\"\"Migrate entire legacy project to new structure.\"\"\"\n\n    legacy_path = Path(legacy_dir)\n    new_path = Path(new_dir)\n    new_path.mkdir(exist_ok=True)\n\n    print(f\"Migrating {legacy_dir} -&gt; {new_dir}\")\n\n    # 1. Migrate configuration\n    legacy_config = legacy_path / \"config.json\"\n    if legacy_config.exists():\n        print(\"Migrating configuration...\")\n        config = lb.load_config(str(legacy_config))\n        config.save(str(new_path / \"config.json\"))\n\n    # 2. Copy and organize data\n    print(\"Organizing data...\")\n    data_dir = new_path / \"data\"\n    data_dir.mkdir(exist_ok=True)\n\n    for data_file in legacy_path.glob(\"*.txt\"):\n        if \"data\" in data_file.name.lower():\n            shutil.copy2(data_file, data_dir / data_file.name)\n\n    # 3. Migrate model if exists\n    model_files = list(legacy_path.glob(\"*.pt\"))\n    if model_files:\n        print(\"Migrating model...\")\n        model_dir = new_path / \"model\"\n        model_dir.mkdir(exist_ok=True)\n\n        for model_file in model_files:\n            shutil.copy2(model_file, model_dir / model_file.name)\n\n    # 4. Create new training script\n    print(\"Creating new training script...\")\n    create_training_script(new_path)\n\n    print(\"Migration completed!\")\n\ndef create_training_script(project_dir):\n    \"\"\"Create new training script using LLMBuilder package.\"\"\"\n\n    script_content = '''#!/usr/bin/env python3\n\"\"\"\nMigrated training script using LLMBuilder package.\n\"\"\"\n\nimport llmbuilder as lb\nfrom llmbuilder.data import TextDataset\n\ndef main():\n    # Load configuration\n    config = lb.load_config(\"config.json\")\n\n    # Build model\n    model = lb.build_model(config.model)\n\n    # Prepare dataset\n    dataset = TextDataset(\n        \"data/training_data.txt\",\n        block_size=config.model.max_seq_length\n    )\n\n    # Train model\n    results = lb.train_model(model, dataset, config.training)\n\n    print(f\"Training completed: {results.final_loss:.4f} final loss\")\n\nif __name__ == \"__main__\":\n    main()\n'''\n\n    script_path = project_dir / \"train.py\"\n    with open(script_path, 'w') as f:\n        f.write(script_content)\n\n# Usage\nif __name__ == \"__main__\":\n    migrate_project(\"legacy_project/\", \"new_project/\")\n</code></pre>"},{"location":"reference/migration/#post-migration-checklist","title":"\u2705 Post-Migration Checklist","text":"<p>After migration, verify these items:</p> <ul> <li>[ ] Configuration loads without errors</li> <li>[ ] Model builds with correct architecture</li> <li>[ ] Training runs successfully</li> <li>[ ] Generation produces expected quality</li> <li>[ ] Performance meets expectations</li> <li>[ ] All CLI commands work</li> <li>[ ] Export functionality works</li> <li>[ ] Tests pass</li> </ul>"},{"location":"reference/migration/#getting-help","title":"\ud83c\udd98 Getting Help","text":"<p>If you encounter issues during migration:</p> <ol> <li>Check the FAQ: Common migration issues are covered</li> <li>GitHub Issues: Search for similar migration problems</li> <li>GitHub Discussions: Ask the community for help</li> <li>Documentation: Review the complete guides</li> <li>Examples: Look at working migration examples</li> </ol> <p>Migration Tips</p> <ul> <li>Start with a small test project before migrating everything</li> <li>Keep backups of your legacy code and models</li> <li>Test thoroughly after migration</li> <li>Take advantage of new features like automatic data cleaning</li> <li>Update your deployment scripts to use the new export functionality</li> </ul>"},{"location":"reference/model-card-template/","title":"Model Card Template","text":"<p>A minimal template for documenting models trained with LLMBuilder.</p>"},{"location":"reference/model-card-template/#model-card-model-name","title":"Model Card: [Model Name]","text":""},{"location":"reference/model-card-template/#basic-info","title":"Basic Info","text":"<ul> <li>Model: [Name] v[Version]</li> <li>Size: [e.g., 125M parameters]</li> <li>Type: [e.g., GPT-style Transformer]</li> <li>License: [e.g., MIT]</li> <li>Date: [Training date]</li> </ul>"},{"location":"reference/model-card-template/#architecture","title":"Architecture","text":"<ul> <li>Layers: [e.g., 12]</li> <li>Hidden Size: [e.g., 768]</li> <li>Vocab Size: [e.g., 32K tokens]</li> <li>Max Length: [e.g., 2048 tokens]</li> </ul>"},{"location":"reference/model-card-template/#training-data","title":"Training Data","text":"<ul> <li>Dataset: [Name and size]</li> <li>Languages: [e.g., English]</li> <li>Domains: [e.g., Web text, books]</li> <li>Processing: [Brief description of cleaning/dedup]</li> </ul>"},{"location":"reference/model-card-template/#performance","title":"Performance","text":"Metric Score Perplexity [e.g., 15.2] [Other metrics] [Values]"},{"location":"reference/model-card-template/#usage","title":"Usage","text":"<pre><code>import llmbuilder as lb\n\nmodel = lb.load_model(\"path/to/model.pt\")\ntokenizer = lb.load_tokenizer(\"path/to/tokenizer/\")\n\ntext = lb.generate_text(\n    model=model,\n    tokenizer=tokenizer,\n    prompt=\"Your prompt here\",\n    max_new_tokens=100\n)\n</code></pre>"},{"location":"reference/model-card-template/#limitations","title":"Limitations","text":"<ul> <li>[Key technical limitations]</li> <li>[Known biases or risks]</li> <li>[Recommended safety measures]</li> </ul>"},{"location":"reference/model-card-template/#files","title":"Files","text":"<ul> <li><code>model.pt</code> - Model weights</li> <li><code>config.json</code> - Configuration</li> <li><code>tokenizer/</code> - Tokenizer files</li> <li><code>model_q4_0.gguf</code> - Quantized version</li> </ul>"},{"location":"reference/model-card-template/#citation","title":"Citation","text":"<pre><code>@misc{[model_name],\n  title={[Model Name]},\n  author={[Your Name]},\n  year={[Year]},\n  note={Trained using LLMBuilder}\n}\n</code></pre>"},{"location":"user-guide/advanced-data-processing/","title":"Advanced Data Processing","text":"<p>LLMBuilder provides sophisticated data processing capabilities designed to handle diverse document formats, perform intelligent deduplication, and prepare high-quality training datasets. This guide covers the advanced features that go beyond basic text processing.</p>"},{"location":"user-guide/advanced-data-processing/#overview","title":"Overview","text":"<p>The advanced data processing system includes:</p> <ul> <li>Multi-format Document Ingestion: Process HTML, Markdown, EPUB, PDF, and plain text files</li> <li>Intelligent Text Extraction: OCR fallback for scanned PDFs, clean HTML parsing, structured document handling</li> <li>Advanced Deduplication: Both exact and semantic duplicate detection with configurable thresholds</li> <li>Flexible Tokenizer Training: Support for multiple algorithms including BPE, SentencePiece, and Unigram</li> <li>GGUF Model Conversion: Convert trained models to GGUF format with various quantization levels</li> <li>Configuration Management: Template-based configuration system with validation</li> </ul>"},{"location":"user-guide/advanced-data-processing/#multi-format-document-ingestion","title":"Multi-Format Document Ingestion","text":""},{"location":"user-guide/advanced-data-processing/#supported-formats","title":"Supported Formats","text":"<p>LLMBuilder can process the following document formats:</p> Format Extension Features HTML <code>.html</code>, <code>.htm</code> Clean text extraction, script/style removal, configurable parsers Markdown <code>.md</code>, <code>.markdown</code> Syntax removal, structure preservation, metadata extraction EPUB <code>.epub</code> Chapter organization, table of contents, metadata handling PDF <code>.pdf</code> Text extraction with OCR fallback, quality assessment Plain Text <code>.txt</code> Direct processing with encoding detection"},{"location":"user-guide/advanced-data-processing/#basic-usage","title":"Basic Usage","text":"<pre><code># Process a single file\nllmbuilder data load --input document.pdf --output processed.txt\n\n# Process a directory of mixed formats\nllmbuilder data load --input documents/ --output dataset.txt --format all\n\n# Enable OCR for scanned PDFs\nllmbuilder data load --input scanned.pdf --output text.txt --enable-ocr\n</code></pre>"},{"location":"user-guide/advanced-data-processing/#configuration-options","title":"Configuration Options","text":"<p>The ingestion system can be configured through the configuration file:</p> <pre><code>{\n  \"data\": {\n    \"ingestion\": {\n      \"supported_formats\": [\"html\", \"markdown\", \"epub\", \"pdf\", \"txt\"],\n      \"batch_size\": 100,\n      \"num_workers\": 4,\n      \"output_format\": \"txt\",\n      \"preserve_structure\": false,\n      \"extract_metadata\": true,\n\n      \"html_parser\": \"lxml\",\n      \"remove_scripts\": true,\n      \"remove_styles\": true,\n\n      \"enable_ocr\": true,\n      \"ocr_quality_threshold\": 0.5,\n      \"ocr_language\": \"eng\",\n      \"pdf_extraction_method\": \"auto\",\n\n      \"extract_toc\": true,\n      \"chapter_separator\": \"\\n\\n---\\n\\n\"\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/advanced-data-processing/#html-processing","title":"HTML Processing","text":"<p>The HTML processor provides clean text extraction with configurable options:</p> <pre><code>from llmbuilder.data.ingest import HTMLProcessor\n\nprocessor = HTMLProcessor(\n    parser='lxml',  # html.parser, lxml, html5lib\n    remove_scripts=True,\n    remove_styles=True\n)\n\ntext = processor.process_file('webpage.html')\n</code></pre> <p>Features:</p> <ul> <li>Multiple parser backends (html.parser, lxml, html5lib)</li> <li>Script and style tag removal</li> <li>Encoding detection and handling</li> <li>Malformed HTML recovery</li> </ul>"},{"location":"user-guide/advanced-data-processing/#pdf-processing-with-ocr","title":"PDF Processing with OCR","text":"<p>The PDF processor includes intelligent OCR fallback:</p> <pre><code>from llmbuilder.data.ingest import PDFProcessor\n\nprocessor = PDFProcessor(\n    enable_ocr=True,\n    ocr_quality_threshold=0.5,\n    ocr_language='eng'\n)\n\ntext = processor.process_file('document.pdf')\n</code></pre> <p>Features:</p> <ul> <li>Primary text extraction using PyMuPDF</li> <li>Quality assessment to trigger OCR</li> <li>OCR fallback using Tesseract</li> <li>Mixed content handling (text + scanned pages)</li> </ul>"},{"location":"user-guide/advanced-data-processing/#epub-processing","title":"EPUB Processing","text":"<p>Extract text from EPUB files with chapter organization:</p> <pre><code>from llmbuilder.data.ingest import EPUBProcessor\n\nprocessor = EPUBProcessor(\n    extract_toc=True,\n    chapter_separator='\\n\\n---\\n\\n'\n)\n\ntext = processor.process_file('book.epub')\n</code></pre> <p>Features:</p> <ul> <li>Chapter-by-chapter extraction</li> <li>Table of contents handling</li> <li>Metadata preservation</li> <li>Multiple EPUB format support</li> </ul>"},{"location":"user-guide/advanced-data-processing/#advanced-deduplication","title":"Advanced Deduplication","text":""},{"location":"user-guide/advanced-data-processing/#exact-deduplication","title":"Exact Deduplication","text":"<p>Remove exact duplicates using normalized hashing:</p> <pre><code># Basic exact deduplication\nllmbuilder data deduplicate --input dataset.txt --output clean_dataset.txt --method exact\n\n# With custom normalization\nllmbuilder data deduplicate --input dataset.txt --output clean_dataset.txt --method exact --normalize\n</code></pre>"},{"location":"user-guide/advanced-data-processing/#semantic-deduplication","title":"Semantic Deduplication","text":"<p>Detect near-duplicates using sentence embeddings:</p> <pre><code># Semantic deduplication with default threshold\nllmbuilder data deduplicate --input dataset.txt --output clean_dataset.txt --method semantic\n\n# Custom similarity threshold\nllmbuilder data deduplicate --input dataset.txt --output clean_dataset.txt --method semantic --threshold 0.9\n\n# Use GPU for faster processing\nllmbuilder data deduplicate --input dataset.txt --output clean_dataset.txt --method semantic --use-gpu\n</code></pre>"},{"location":"user-guide/advanced-data-processing/#combined-deduplication","title":"Combined Deduplication","text":"<p>Use both exact and semantic deduplication:</p> <pre><code>llmbuilder data deduplicate --input dataset.txt --output clean_dataset.txt --method both --threshold 0.85\n</code></pre>"},{"location":"user-guide/advanced-data-processing/#configuration","title":"Configuration","text":"<p>Configure deduplication behavior:</p> <pre><code>{\n  \"data\": {\n    \"deduplication\": {\n      \"enable_exact_deduplication\": true,\n      \"enable_semantic_deduplication\": true,\n      \"similarity_threshold\": 0.85,\n      \"embedding_model\": \"all-MiniLM-L6-v2\",\n      \"batch_size\": 1000,\n      \"chunk_size\": 512,\n      \"min_text_length\": 50,\n      \"normalize_text\": true,\n      \"use_gpu_for_embeddings\": true,\n      \"embedding_cache_size\": 10000,\n      \"similarity_metric\": \"cosine\"\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/advanced-data-processing/#flexible-tokenizer-training","title":"Flexible Tokenizer Training","text":""},{"location":"user-guide/advanced-data-processing/#supported-algorithms","title":"Supported Algorithms","text":"<p>LLMBuilder supports multiple tokenization algorithms:</p> Algorithm Library Best For BPE Hugging Face Tokenizers General purpose, subword tokenization SentencePiece Google SentencePiece Language-agnostic, handles any text Unigram Hugging Face Tokenizers Probabilistic subword segmentation WordPiece Hugging Face Tokenizers BERT-style tokenization"},{"location":"user-guide/advanced-data-processing/#basic-training","title":"Basic Training","text":"<pre><code># Train a BPE tokenizer\nllmbuilder data tokenizer --input corpus.txt --output tokenizer/ --algorithm bpe --vocab-size 16000\n\n# Train a SentencePiece tokenizer\nllmbuilder data tokenizer --input corpus.txt --output tokenizer/ --algorithm sentencepiece --vocab-size 32000\n\n# Train with custom special tokens\nllmbuilder data tokenizer --input corpus.txt --output tokenizer/ --algorithm bpe --vocab-size 16000 --special-tokens \"&lt;pad&gt;,&lt;unk&gt;,&lt;s&gt;,&lt;/s&gt;,&lt;mask&gt;\"\n</code></pre>"},{"location":"user-guide/advanced-data-processing/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>{\n  \"tokenizer_training\": {\n    \"vocab_size\": 32000,\n    \"algorithm\": \"sentencepiece\",\n    \"min_frequency\": 2,\n    \"special_tokens\": [\"&lt;pad&gt;\", \"&lt;unk&gt;\", \"&lt;s&gt;\", \"&lt;/s&gt;\", \"&lt;mask&gt;\"],\n    \"character_coverage\": 0.9995,\n    \"max_sentence_length\": 4192,\n    \"shuffle_input_sentence\": true,\n    \"normalization_rule_name\": \"nmt_nfkc_cf\",\n    \"remove_extra_whitespaces\": true,\n    \"add_dummy_prefix\": true,\n    \"continuing_subword_prefix\": \"##\",\n    \"end_of_word_suffix\": \"\",\n    \"num_threads\": 4,\n    \"max_training_time\": 3600,\n    \"validation_split\": 0.1\n  }\n}\n</code></pre>"},{"location":"user-guide/advanced-data-processing/#validation-and-testing","title":"Validation and Testing","text":"<p>Validate trained tokenizers:</p> <pre><code># Test tokenizer on sample text\nllmbuilder tokenizer test --tokenizer tokenizer/ --text \"Hello world, this is a test.\"\n\n# Benchmark tokenizer performance\nllmbuilder tokenizer benchmark --tokenizer tokenizer/ --test-file test_corpus.txt\n</code></pre>"},{"location":"user-guide/advanced-data-processing/#gguf-model-conversion","title":"GGUF Model Conversion","text":""},{"location":"user-guide/advanced-data-processing/#basic-conversion","title":"Basic Conversion","text":"<p>Convert trained models to GGUF format for inference:</p> <pre><code># Basic conversion with Q8_0 quantization\nllmbuilder convert gguf model/ --output model.gguf\n\n# Specify quantization level\nllmbuilder convert gguf model/ --output model.gguf --quantization Q4_0\n\n# Enable validation\nllmbuilder convert gguf model/ --output model.gguf --quantization Q4_0 --validate\n</code></pre>"},{"location":"user-guide/advanced-data-processing/#batch-conversion","title":"Batch Conversion","text":"<p>Convert multiple models with different quantization levels:</p> <pre><code># Convert with all quantization levels\nllmbuilder convert gguf model/ --output model.gguf --all-quantizations\n\n# Batch convert multiple models\nllmbuilder convert batch --input-dir models/ --output-dir gguf_models/ --quantization Q8_0 Q4_0 Q4_1\n</code></pre>"},{"location":"user-guide/advanced-data-processing/#supported-quantization-levels","title":"Supported Quantization Levels","text":"Level Description Use Case F32 Full precision Maximum quality, large size F16 Half precision Good quality, moderate size Q8_0 8-bit quantization Balanced quality/size Q5_1 5-bit quantization Good compression Q5_0 5-bit quantization Better compression Q4_1 4-bit quantization High compression Q4_0 4-bit quantization Maximum compression"},{"location":"user-guide/advanced-data-processing/#configuration_1","title":"Configuration","text":"<pre><code>{\n  \"gguf_conversion\": {\n    \"quantization_level\": \"Q4_0\",\n    \"validate_output\": true,\n    \"conversion_timeout\": 3600,\n    \"preferred_script\": \"auto\",\n    \"output_naming\": \"quantization_suffix\",\n    \"preserve_metadata\": true\n  }\n}\n</code></pre>"},{"location":"user-guide/advanced-data-processing/#configuration-management","title":"Configuration Management","text":""},{"location":"user-guide/advanced-data-processing/#using-templates","title":"Using Templates","text":"<p>LLMBuilder provides pre-configured templates for common use cases:</p> <pre><code># List available templates\nllmbuilder config templates\n\n# Create config from template\nllmbuilder config from-template basic_config --output my_config.json\n\n# Create with overrides\nllmbuilder config from-template basic_config --output my_config.json \\\n  --override model.vocab_size=24000 \\\n  --override training.batch_size=64\n</code></pre>"},{"location":"user-guide/advanced-data-processing/#available-templates","title":"Available Templates","text":"Template Description Best For <code>basic_config</code> Balanced configuration General use, moderate resources <code>cpu_optimized_config</code> CPU-optimized settings CPU-only training <code>advanced_processing_config</code> Full feature set High-end GPU, large datasets <code>inference_optimized_config</code> Inference settings Production deployment <code>large_scale_config</code> Large model training High-end hardware, large models"},{"location":"user-guide/advanced-data-processing/#configuration-validation","title":"Configuration Validation","text":"<p>Validate configuration files:</p> <pre><code># Basic validation\nllmbuilder config validate my_config.json\n\n# Detailed validation with summary\nllmbuilder config validate my_config.json --detailed\n\n# Show configuration summary\nllmbuilder config summary my_config.json\n</code></pre>"},{"location":"user-guide/advanced-data-processing/#end-to-end-workflows","title":"End-to-End Workflows","text":""},{"location":"user-guide/advanced-data-processing/#complete-data-processing-pipeline","title":"Complete Data Processing Pipeline","text":"<pre><code># 1. Ingest multi-format documents\nllmbuilder data load --input documents/ --output raw_text.txt --format all --clean\n\n# 2. Deduplicate the dataset\nllmbuilder data deduplicate --input raw_text.txt --output clean_text.txt --method both --threshold 0.85\n\n# 3. Train a tokenizer\nllmbuilder data tokenizer --input clean_text.txt --output tokenizer/ --algorithm sentencepiece --vocab-size 32000\n\n# 4. Train the model (using existing training commands)\nllmbuilder train model --data clean_text.txt --tokenizer tokenizer/ --output model/ --config advanced_config.json\n\n# 5. Convert to GGUF format\nllmbuilder convert gguf model/ --output model_q4.gguf --quantization Q4_0 --validate\n</code></pre>"},{"location":"user-guide/advanced-data-processing/#configuration-driven-workflow","title":"Configuration-Driven Workflow","text":"<p>Create a comprehensive configuration file:</p> <pre><code>{\n  \"data\": {\n    \"ingestion\": {\n      \"supported_formats\": [\"html\", \"markdown\", \"epub\", \"pdf\"],\n      \"enable_ocr\": true,\n      \"batch_size\": 200\n    },\n    \"deduplication\": {\n      \"enable_exact_deduplication\": true,\n      \"enable_semantic_deduplication\": true,\n      \"similarity_threshold\": 0.9\n    }\n  },\n  \"tokenizer_training\": {\n    \"algorithm\": \"sentencepiece\",\n    \"vocab_size\": 32000\n  },\n  \"gguf_conversion\": {\n    \"quantization_level\": \"Q4_0\",\n    \"validate_output\": true\n  }\n}\n</code></pre> <p>Then use it throughout the pipeline:</p> <pre><code># All commands will use the same configuration\nllmbuilder data load --config my_config.json --input docs/ --output dataset.txt\nllmbuilder data deduplicate --config my_config.json --input dataset.txt --output clean.txt\nllmbuilder data tokenizer --config my_config.json --input clean.txt --output tokenizer/\n</code></pre>"},{"location":"user-guide/advanced-data-processing/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/advanced-data-processing/#memory-management","title":"Memory Management","text":"<p>For large datasets, optimize memory usage:</p> <pre><code>{\n  \"data\": {\n    \"ingestion\": {\n      \"batch_size\": 50,\n      \"num_workers\": 2\n    },\n    \"deduplication\": {\n      \"batch_size\": 500,\n      \"embedding_cache_size\": 5000,\n      \"use_gpu_for_embeddings\": false\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/advanced-data-processing/#gpu-acceleration","title":"GPU Acceleration","text":"<p>Enable GPU acceleration where available:</p> <pre><code>{\n  \"data\": {\n    \"deduplication\": {\n      \"use_gpu_for_embeddings\": true,\n      \"batch_size\": 2000\n    }\n  },\n  \"system\": {\n    \"device\": \"cuda\",\n    \"mixed_precision\": true\n  }\n}\n</code></pre>"},{"location":"user-guide/advanced-data-processing/#parallel-processing","title":"Parallel Processing","text":"<p>Optimize for multi-core systems:</p> <pre><code>{\n  \"data\": {\n    \"ingestion\": {\n      \"num_workers\": 8\n    }\n  },\n  \"tokenizer_training\": {\n    \"num_threads\": 8\n  },\n  \"system\": {\n    \"num_workers\": 8\n  }\n}\n</code></pre>"},{"location":"user-guide/advanced-data-processing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/advanced-data-processing/#common-issues","title":"Common Issues","text":"<p>OCR Dependencies Missing</p> <pre><code># Install Tesseract\nsudo apt-get install tesseract-ocr  # Ubuntu/Debian\nbrew install tesseract              # macOS\n</code></pre> <p>Memory Issues with Large Files</p> <ul> <li>Reduce batch sizes in configuration</li> <li>Use streaming processing for very large files</li> <li>Enable disk-based caching for embeddings</li> </ul> <p>Slow Semantic Deduplication</p> <ul> <li>Enable GPU acceleration</li> <li>Reduce embedding model size</li> <li>Increase batch size</li> <li>Use embedding caching</li> </ul> <p>GGUF Conversion Failures</p> <ul> <li>Ensure llama.cpp is installed and accessible</li> <li>Check model format compatibility</li> <li>Verify sufficient disk space</li> <li>Use longer conversion timeout</li> </ul>"},{"location":"user-guide/advanced-data-processing/#debug-mode","title":"Debug Mode","text":"<p>Enable verbose logging for troubleshooting:</p> <pre><code># Enable debug logging\nexport LLMBUILDER_LOG_LEVEL=DEBUG\n\n# Run with verbose output\nllmbuilder data load --input docs/ --output dataset.txt --verbose\n</code></pre>"},{"location":"user-guide/advanced-data-processing/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/advanced-data-processing/#data-quality","title":"Data Quality","text":"<ol> <li>Always validate input data before processing</li> <li>Use appropriate OCR thresholds for PDF processing</li> <li>Configure deduplication carefully to avoid over-removal</li> <li>Monitor processing statistics for quality assessment</li> </ol>"},{"location":"user-guide/advanced-data-processing/#performance","title":"Performance","text":"<ol> <li>Use GPU acceleration for semantic deduplication</li> <li>Optimize batch sizes for your hardware</li> <li>Enable parallel processing for multi-core systems</li> <li>Use appropriate quantization levels for your use case</li> </ol>"},{"location":"user-guide/advanced-data-processing/#configuration-management_1","title":"Configuration Management","text":"<ol> <li>Use templates as starting points</li> <li>Validate configurations before use</li> <li>Version control your configuration files</li> <li>Document custom settings and their rationale</li> </ol>"},{"location":"user-guide/advanced-data-processing/#workflow-organization","title":"Workflow Organization","text":"<ol> <li>Process data incrementally for large datasets</li> <li>Keep intermediate results for debugging</li> <li>Use consistent naming conventions for outputs</li> <li>Monitor resource usage during processing</li> </ol> <p>This comprehensive guide covers all aspects of LLMBuilder's advanced data processing capabilities. For specific API documentation, see the API Reference.</p>"},{"location":"user-guide/configuration/","title":"Configuration","text":"<p>LLMBuilder uses a hierarchical configuration system that makes it easy to customize every aspect of your model training and inference. This guide covers all configuration options and best practices.</p>"},{"location":"user-guide/configuration/#configuration-overview","title":"\ud83c\udfaf Configuration Overview","text":"<p>LLMBuilder configurations are organized into logical sections:</p> <pre><code>graph TB\n    A[Configuration] --&gt; B[Model Config]\n    A --&gt; C[Training Config]\n    A --&gt; D[Data Config]\n    A --&gt; E[System Config]\n\n    B --&gt; B1[Architecture]\n    B --&gt; B2[Vocabulary]\n    B --&gt; B3[Sequence Length]\n\n    C --&gt; C1[Learning Rate]\n    C --&gt; C2[Batch Size]\n    C --&gt; C3[Optimization]\n\n    D --&gt; D1[Preprocessing]\n    D --&gt; D2[Tokenization]\n    D --&gt; D3[Validation Split]\n\n    E --&gt; E1[Device]\n    E --&gt; E2[Memory]\n    E --&gt; E3[Logging]</code></pre>"},{"location":"user-guide/configuration/#configuration-methods","title":"\ud83d\udccb Configuration Methods","text":""},{"location":"user-guide/configuration/#method-1-using-templates-recommended","title":"Method 1: Using Templates (Recommended)","text":"<p>LLMBuilder provides pre-configured templates for common use cases:</p> <pre><code># List available templates\nllmbuilder config templates\n\n# Create configuration from template\nllmbuilder config from-template basic_config --output my_config.json\n\n# Create with custom overrides\nllmbuilder config from-template basic_config --output my_config.json \\\n  --override model.vocab_size=24000 \\\n  --override training.batch_size=32\n</code></pre> <p>Available templates:</p> Template Use Case Vocab Size Layers Device Best For <code>basic_config</code> General purpose 16,000 8 auto Balanced training <code>cpu_optimized_config</code> CPU training 8,000 4 cpu Limited resources <code>advanced_processing_config</code> Full features 32,000 12 cuda Large datasets <code>inference_optimized_config</code> Inference only 16,000 8 auto Production deployment <code>large_scale_config</code> Large models 50,000 24 cuda High-end hardware"},{"location":"user-guide/configuration/#method-2-using-code-presets","title":"Method 2: Using Code Presets","text":"<pre><code>from llmbuilder.config.defaults import DefaultConfigs\n\n# Load a preset configuration\nconfig = DefaultConfigs.get_preset(\"cpu_small\")\nconfig = DefaultConfigs.get_preset(\"gpu_medium\")\nconfig = DefaultConfigs.get_preset(\"gpu_large\")\nconfig = DefaultConfigs.get_preset(\"inference\")\n</code></pre>"},{"location":"user-guide/configuration/#method-3-from-configuration-file","title":"Method 3: From Configuration File","text":"<pre><code>from llmbuilder.config.manager import load_config\n\n# Load from JSON file\nconfig = load_config(\"my_config.json\")\n\n# Using the configuration manager directly\nfrom llmbuilder.config.manager import config_manager\nconfig = config_manager.load_config_file(\"my_config.json\")\n</code></pre>"},{"location":"user-guide/configuration/#method-4-programmatic-configuration","title":"Method 4: Programmatic Configuration","text":"<pre><code>from llmbuilder.config import Config, ModelConfig, TrainingConfig\n\nconfig = Config(\n    model=ModelConfig(\n        vocab_size=16000,\n        num_layers=12,\n        num_heads=12,\n        embedding_dim=768,\n        max_seq_length=1024,\n        dropout=0.1\n    ),\n    training=TrainingConfig(\n        batch_size=16,\n        num_epochs=10,\n        learning_rate=3e-4,\n        warmup_steps=1000\n    )\n)\n</code></pre>"},{"location":"user-guide/configuration/#model-configuration","title":"\ud83e\udde0 Model Configuration","text":""},{"location":"user-guide/configuration/#core-architecture-settings","title":"Core Architecture Settings","text":"<pre><code>{\n  \"model\": {\n    \"vocab_size\": 16000,\n    \"num_layers\": 12,\n    \"num_heads\": 12,\n    \"embedding_dim\": 768,\n    \"max_seq_length\": 1024,\n    \"dropout\": 0.1,\n    \"bias\": true,\n    \"model_type\": \"gpt\"\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration/#parameter-explanations","title":"Parameter Explanations","text":"<p><code>vocab_size</code> (int, default: 16000) : Size of the vocabulary. Must match your tokenizer's vocabulary size.</p> <p><code>num_layers</code> (int, default: 12) : Number of transformer layers. More layers = more capacity but slower training.</p> <p><code>num_heads</code> (int, default: 12) : Number of attention heads per layer. Should divide <code>embedding_dim</code> evenly.</p> <p><code>embedding_dim</code> (int, default: 768) : Dimension of token embeddings and hidden states. Larger = more capacity.</p> <p><code>max_seq_length</code> (int, default: 1024) : Maximum sequence length the model can process. Affects memory usage quadratically.</p> <p><code>dropout</code> (float, default: 0.1) : Dropout rate for regularization. Higher values prevent overfitting.</p>"},{"location":"user-guide/configuration/#advanced-model-settings","title":"Advanced Model Settings","text":"<pre><code>{\n  \"model\": {\n    \"activation\": \"gelu\",\n    \"layer_norm_eps\": 1e-5,\n    \"initializer_range\": 0.02,\n    \"use_cache\": true,\n    \"gradient_checkpointing\": false,\n    \"tie_word_embeddings\": true\n  }\n}\n</code></pre> <p><code>activation</code> (str, default: \"gelu\") : Activation function. Options: \"gelu\", \"relu\", \"swish\", \"silu\".</p> <p><code>gradient_checkpointing</code> (bool, default: false) : Trade compute for memory. Enables training larger models with less GPU memory.</p> <p><code>tie_word_embeddings</code> (bool, default: true) : Share input and output embedding weights. Reduces parameters by ~vocab_size * embedding_dim.</p>"},{"location":"user-guide/configuration/#training-configuration","title":"\ud83c\udfcb\ufe0f Training Configuration","text":""},{"location":"user-guide/configuration/#basic-training-settings","title":"Basic Training Settings","text":"<pre><code>{\n  \"training\": {\n    \"batch_size\": 16,\n    \"num_epochs\": 10,\n    \"learning_rate\": 3e-4,\n    \"weight_decay\": 0.01,\n    \"max_grad_norm\": 1.0,\n    \"warmup_steps\": 1000,\n    \"save_every\": 1000,\n    \"eval_every\": 500,\n    \"log_every\": 100\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration/#parameter-explanations_1","title":"Parameter Explanations","text":"<p><code>batch_size</code> (int, default: 16) : Number of samples per training step. Larger batches are more stable but use more memory.</p> <p><code>learning_rate</code> (float, default: 3e-4) : Step size for parameter updates. Too high = unstable, too low = slow convergence.</p> <p><code>weight_decay</code> (float, default: 0.01) : L2 regularization strength. Helps prevent overfitting.</p> <p><code>max_grad_norm</code> (float, default: 1.0) : Gradient clipping threshold. Prevents exploding gradients.</p> <p><code>warmup_steps</code> (int, default: 1000) : Number of steps to linearly increase learning rate from 0 to target value.</p>"},{"location":"user-guide/configuration/#advanced-training-settings","title":"Advanced Training Settings","text":"<pre><code>{\n  \"training\": {\n    \"optimizer\": \"adamw\",\n    \"scheduler\": \"cosine\",\n    \"beta1\": 0.9,\n    \"beta2\": 0.999,\n    \"eps\": 1e-8,\n    \"gradient_accumulation_steps\": 1,\n    \"mixed_precision\": \"fp16\",\n    \"dataloader_num_workers\": 4,\n    \"pin_memory\": true\n  }\n}\n</code></pre> <p><code>optimizer</code> (str, default: \"adamw\") : Optimization algorithm. Options: \"adamw\", \"adam\", \"sgd\", \"adafactor\".</p> <p><code>scheduler</code> (str, default: \"cosine\") : Learning rate schedule. Options: \"cosine\", \"linear\", \"constant\", \"polynomial\".</p> <p><code>mixed_precision</code> (str, default: \"fp16\") : Use mixed precision training. Options: \"fp16\", \"bf16\", \"fp32\".</p> <p><code>gradient_accumulation_steps</code> (int, default: 1) : Accumulate gradients over multiple steps. Effective batch size = batch_size * gradient_accumulation_steps.</p>"},{"location":"user-guide/configuration/#data-configuration","title":"\ud83d\udcca Data Configuration","text":""},{"location":"user-guide/configuration/#basic-data-settings","title":"Basic Data Settings","text":"<pre><code>{\n  \"data\": {\n    \"max_length\": 1024,\n    \"stride\": 512,\n    \"min_length\": 10,\n    \"clean_text\": true,\n    \"remove_duplicates\": true,\n    \"shuffle\": true,\n    \"validation_split\": 0.1,\n    \"test_split\": 0.1\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration/#advanced-data-processing","title":"Advanced Data Processing","text":"<p>LLMBuilder includes sophisticated data processing capabilities:</p> <pre><code>{\n  \"data\": {\n    \"ingestion\": {\n      \"supported_formats\": [\"html\", \"markdown\", \"epub\", \"pdf\", \"txt\"],\n      \"batch_size\": 100,\n      \"num_workers\": 4,\n      \"output_format\": \"txt\",\n      \"preserve_structure\": false,\n      \"extract_metadata\": true,\n\n      \"html_parser\": \"lxml\",\n      \"remove_scripts\": true,\n      \"remove_styles\": true,\n\n      \"enable_ocr\": true,\n      \"ocr_quality_threshold\": 0.5,\n      \"ocr_language\": \"eng\",\n      \"pdf_extraction_method\": \"auto\",\n\n      \"extract_toc\": true,\n      \"chapter_separator\": \"\\n\\n---\\n\\n\"\n    },\n    \"deduplication\": {\n      \"enable_exact_deduplication\": true,\n      \"enable_semantic_deduplication\": true,\n      \"similarity_threshold\": 0.85,\n      \"embedding_model\": \"all-MiniLM-L6-v2\",\n      \"batch_size\": 1000,\n      \"chunk_size\": 512,\n      \"min_text_length\": 50,\n      \"normalize_text\": true,\n      \"use_gpu_for_embeddings\": true,\n      \"embedding_cache_size\": 10000,\n      \"similarity_metric\": \"cosine\"\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration/#data-processing-options","title":"Data Processing Options","text":"<p><code>max_length</code> (int, default: 1024) : Maximum sequence length. Should match model's <code>max_seq_length</code>.</p> <p><code>stride</code> (int, default: 512) : Overlap between consecutive sequences. Larger stride = less overlap = more diverse samples.</p> <p><code>min_length</code> (int, default: 10) : Minimum sequence length to keep. Filters out very short sequences.</p>"},{"location":"user-guide/configuration/#document-ingestion-settings","title":"Document Ingestion Settings","text":"<p><code>supported_formats</code> (list, default: [\"html\", \"markdown\", \"epub\", \"pdf\", \"txt\"]) : File formats to process during ingestion.</p> <p><code>enable_ocr</code> (bool, default: true) : Enable OCR fallback for scanned PDFs.</p> <p><code>ocr_quality_threshold</code> (float, default: 0.5) : Quality threshold to trigger OCR (0.0-1.0).</p>"},{"location":"user-guide/configuration/#deduplication-settings","title":"Deduplication Settings","text":"<p><code>enable_exact_deduplication</code> (bool, default: true) : Remove exact duplicate texts using normalized hashing.</p> <p><code>enable_semantic_deduplication</code> (bool, default: true) : Remove semantically similar texts using embeddings.</p> <p><code>similarity_threshold</code> (float, default: 0.85) : Similarity threshold for semantic deduplication (0.0-1.0).</p> <p><code>use_gpu_for_embeddings</code> (bool, default: true) : Use GPU acceleration for embedding computation.</p>"},{"location":"user-guide/configuration/#tokenizer-training-configuration","title":"\ud83d\udd24 Tokenizer Training Configuration","text":"<p>LLMBuilder supports advanced tokenizer training with multiple algorithms:</p> <pre><code>{\n  \"tokenizer_training\": {\n    \"vocab_size\": 16000,\n    \"algorithm\": \"bpe\",\n    \"min_frequency\": 2,\n    \"special_tokens\": [\"&lt;pad&gt;\", \"&lt;unk&gt;\", \"&lt;s&gt;\", \"&lt;/s&gt;\"],\n    \"character_coverage\": 0.9995,\n    \"max_sentence_length\": 4192,\n    \"shuffle_input_sentence\": true,\n    \"normalization_rule_name\": \"nmt_nfkc_cf\",\n    \"remove_extra_whitespaces\": true,\n    \"add_dummy_prefix\": true,\n    \"continuing_subword_prefix\": \"##\",\n    \"end_of_word_suffix\": \"\",\n    \"num_threads\": 4,\n    \"max_training_time\": 3600,\n    \"validation_split\": 0.1\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration/#tokenizer-algorithm-options","title":"Tokenizer Algorithm Options","text":"<p><code>algorithm</code> (str, default: \"bpe\") : Tokenization algorithm. Options: \"bpe\", \"unigram\", \"wordpiece\", \"sentencepiece\".</p> <p><code>vocab_size</code> (int, default: 16000) : Target vocabulary size. Must match model's vocab_size.</p> <p><code>min_frequency</code> (int, default: 2) : Minimum frequency for subword units to be included.</p> <p><code>character_coverage</code> (float, default: 0.9995) : Character coverage for SentencePiece (0.0-1.0).</p>"},{"location":"user-guide/configuration/#algorithm-specific-settings","title":"Algorithm-Specific Settings","text":"<p>For BPE and WordPiece:</p> <ul> <li><code>continuing_subword_prefix</code>: Prefix for continuing subwords (e.g., \"##\")</li> <li><code>end_of_word_suffix</code>: Suffix for end-of-word tokens</li> </ul> <p>For SentencePiece:</p> <ul> <li><code>normalization_rule_name</code>: Text normalization rule</li> <li><code>add_dummy_prefix</code>: Add dummy prefix for better tokenization</li> <li><code>shuffle_input_sentence</code>: Shuffle input during training</li> </ul>"},{"location":"user-guide/configuration/#gguf-conversion-configuration","title":"\ud83d\udd04 GGUF Conversion Configuration","text":"<p>Configure model conversion to GGUF format for inference:</p> <pre><code>{\n  \"gguf_conversion\": {\n    \"quantization_level\": \"Q8_0\",\n    \"validate_output\": true,\n    \"conversion_timeout\": 3600,\n    \"preferred_script\": \"auto\",\n    \"script_paths\": {},\n    \"output_naming\": \"auto\",\n    \"custom_suffix\": \"\",\n    \"preserve_metadata\": true\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration/#quantization-options","title":"Quantization Options","text":"<p><code>quantization_level</code> (str, default: \"Q8_0\") : Quantization level. Options: \"F32\", \"F16\", \"Q8_0\", \"Q5_1\", \"Q5_0\", \"Q4_1\", \"Q4_0\".</p> Level Precision Size Quality Use Case F32 32-bit Largest Highest Research, debugging F16 16-bit Large High High-quality inference Q8_0 8-bit Medium Good Balanced quality/size Q5_1 5-bit Small Fair Mobile deployment Q4_0 4-bit Smallest Lower Edge devices"},{"location":"user-guide/configuration/#conversion-settings","title":"Conversion Settings","text":"<p><code>validate_output</code> (bool, default: true) : Validate converted GGUF files for integrity.</p> <p><code>preferred_script</code> (str, default: \"auto\") : Preferred conversion script. Options: \"auto\", \"llama_cpp\", \"convert_hf_to_gguf\".</p> <p><code>conversion_timeout</code> (int, default: 3600) : Maximum time (seconds) to wait for conversion.</p> <p><code>output_naming</code> (str, default: \"auto\") : Output file naming strategy. Options: \"auto\", \"quantization_suffix\", \"custom\".</p>"},{"location":"user-guide/configuration/#system-configuration","title":"\ud83d\udda5\ufe0f System Configuration","text":"<pre><code>{\n  \"system\": {\n    \"device\": \"auto\",\n    \"num_gpus\": 1,\n    \"distributed\": false,\n    \"compile\": false,\n    \"seed\": 42,\n    \"deterministic\": false,\n    \"benchmark\": true,\n    \"cache_dir\": \"./cache\",\n    \"output_dir\": \"./output\",\n    \"logging_level\": \"INFO\"\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration/#device-and-performance-settings","title":"Device and Performance Settings","text":"<p><code>device</code> (str, default: \"auto\") : Device to use. Options: \"auto\", \"cpu\", \"cuda\", \"cuda:0\", etc.</p> <p><code>compile</code> (bool, default: false) : Use PyTorch 2.0 compilation for faster training (experimental).</p> <p><code>benchmark</code> (bool, default: true) : Enable cuDNN benchmarking for consistent input sizes.</p>"},{"location":"user-guide/configuration/#creating-custom-configurations","title":"\ud83d\udee0\ufe0f Creating Custom Configurations","text":""},{"location":"user-guide/configuration/#using-the-cli","title":"Using the CLI","text":"<pre><code># Interactive configuration creation\nllmbuilder config create --interactive\n\n# Create from preset and customize\nllmbuilder config create --preset cpu_small --output my_config.json\n</code></pre>"},{"location":"user-guide/configuration/#programmatic-creation","title":"Programmatic Creation","text":"<pre><code>from llmbuilder.config import Config, ModelConfig, TrainingConfig\n\n# Create custom configuration\nconfig = Config(\n    model=ModelConfig(\n        vocab_size=32000,  # Larger vocabulary\n        num_layers=24,     # Deeper model\n        num_heads=16,      # More attention heads\n        embedding_dim=1024, # Larger embeddings\n        max_seq_length=2048, # Longer sequences\n        dropout=0.1\n    ),\n    training=TrainingConfig(\n        batch_size=8,      # Smaller batch for larger model\n        num_epochs=20,     # More training\n        learning_rate=1e-4, # Lower learning rate\n        warmup_steps=2000,  # Longer warmup\n        gradient_accumulation_steps=4  # Effective batch size = 32\n    )\n)\n\n# Save configuration\nconfig.save(\"custom_config.json\")\n</code></pre>"},{"location":"user-guide/configuration/#configuration-inheritance","title":"Configuration Inheritance","text":"<pre><code># Start with a preset\nbase_config = lb.load_config(preset=\"gpu_medium\")\n\n# Modify specific settings\nbase_config.model.num_layers = 18\nbase_config.training.learning_rate = 1e-4\nbase_config.training.batch_size = 12\n\n# Save modified configuration\nbase_config.save(\"modified_config.json\")\n</code></pre>"},{"location":"user-guide/configuration/#configuration-validation","title":"\ud83d\udd27 Configuration Validation","text":""},{"location":"user-guide/configuration/#cli-validation-recommended","title":"CLI Validation (Recommended)","text":"<pre><code># Basic validation\nllmbuilder config validate my_config.json\n\n# Detailed validation with summary\nllmbuilder config validate my_config.json --detailed\n\n# Show configuration summary\nllmbuilder config summary my_config.json\n</code></pre>"},{"location":"user-guide/configuration/#programmatic-validation","title":"Programmatic Validation","text":"<pre><code>from llmbuilder.config.manager import validate_config, config_manager\n\n# Simple validation\nis_valid = validate_config(\"my_config.json\")\n\n# Detailed validation\nresult = config_manager.validate_config_file(\"my_config.json\")\nif result[\"valid\"]:\n    print(\"Configuration is valid!\")\n    print(\"Summary:\", result[\"config_summary\"])\nelse:\n    print(\"Validation errors:\")\n    for error in result[\"errors\"]:\n        print(f\"  - {error}\")\n</code></pre>"},{"location":"user-guide/configuration/#automatic-validation","title":"Automatic Validation","text":"<p>LLMBuilder automatically validates configurations when loading:</p> <pre><code>from llmbuilder.config.manager import load_config\n\ntry:\n    config = load_config(\"my_config.json\")\n    print(\"Configuration loaded and validated successfully!\")\nexcept ValueError as e:\n    print(f\"Configuration validation failed: {e}\")\n</code></pre>"},{"location":"user-guide/configuration/#common-validation-errors","title":"Common Validation Errors","text":"<p>Common Issues</p> <ul> <li><code>num_heads</code> doesn't divide <code>embedding_dim</code>: embedding_dim must be divisible by num_heads</li> <li><code>vocab_size</code> mismatch: Must match tokenizer vocabulary size</li> <li>Memory constraints: batch_size \u00d7 max_seq_length \u00d7 embedding_dim too large for available memory</li> <li>Invalid device: Specified device not available</li> </ul>"},{"location":"user-guide/configuration/#performance-tuning","title":"\ud83d\udcc8 Performance Tuning","text":""},{"location":"user-guide/configuration/#memory-optimization","title":"Memory Optimization","text":"<pre><code>{\n  \"model\": {\n    \"gradient_checkpointing\": true,\n    \"tie_word_embeddings\": true\n  },\n  \"training\": {\n    \"mixed_precision\": \"fp16\",\n    \"gradient_accumulation_steps\": 4,\n    \"batch_size\": 4\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration/#speed-optimization","title":"Speed Optimization","text":"<pre><code>{\n  \"system\": {\n    \"compile\": true,\n    \"benchmark\": true\n  },\n  \"training\": {\n    \"dataloader_num_workers\": 8,\n    \"pin_memory\": true\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration/#quality-optimization","title":"Quality Optimization","text":"<pre><code>{\n  \"model\": {\n    \"num_layers\": 24,\n    \"embedding_dim\": 1024,\n    \"dropout\": 0.1\n  },\n  \"training\": {\n    \"learning_rate\": 1e-4,\n    \"warmup_steps\": 2000,\n    \"weight_decay\": 0.01\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration/#configuration-best-practices","title":"\ud83c\udfaf Configuration Best Practices","text":""},{"location":"user-guide/configuration/#1-start-with-presets","title":"1. Start with Presets","text":"<p>Always start with a preset that matches your hardware:</p> <pre><code># For CPU development\nconfig = lb.load_config(preset=\"cpu_small\")\n\n# For single GPU\nconfig = lb.load_config(preset=\"gpu_medium\")\n\n# For multiple GPUs\nconfig = lb.load_config(preset=\"gpu_large\")\n</code></pre>"},{"location":"user-guide/configuration/#2-scale-gradually","title":"2. Scale Gradually","text":"<p>When increasing model size, scale parameters proportionally:</p> <pre><code># If doubling embedding_dim, consider:\nconfig.model.embedding_dim *= 2\nconfig.model.num_heads *= 2  # Keep head_dim constant\nconfig.training.learning_rate *= 0.7  # Reduce LR for larger models\nconfig.training.warmup_steps *= 2  # Longer warmup\n</code></pre>"},{"location":"user-guide/configuration/#3-monitor-memory-usage","title":"3. Monitor Memory Usage","text":"<pre><code># Check memory requirements before training\nfrom llmbuilder.utils import estimate_memory_usage\n\nmemory_gb = estimate_memory_usage(config)\nprint(f\"Estimated memory usage: {memory_gb:.1f} GB\")\n</code></pre>"},{"location":"user-guide/configuration/#4-use-configuration-templates","title":"4. Use Configuration Templates","text":"<p>Create templates for common scenarios:</p> <pre><code># templates/research_config.py\ndef get_research_config(vocab_size, dataset_size):\n    \"\"\"Configuration optimized for research experiments.\"\"\"\n    return Config(\n        model=ModelConfig(\n            vocab_size=vocab_size,\n            num_layers=12 if dataset_size &lt; 1e6 else 24,\n            embedding_dim=768,\n            max_seq_length=1024,\n            dropout=0.1\n        ),\n        training=TrainingConfig(\n            batch_size=16,\n            learning_rate=3e-4,\n            num_epochs=10 if dataset_size &lt; 1e6 else 20,\n            warmup_steps=min(1000, dataset_size // 100)\n        )\n    )\n</code></pre>"},{"location":"user-guide/configuration/#debugging-configuration-issues","title":"\ud83d\udd0d Debugging Configuration Issues","text":""},{"location":"user-guide/configuration/#enable-verbose-logging","title":"Enable Verbose Logging","text":"<pre><code>{\n  \"system\": {\n    \"logging_level\": \"DEBUG\"\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration/#check-configuration-summary","title":"Check Configuration Summary","text":"<pre><code>config = lb.load_config(\"my_config.json\")\nprint(config.summary())\n</code></pre>"},{"location":"user-guide/configuration/#validate-against-hardware","title":"Validate Against Hardware","text":"<pre><code>llmbuilder config validate my_config.json --check-hardware\n</code></pre>"},{"location":"user-guide/configuration/#configuration-examples","title":"\ud83d\udcda Configuration Examples","text":""},{"location":"user-guide/configuration/#minimal-cpu-configuration","title":"Minimal CPU Configuration","text":"<pre><code>{\n  \"model\": {\n    \"vocab_size\": 8000,\n    \"num_layers\": 4,\n    \"num_heads\": 4,\n    \"embedding_dim\": 256,\n    \"max_seq_length\": 512\n  },\n  \"training\": {\n    \"batch_size\": 4,\n    \"num_epochs\": 5,\n    \"learning_rate\": 1e-3\n  },\n  \"data\": {\n    \"ingestion\": {\n      \"supported_formats\": [\"txt\"],\n      \"num_workers\": 2,\n      \"enable_ocr\": false\n    },\n    \"deduplication\": {\n      \"enable_semantic_deduplication\": false,\n      \"use_gpu_for_embeddings\": false\n    }\n  },\n  \"tokenizer_training\": {\n    \"algorithm\": \"bpe\",\n    \"vocab_size\": 8000,\n    \"num_threads\": 2\n  },\n  \"system\": {\n    \"device\": \"cpu\"\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration/#high-performance-gpu-configuration","title":"High-Performance GPU Configuration","text":"<pre><code>{\n  \"model\": {\n    \"vocab_size\": 32000,\n    \"num_layers\": 24,\n    \"num_heads\": 16,\n    \"embedding_dim\": 1024,\n    \"max_seq_length\": 2048,\n    \"gradient_checkpointing\": true\n  },\n  \"training\": {\n    \"batch_size\": 8,\n    \"num_epochs\": 20,\n    \"learning_rate\": 1e-4,\n    \"mixed_precision\": \"fp16\",\n    \"gradient_accumulation_steps\": 4\n  },\n  \"data\": {\n    \"ingestion\": {\n      \"supported_formats\": [\"html\", \"markdown\", \"epub\", \"pdf\", \"txt\"],\n      \"batch_size\": 200,\n      \"num_workers\": 8,\n      \"enable_ocr\": true,\n      \"ocr_quality_threshold\": 0.7\n    },\n    \"deduplication\": {\n      \"enable_exact_deduplication\": true,\n      \"enable_semantic_deduplication\": true,\n      \"similarity_threshold\": 0.9,\n      \"use_gpu_for_embeddings\": true,\n      \"batch_size\": 2000\n    }\n  },\n  \"tokenizer_training\": {\n    \"algorithm\": \"sentencepiece\",\n    \"vocab_size\": 32000,\n    \"character_coverage\": 0.9998,\n    \"num_threads\": 8\n  },\n  \"gguf_conversion\": {\n    \"quantization_level\": \"Q4_0\",\n    \"validate_output\": true,\n    \"preferred_script\": \"llama_cpp\"\n  },\n  \"system\": {\n    \"device\": \"cuda\",\n    \"compile\": true\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration/#advanced-data-processing-configuration","title":"Advanced Data Processing Configuration","text":"<pre><code>{\n  \"data\": {\n    \"max_length\": 2048,\n    \"ingestion\": {\n      \"supported_formats\": [\"html\", \"markdown\", \"epub\", \"pdf\"],\n      \"batch_size\": 500,\n      \"num_workers\": 16,\n      \"output_format\": \"jsonl\",\n      \"preserve_structure\": true,\n      \"extract_metadata\": true,\n      \"html_parser\": \"lxml\",\n      \"enable_ocr\": true,\n      \"ocr_quality_threshold\": 0.8,\n      \"pdf_extraction_method\": \"auto\"\n    },\n    \"deduplication\": {\n      \"enable_exact_deduplication\": true,\n      \"enable_semantic_deduplication\": true,\n      \"similarity_threshold\": 0.92,\n      \"embedding_model\": \"all-MiniLM-L6-v2\",\n      \"batch_size\": 5000,\n      \"chunk_size\": 2048,\n      \"min_text_length\": 200,\n      \"use_gpu_for_embeddings\": true,\n      \"embedding_cache_size\": 50000\n    }\n  },\n  \"tokenizer_training\": {\n    \"algorithm\": \"sentencepiece\",\n    \"vocab_size\": 50000,\n    \"min_frequency\": 5,\n    \"special_tokens\": [\"&lt;pad&gt;\", \"&lt;unk&gt;\", \"&lt;s&gt;\", \"&lt;/s&gt;\", \"&lt;mask&gt;\", \"&lt;cls&gt;\", \"&lt;sep&gt;\"],\n    \"character_coverage\": 0.9998,\n    \"max_sentence_length\": 16384,\n    \"num_threads\": 16,\n    \"max_training_time\": 14400\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration/#inference-optimized-configuration","title":"Inference-Optimized Configuration","text":"<pre><code>{\n  \"model\": {\n    \"vocab_size\": 16000,\n    \"num_layers\": 8,\n    \"num_heads\": 8,\n    \"embedding_dim\": 512,\n    \"dropout\": 0.0\n  },\n  \"inference\": {\n    \"max_new_tokens\": 256,\n    \"temperature\": 0.7,\n    \"top_k\": 40,\n    \"top_p\": 0.9,\n    \"repetition_penalty\": 1.1\n  },\n  \"gguf_conversion\": {\n    \"quantization_level\": \"Q4_0\",\n    \"validate_output\": true,\n    \"output_naming\": \"quantization_suffix\"\n  },\n  \"system\": {\n    \"device\": \"auto\",\n    \"compile\": true,\n    \"num_workers\": 1\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration/#fine-tuning-configuration","title":"Fine-tuning Configuration","text":"<pre><code>{\n  \"training\": {\n    \"learning_rate\": 5e-5,\n    \"num_epochs\": 3,\n    \"warmup_steps\": 100,\n    \"weight_decay\": 0.01,\n    \"save_every\": 100\n  },\n  \"data\": {\n    \"max_length\": 512,\n    \"stride\": 256,\n    \"deduplication\": {\n      \"enable_exact_deduplication\": true,\n      \"enable_semantic_deduplication\": false\n    }\n  },\n  \"tokenizer_training\": {\n    \"algorithm\": \"bpe\",\n    \"min_frequency\": 1,\n    \"max_training_time\": 600\n  }\n}\n</code></pre> <p>Configuration Tips</p> <ul> <li>Always validate your configuration before training</li> <li>Start with presets and modify incrementally</li> <li>Monitor memory usage and adjust batch size accordingly</li> <li>Use gradient accumulation to simulate larger batch sizes</li> <li>Save successful configurations for future use</li> </ul>"},{"location":"user-guide/data-processing/","title":"Data Processing","text":"<p>Data is the foundation of any successful language model. LLMBuilder provides comprehensive tools for loading, cleaning, and preparing text data from various sources, including advanced multi-format ingestion, semantic deduplication, and intelligent text normalization. This guide covers everything from basic text files to complex document processing pipelines.</p>"},{"location":"user-guide/data-processing/#overview","title":"\ud83c\udfaf Overview","text":"<p>LLMBuilder's data processing pipeline handles:</p> <pre><code>graph LR\n    A[Multi-Format Documents] --&gt; B[Ingestion Pipeline]\n    B --&gt; C[Text Normalization]\n    C --&gt; D[Deduplication Engine]\n    D --&gt; E[Dataset Creator]\n    E --&gt; F[Training Ready]\n\n    A1[HTML] --&gt; B\n    A2[Markdown] --&gt; B\n    A3[EPUB] --&gt; B\n    A4[PDF + OCR] --&gt; B\n    A5[TXT] --&gt; B\n\n    D1[Exact Dedup] --&gt; D\n    D2[Semantic Dedup] --&gt; D\n\n    style A fill:#e1f5fe\n    style D fill:#f3e5f5\n    style F fill:#e8f5e8</code></pre>"},{"location":"user-guide/data-processing/#supported-file-formats","title":"\ud83d\udcc1 Supported File Formats","text":"<p>LLMBuilder can process various document formats:</p> Format Extension Description Features Quality Plain Text <code>.txt</code> Raw text files Direct processing \u2b50\u2b50\u2b50\u2b50\u2b50 HTML <code>.html</code>, <code>.htm</code> Web pages Tag removal, structure preservation \u2b50\u2b50\u2b50\u2b50 Markdown <code>.md</code> Markdown files Syntax parsing, link extraction \u2b50\u2b50\u2b50\u2b50\u2b50 EPUB <code>.epub</code> E-book format Chapter extraction, metadata \u2b50\u2b50\u2b50\u2b50\u2b50 PDF <code>.pdf</code> Portable Document Format OCR fallback, layout preservation \u2b50\u2b50\u2b50\u2b50"},{"location":"user-guide/data-processing/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"user-guide/data-processing/#advanced-multi-format-ingestion","title":"Advanced Multi-Format Ingestion","text":"<pre><code># Process all supported formats with advanced features\nllmbuilder data load \\\n  --input ./documents \\\n  --output processed_text.txt \\\n  --format all \\\n  --clean \\\n  --enable-ocr \\\n  --extract-metadata\n</code></pre>"},{"location":"user-guide/data-processing/#deduplication-pipeline","title":"Deduplication Pipeline","text":"<pre><code># Remove exact and semantic duplicates\nllmbuilder data deduplicate \\\n  --input processed_text.txt \\\n  --output clean_text.txt \\\n  --exact \\\n  --semantic \\\n  --threshold 0.85\n</code></pre>"},{"location":"user-guide/data-processing/#advanced-ingestion-pipeline","title":"Advanced Ingestion Pipeline","text":"<pre><code>from llmbuilder.data.ingest import IngestionPipeline\nfrom llmbuilder.data.dedup import DeduplicationPipeline\n\n# Initialize advanced ingestion pipeline\npipeline = IngestionPipeline(\n    supported_formats=['html', 'markdown', 'epub', 'pdf', 'txt'],\n    batch_size=100,\n    num_workers=4,\n    enable_ocr=True,\n    ocr_quality_threshold=0.7,\n    extract_metadata=True\n)\n\n# Process entire directory\nresults = pipeline.process_directory(\"./raw_documents\", \"./processed\")\n\nprint(f\"Processed {results.total_files} files\")\nprint(f\"Success: {results.successful_files}\")\nprint(f\"Errors: {results.failed_files}\")\n\n# Advanced deduplication\ndedup_pipeline = DeduplicationPipeline(\n    enable_exact_deduplication=True,\n    enable_semantic_deduplication=True,\n    similarity_threshold=0.85,\n    embedding_model=\"all-MiniLM-L6-v2\"\n)\n\n# Remove duplicates\ndedup_results = dedup_pipeline.process_file(\n    \"./processed/combined.txt\",\n    \"./clean/deduplicated.txt\"\n)\n\nprint(f\"Original lines: {dedup_results.original_count}\")\nprint(f\"After deduplication: {dedup_results.final_count}\")\nprint(f\"Removed: {dedup_results.removed_count} ({dedup_results.removal_percentage:.1f}%)\")\n</code></pre>"},{"location":"user-guide/data-processing/#advanced-data-processing-features","title":"\ud83d\udd04 Advanced Data Processing Features","text":""},{"location":"user-guide/data-processing/#multi-format-document-ingestion","title":"Multi-Format Document Ingestion","text":"<p>LLMBuilder's advanced ingestion pipeline can process multiple document formats simultaneously with intelligent content extraction:</p> <pre><code>from llmbuilder.data.ingest import IngestionPipeline\n\n# Configure advanced ingestion\nconfig = {\n    \"supported_formats\": [\"html\", \"markdown\", \"epub\", \"pdf\", \"txt\"],\n    \"batch_size\": 200,\n    \"num_workers\": 8,\n    \"output_format\": \"jsonl\",  # or \"txt\"\n    \"preserve_structure\": True,\n    \"extract_metadata\": True,\n\n    # HTML-specific settings\n    \"html_parser\": \"lxml\",\n    \"remove_scripts\": True,\n    \"remove_styles\": True,\n\n    # PDF-specific settings\n    \"enable_ocr\": True,\n    \"ocr_quality_threshold\": 0.7,\n    \"ocr_language\": \"eng\",\n\n    # EPUB-specific settings\n    \"extract_toc\": True,\n    \"chapter_separator\": \"\\n\\n---\\n\\n\"\n}\n\npipeline = IngestionPipeline(**config)\nresults = pipeline.process_directory(\"./documents\", \"./processed\")\n</code></pre>"},{"location":"user-guide/data-processing/#intelligent-deduplication","title":"Intelligent Deduplication","text":"<p>Remove both exact and semantic duplicates with configurable similarity thresholds:</p> <pre><code>from llmbuilder.data.dedup import DeduplicationPipeline\n\n# Configure deduplication\ndedup_config = {\n    \"enable_exact_deduplication\": True,\n    \"enable_semantic_deduplication\": True,\n    \"similarity_threshold\": 0.9,\n    \"embedding_model\": \"all-MiniLM-L6-v2\",\n    \"batch_size\": 2000,\n    \"chunk_size\": 1024,\n    \"min_text_length\": 100,\n    \"use_gpu_for_embeddings\": True,\n    \"similarity_metric\": \"cosine\"\n}\n\ndedup = DeduplicationPipeline(**dedup_config)\n\n# Process with detailed statistics\nresults = dedup.process_file(\n    input_file=\"./processed/raw_text.txt\",\n    output_file=\"./clean/deduplicated.txt\"\n)\n\nprint(f\"Deduplication Results:\")\nprint(f\"  Original: {results.original_count:,} lines\")\nprint(f\"  Exact duplicates removed: {results.exact_duplicates:,}\")\nprint(f\"  Semantic duplicates removed: {results.semantic_duplicates:,}\")\nprint(f\"  Final: {results.final_count:,} lines\")\nprint(f\"  Reduction: {results.removal_percentage:.1f}%\")\n</code></pre>"},{"location":"user-guide/data-processing/#text-normalization","title":"Text Normalization","text":"<p>Advanced text normalization with configurable rules:</p> <pre><code>from llmbuilder.data.normalize import TextNormalizer\n\nnormalizer = TextNormalizer(\n    normalize_whitespace=True,\n    fix_encoding_issues=True,\n    remove_control_characters=True,\n    normalize_quotes=True,\n    normalize_dashes=True,\n    normalize_unicode=True\n)\n\n# Normalize text with statistics\nnormalized_text, stats = normalizer.normalize_with_stats(raw_text)\n\nprint(f\"Normalization applied {stats.total_changes} changes:\")\nfor rule, count in stats.rule_applications.items():\n    print(f\"  {rule}: {count} changes\")\n</code></pre>"},{"location":"user-guide/data-processing/#file-format-specifics","title":"\ud83d\udcc4 File Format Specifics","text":""},{"location":"user-guide/data-processing/#html-processing","title":"HTML Processing","text":"<p>HTML documents are processed with intelligent content extraction:</p> <pre><code>from llmbuilder.data.processors import HTMLProcessor\n\nprocessor = HTMLProcessor(\n    parser=\"lxml\",              # html.parser, lxml, html5lib\n    remove_scripts=True,        # Remove JavaScript\n    remove_styles=True,         # Remove CSS\n    preserve_links=False,       # Extract link text only\n    extract_tables=True,        # Convert tables to text\n    min_text_length=20         # Filter short elements\n)\n\ntext = processor.process_file(\"webpage.html\")\n</code></pre>"},{"location":"user-guide/data-processing/#markdown-processing","title":"Markdown Processing","text":"<p>Markdown files are converted to clean text while preserving structure:</p> <pre><code>from llmbuilder.data.processors import MarkdownProcessor\n\nprocessor = MarkdownProcessor(\n    preserve_code_blocks=True,   # Keep code formatting\n    extract_links=True,          # Extract link URLs\n    preserve_tables=True,        # Convert tables to text\n    remove_html_tags=True        # Strip embedded HTML\n)\n\ntext = processor.process_file(\"document.md\")\n</code></pre>"},{"location":"user-guide/data-processing/#epub-processing","title":"EPUB Processing","text":"<p>E-books are processed with chapter-aware extraction:</p> <pre><code>from llmbuilder.data.processors import EPUBProcessor\n\nprocessor = EPUBProcessor(\n    extract_toc=True,           # Include table of contents\n    chapter_separator=\"\\n\\n---\\n\\n\",  # Chapter delimiter\n    include_metadata=True,       # Extract book metadata\n    preserve_formatting=False    # Convert to plain text\n)\n\ntext = processor.process_file(\"book.epub\")\n</code></pre>"},{"location":"user-guide/data-processing/#pdf-processing","title":"PDF Processing","text":"<p>PDFs are processed with OCR fallback for scanned documents:</p> <pre><code>from llmbuilder.data.processors import PDFProcessor\n\nprocessor = PDFProcessor(\n    enable_ocr=True,            # OCR for scanned PDFs\n    ocr_quality_threshold=0.5,  # Quality threshold for OCR\n    ocr_language=\"eng\",         # OCR language\n    preserve_layout=True,       # Maintain document structure\n    extract_images=False        # Skip image text extraction\n)\n\ntext = processor.process_file(\"document.pdf\")\n</code></pre>"},{"location":"user-guide/data-processing/#pdf-processing-options","title":"PDF Processing Options","text":"<pre><code>pdf_options = {\n    \"extract_images\": False,        # Extract text from images (OCR)\n    \"preserve_layout\": True,        # Keep paragraph structure\n    \"min_font_size\": 8,            # Minimum font size to extract\n    \"ignore_headers_footers\": True, # Skip headers/footers\n    \"page_range\": (1, 10),         # Extract specific pages\n    \"password\": None               # PDF password if needed\n}\n</code></pre>"},{"location":"user-guide/data-processing/#word-document-processing","title":"Word Document Processing","text":"<p>DOCX files are processed with full formatting awareness:</p> <pre><code>loader = DataLoader(\n    docx_options={\n        \"include_tables\": True,     # Extract table content\n        \"include_headers\": False,   # Skip headers\n        \"include_footers\": False,   # Skip footers\n        \"preserve_formatting\": True # Keep basic formatting\n    }\n)\n</code></pre>"},{"location":"user-guide/data-processing/#html-processing_1","title":"HTML Processing","text":"<p>HTML content is cleaned and converted to plain text:</p> <pre><code>loader = DataLoader(\n    html_options={\n        \"remove_scripts\": True,     # Remove JavaScript\n        \"remove_styles\": True,      # Remove CSS\n        \"preserve_links\": False,    # Keep link text only\n        \"extract_tables\": True,     # Convert tables to text\n        \"min_text_length\": 20      # Filter short elements\n    }\n)\n</code></pre>"},{"location":"user-guide/data-processing/#text-cleaning","title":"\ud83e\uddf9 Text Cleaning","text":"<p>LLMBuilder includes comprehensive text cleaning capabilities:</p>"},{"location":"user-guide/data-processing/#automatic-cleaning","title":"Automatic Cleaning","text":"<pre><code>from llmbuilder.data import TextCleaner\n\ncleaner = TextCleaner(\n    normalize_whitespace=True,      # Fix spacing issues\n    remove_special_chars=False,     # Keep punctuation\n    fix_encoding=True,              # Fix encoding issues\n    remove_urls=True,               # Remove web URLs\n    remove_emails=True,             # Remove email addresses\n    remove_phone_numbers=True,      # Remove phone numbers\n    min_sentence_length=10,         # Filter short sentences\n    max_sentence_length=1000,       # Filter very long sentences\n    remove_duplicates=True,         # Remove duplicate sentences\n    language_filter=\"en\"            # Keep only English text\n)\n\ncleaned_text = cleaner.clean(raw_text)\n</code></pre>"},{"location":"user-guide/data-processing/#custom-cleaning-rules","title":"Custom Cleaning Rules","text":"<pre><code># Define custom cleaning function\ndef custom_cleaner(text):\n    # Remove specific patterns\n    import re\n    text = re.sub(r'\\[.*?\\]', '', text)  # Remove bracketed content\n    text = re.sub(r'\\d{4}-\\d{2}-\\d{2}', '', text)  # Remove dates\n    return text\n\n# Apply custom cleaning\ncleaner = TextCleaner(custom_functions=[custom_cleaner])\ncleaned_text = cleaner.clean(raw_text)\n</code></pre>"},{"location":"user-guide/data-processing/#cleaning-statistics","title":"Cleaning Statistics","text":"<pre><code>stats = cleaner.get_stats()\nprint(f\"Original length: {stats.original_length:,} characters\")\nprint(f\"Cleaned length: {stats.cleaned_length:,} characters\")\nprint(f\"Removed: {stats.removed_length:,} characters ({stats.removal_percentage:.1f}%)\")\nprint(f\"Sentences: {stats.sentence_count}\")\nprint(f\"Paragraphs: {stats.paragraph_count}\")\n</code></pre>"},{"location":"user-guide/data-processing/#dataset-creation","title":"\ud83d\udcca Dataset Creation","text":""},{"location":"user-guide/data-processing/#basic-dataset-creation","title":"Basic Dataset Creation","text":"<pre><code>from llmbuilder.data import TextDataset\n\n# Create dataset from text file\ndataset = TextDataset(\n    data_path=\"training_data.txt\",\n    block_size=1024,        # Sequence length\n    stride=512,             # Overlap between sequences\n    cache_in_memory=True    # Load all data into memory\n)\n\nprint(f\"Dataset size: {len(dataset):,} samples\")\n</code></pre>"},{"location":"user-guide/data-processing/#multi-file-dataset","title":"Multi-File Dataset","text":"<pre><code>from llmbuilder.data import MultiFileDataset\n\n# Create dataset from multiple files\ndataset = MultiFileDataset(\n    file_paths=[\"file1.txt\", \"file2.txt\", \"file3.txt\"],\n    block_size=1024,\n    stride=512,\n    shuffle_files=True,     # Randomize file order\n    max_files=None          # Use all files\n)\n</code></pre>"},{"location":"user-guide/data-processing/#dataset-splitting","title":"Dataset Splitting","text":"<pre><code>from llmbuilder.data import split_dataset\n\n# Split dataset into train/validation/test\ntrain_dataset, val_dataset, test_dataset = split_dataset(\n    dataset,\n    train_ratio=0.8,\n    val_ratio=0.1,\n    test_ratio=0.1,\n    seed=42\n)\n\nprint(f\"Train: {len(train_dataset):,} samples\")\nprint(f\"Validation: {len(val_dataset):,} samples\")\nprint(f\"Test: {len(test_dataset):,} samples\")\n</code></pre>"},{"location":"user-guide/data-processing/#data-preprocessing-pipeline","title":"\ud83d\udd04 Data Preprocessing Pipeline","text":""},{"location":"user-guide/data-processing/#complete-pipeline-example","title":"Complete Pipeline Example","text":"<pre><code>from llmbuilder.data import DataLoader, TextCleaner, TextDataset\nfrom pathlib import Path\n\ndef create_training_dataset(input_dir, output_file, block_size=1024):\n    \"\"\"Complete data preprocessing pipeline.\"\"\"\n\n    # Step 1: Load raw data\n    print(\"\ud83d\udcc1 Loading raw documents...\")\n    loader = DataLoader(\n        min_length=100,\n        clean_text=True,\n        remove_duplicates=True\n    )\n\n    texts = []\n    input_path = Path(input_dir)\n\n    for file_path in input_path.rglob(\"*\"):\n        if file_path.suffix.lower() in loader.supported_extensions:\n            try:\n                text = loader.load_file(file_path)\n                if text:\n                    texts.append(text)\n                    print(f\"  \u2705 {file_path.name}: {len(text):,} chars\")\n            except Exception as e:\n                print(f\"  \u274c {file_path.name}: {e}\")\n\n    # Step 2: Combine and clean\n    print(f\"\\n\ud83e\uddf9 Cleaning {len(texts)} documents...\")\n    combined_text = \"\\n\\n\".join(texts)\n\n    cleaner = TextCleaner(\n        normalize_whitespace=True,\n        remove_urls=True,\n        remove_emails=True,\n        min_sentence_length=20,\n        remove_duplicates=True\n    )\n\n    cleaned_text = cleaner.clean(combined_text)\n    stats = cleaner.get_stats()\n\n    print(f\"  Original: {stats.original_length:,} characters\")\n    print(f\"  Cleaned: {stats.cleaned_length:,} characters\")\n    print(f\"  Removed: {stats.removal_percentage:.1f}%\")\n\n    # Step 3: Save processed text\n    print(f\"\\n\ud83d\udcbe Saving to {output_file}...\")\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        f.write(cleaned_text)\n\n    # Step 4: Create dataset\n    print(f\"\\n\ud83d\udcca Creating dataset...\")\n    dataset = TextDataset(\n        data_path=output_file,\n        block_size=block_size,\n        stride=block_size // 2\n    )\n\n    print(f\"  Dataset size: {len(dataset):,} samples\")\n    print(f\"  Sequence length: {block_size}\")\n    print(f\"  Total tokens: {len(dataset) * block_size:,}\")\n\n    return dataset\n\n# Usage\ndataset = create_training_dataset(\n    input_dir=\"./raw_documents\",\n    output_file=\"./processed_data.txt\",\n    block_size=1024\n)\n</code></pre>"},{"location":"user-guide/data-processing/#advanced-data-processing","title":"\ud83c\udf9b\ufe0f Advanced Data Processing","text":""},{"location":"user-guide/data-processing/#streaming-large-datasets","title":"Streaming Large Datasets","text":"<p>For very large datasets that don't fit in memory:</p> <pre><code>from llmbuilder.data import StreamingDataset\n\n# Create streaming dataset\ndataset = StreamingDataset(\n    data_path=\"huge_dataset.txt\",\n    block_size=1024,\n    buffer_size=10000,      # Number of samples to keep in memory\n    shuffle_buffer=1000     # Shuffle buffer size\n)\n\n# Use with DataLoader\nfrom torch.utils.data import DataLoader\ndataloader = DataLoader(dataset, batch_size=16, num_workers=4)\n</code></pre>"},{"location":"user-guide/data-processing/#custom-data-processing","title":"Custom Data Processing","text":"<pre><code>from llmbuilder.data import BaseDataset\n\nclass CustomDataset(BaseDataset):\n    \"\"\"Custom dataset with domain-specific processing.\"\"\"\n\n    def __init__(self, data_path, **kwargs):\n        super().__init__(**kwargs)\n        self.data = self.load_custom_data(data_path)\n\n    def load_custom_data(self, data_path):\n        \"\"\"Load and process data in custom format.\"\"\"\n        # Your custom loading logic here\n        pass\n\n    def __getitem__(self, idx):\n        \"\"\"Get a single sample.\"\"\"\n        # Your custom sample creation logic\n        pass\n\n    def __len__(self):\n        return len(self.data)\n</code></pre>"},{"location":"user-guide/data-processing/#data-augmentation","title":"Data Augmentation","text":"<pre><code>from llmbuilder.data import DataAugmenter\n\naugmenter = DataAugmenter(\n    synonym_replacement=0.1,    # Replace 10% of words with synonyms\n    random_insertion=0.1,       # Insert random words\n    random_swap=0.1,           # Swap word positions\n    random_deletion=0.1,       # Delete random words\n    paraphrase=True            # Generate paraphrases\n)\n\n# Augment training data\naugmented_texts = []\nfor text in original_texts:\n    augmented = augmenter.augment(text, num_variants=3)\n    augmented_texts.extend(augmented)\n</code></pre>"},{"location":"user-guide/data-processing/#data-quality-assessment","title":"\ud83d\udcc8 Data Quality Assessment","text":""},{"location":"user-guide/data-processing/#quality-metrics","title":"Quality Metrics","text":"<pre><code>from llmbuilder.data import assess_data_quality\n\n# Analyze data quality\nquality_report = assess_data_quality(\"training_data.txt\")\n\nprint(f\"\ud83d\udcca Data Quality Report:\")\nprint(f\"  Total characters: {quality_report.total_chars:,}\")\nprint(f\"  Total words: {quality_report.total_words:,}\")\nprint(f\"  Total sentences: {quality_report.total_sentences:,}\")\nprint(f\"  Average sentence length: {quality_report.avg_sentence_length:.1f} words\")\nprint(f\"  Vocabulary size: {quality_report.vocab_size:,}\")\nprint(f\"  Duplicate sentences: {quality_report.duplicate_percentage:.1f}%\")\nprint(f\"  Language diversity: {quality_report.language_scores}\")\nprint(f\"  Readability score: {quality_report.readability_score:.1f}\")\n</code></pre>"},{"location":"user-guide/data-processing/#data-visualization","title":"Data Visualization","text":"<pre><code>import matplotlib.pyplot as plt\nfrom llmbuilder.data import visualize_data\n\n# Create data visualizations\nfig, axes = visualize_data(\"training_data.txt\")\n\n# Sentence length distribution\naxes[0].hist(sentence_lengths, bins=50)\naxes[0].set_title(\"Sentence Length Distribution\")\n\n# Word frequency\naxes[1].bar(top_words, word_counts)\naxes[1].set_title(\"Most Common Words\")\n\nplt.tight_layout()\nplt.savefig(\"data_analysis.png\")\n</code></pre>"},{"location":"user-guide/data-processing/#cli-data-processing","title":"\ud83d\udd27 CLI Data Processing","text":""},{"location":"user-guide/data-processing/#interactive-data-processing","title":"Interactive Data Processing","text":"<pre><code># Interactive data loading with guided setup\nllmbuilder data load --interactive\n</code></pre> <p>This will prompt you for:</p> <ul> <li>Input directory or file</li> <li>Output file path</li> <li>File formats to process</li> <li>Cleaning options</li> <li>Quality filters</li> </ul>"},{"location":"user-guide/data-processing/#batch-processing","title":"Batch Processing","text":"<pre><code># Process multiple directories\nllmbuilder data load \\\n  --input \"dir1,dir2,dir3\" \\\n  --output combined_data.txt \\\n  --format all \\\n  --clean \\\n  --min-length 100 \\\n  --remove-duplicates\n</code></pre>"},{"location":"user-guide/data-processing/#format-specific-processing","title":"Format-Specific Processing","text":"<pre><code># Process only PDF files\nllmbuilder data load \\\n  --input ./documents \\\n  --output pdf_text.txt \\\n  --format pdf \\\n  --pdf-extract-images \\\n  --pdf-min-font-size 10\n\n# Process only Word documents\nllmbuilder data load \\\n  --input ./documents \\\n  --output docx_text.txt \\\n  --format docx \\\n  --docx-include-tables \\\n  --docx-preserve-formatting\n</code></pre>"},{"location":"user-guide/data-processing/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"user-guide/data-processing/#common-issues","title":"Common Issues","text":""},{"location":"user-guide/data-processing/#1-encoding-problems","title":"1. Encoding Problems","text":"<pre><code># Handle encoding issues\nloader = DataLoader(\n    encoding=\"utf-8\",\n    fallback_encoding=\"latin-1\",\n    fix_encoding=True\n)\n</code></pre>"},{"location":"user-guide/data-processing/#2-memory-issues-with-large-files","title":"2. Memory Issues with Large Files","text":"<pre><code># Process large files in chunks\nloader = DataLoader(chunk_size=1000000)  # 1MB chunks\ntexts = loader.load_file_chunked(\"huge_file.txt\")\n</code></pre>"},{"location":"user-guide/data-processing/#3-poor-quality-extraction","title":"3. Poor Quality Extraction","text":"<pre><code># Adjust quality thresholds\nloader = DataLoader(\n    min_length=200,         # Longer minimum length\n    max_length=10000,       # Reasonable maximum\n    quality_threshold=0.7   # Higher quality threshold\n)\n</code></pre>"},{"location":"user-guide/data-processing/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Optimize for speed\nloader = DataLoader(\n    parallel_processing=True,\n    num_workers=8,\n    cache_processed=True,\n    batch_size=1000\n)\n</code></pre>"},{"location":"user-guide/data-processing/#best-practices","title":"\ud83d\udcda Best Practices","text":""},{"location":"user-guide/data-processing/#1-data-quality-over-quantity","title":"1. Data Quality Over Quantity","text":"<ul> <li>Clean data is more valuable than large amounts of noisy data</li> <li>Remove duplicates and near-duplicates</li> <li>Filter out low-quality content</li> </ul>"},{"location":"user-guide/data-processing/#2-diverse-data-sources","title":"2. Diverse Data Sources","text":"<ul> <li>Use multiple document types and sources</li> <li>Include different writing styles and domains</li> <li>Balance formal and informal text</li> </ul>"},{"location":"user-guide/data-processing/#3-preprocessing-consistency","title":"3. Preprocessing Consistency","text":"<ul> <li>Use the same preprocessing pipeline for training and inference</li> <li>Document your preprocessing steps</li> <li>Version your processed datasets</li> </ul>"},{"location":"user-guide/data-processing/#4-memory-management","title":"4. Memory Management","text":"<ul> <li>Use streaming datasets for large corpora</li> <li>Process data in batches</li> <li>Clean up intermediate files</li> </ul>"},{"location":"user-guide/data-processing/#5-quality-monitoring","title":"5. Quality Monitoring","text":"<ul> <li>Regularly assess data quality</li> <li>Monitor for data drift</li> <li>Keep preprocessing logs</li> </ul> <p>Data Processing Tips</p> <ul> <li>Start with a small sample to test your pipeline</li> <li>Always inspect your processed data manually</li> <li>Keep track of data sources and licenses</li> <li>Use version control for your preprocessing scripts</li> <li>Document any domain-specific preprocessing steps</li> </ul>"},{"location":"user-guide/export/","title":"Model Export","text":"<p>Model export allows you to convert your trained LLMBuilder models into different formats for deployment, optimization, and compatibility with various inference engines. This guide covers all export options and deployment strategies.</p>"},{"location":"user-guide/export/#export-overview","title":"\ud83c\udfaf Export Overview","text":"<p>LLMBuilder supports multiple export formats for different deployment scenarios:</p> <pre><code>graph TB\n    A[Trained Model] --&gt; B[Export Options]\n    B --&gt; C[GGUF Format]\n    B --&gt; D[ONNX Format]\n    B --&gt; E[Quantized Models]\n    B --&gt; F[HuggingFace Format]\n\n    C --&gt; C1[llama.cpp]\n    C --&gt; C2[Ollama]\n\n    D --&gt; D1[Mobile Apps]\n    D --&gt; D2[Edge Devices]\n\n    E --&gt; E1[8-bit Models]\n    E --&gt; E2[4-bit Models]\n\n    F --&gt; F1[Transformers]\n    F --&gt; F2[Model Hub]\n\n    style A fill:#e1f5fe\n    style C1 fill:#e8f5e8\n    style D1 fill:#fff3e0\n    style E1 fill:#f3e5f5\n    style F1 fill:#e0f2f1</code></pre>"},{"location":"user-guide/export/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"user-guide/export/#cli-export","title":"CLI Export","text":"<pre><code># Export to GGUF format\nllmbuilder export gguf \\\n  ./model/model.pt \\\n  --output model.gguf \\\n  --quantization q4_0\n\n# Export to ONNX format\nllmbuilder export onnx \\\n  ./model/model.pt \\\n  --output model.onnx \\\n  --opset 11\n\n# Quantize model\nllmbuilder export quantize \\\n  ./model/model.pt \\\n  --output quantized_model.pt \\\n  --method dynamic \\\n  --bits 8\n</code></pre>"},{"location":"user-guide/export/#python-api-export","title":"Python API Export","text":"<pre><code>from llmbuilder.export import export_gguf, export_onnx, quantize_model\n\n# Export to GGUF\nexport_gguf(\n    model_path=\"./model/model.pt\",\n    output_path=\"model.gguf\",\n    quantization=\"q4_0\"\n)\n\n# Export to ONNX\nexport_onnx(\n    model_path=\"./model/model.pt\",\n    output_path=\"model.onnx\",\n    opset_version=11\n)\n\n# Quantize model\nquantize_model(\n    model_path=\"./model/model.pt\",\n    output_path=\"quantized_model.pt\",\n    method=\"dynamic\",\n    bits=8\n)\n</code></pre>"},{"location":"user-guide/export/#export-formats","title":"\ud83d\udce6 Export Formats","text":""},{"location":"user-guide/export/#1-gguf-format","title":"1. GGUF Format","text":"<p>GGUF (GPT-Generated Unified Format) is optimized for CPU inference with llama.cpp:</p> <pre><code>from llmbuilder.export import GGUFExporter\n\nexporter = GGUFExporter(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\"\n)\n\n# Export with different quantization levels\nexporter.export(\n    output_path=\"model_f16.gguf\",\n    quantization=\"f16\"          # Full precision\n)\n\nexporter.export(\n    output_path=\"model_q8.gguf\",\n    quantization=\"q8_0\"         # 8-bit quantization\n)\n\nexporter.export(\n    output_path=\"model_q4.gguf\",\n    quantization=\"q4_0\"         # 4-bit quantization\n)\n</code></pre> <p>Quantization Options:</p> <ul> <li><code>f16</code>: 16-bit floating point (best quality, larger size)</li> <li><code>q8_0</code>: 8-bit quantization (good quality, medium size)</li> <li><code>q4_0</code>: 4-bit quantization (lower quality, smallest size)</li> <li><code>q4_1</code>: 4-bit with better quality</li> <li><code>q5_0</code>, <code>q5_1</code>: 5-bit quantization (balanced)</li> </ul>"},{"location":"user-guide/export/#2-onnx-format","title":"2. ONNX Format","text":"<p>ONNX (Open Neural Network Exchange) for cross-platform deployment:</p> <pre><code>from llmbuilder.export import ONNXExporter\n\nexporter = ONNXExporter(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\"\n)\n\n# Export for different targets\nexporter.export(\n    output_path=\"model_cpu.onnx\",\n    target=\"cpu\",\n    opset_version=11,\n    optimize=True\n)\n\nexporter.export(\n    output_path=\"model_gpu.onnx\",\n    target=\"gpu\",\n    opset_version=14,\n    fp16=True\n)\n</code></pre> <p>ONNX Options:</p> <ul> <li><code>opset_version</code>: ONNX operator set version (11, 13, 14)</li> <li><code>target</code>: Target device (\"cpu\", \"gpu\", \"mobile\")</li> <li><code>optimize</code>: Apply ONNX optimizations</li> <li><code>fp16</code>: Use 16-bit precision for GPU</li> </ul>"},{"location":"user-guide/export/#3-quantized-pytorch-models","title":"3. Quantized PyTorch Models","text":"<p>Quantize models while keeping PyTorch format:</p> <pre><code>from llmbuilder.export import PyTorchQuantizer\n\nquantizer = PyTorchQuantizer(\n    model_path=\"./model/model.pt\"\n)\n\n# Dynamic quantization (post-training)\nquantizer.dynamic_quantize(\n    output_path=\"model_dynamic_int8.pt\",\n    dtype=\"int8\"\n)\n\n# Static quantization (requires calibration data)\nquantizer.static_quantize(\n    output_path=\"model_static_int8.pt\",\n    calibration_data=\"calibration_data.txt\",\n    dtype=\"int8\"\n)\n\n# QAT (Quantization Aware Training)\nquantizer.qat_quantize(\n    output_path=\"model_qat_int8.pt\",\n    training_data=\"training_data.txt\",\n    epochs=3\n)\n</code></pre>"},{"location":"user-guide/export/#4-huggingface-format","title":"4. HuggingFace Format","text":"<p>Export to HuggingFace Transformers format:</p> <pre><code>from llmbuilder.export import HuggingFaceExporter\n\nexporter = HuggingFaceExporter(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\"\n)\n\n# Export to HuggingFace format\nexporter.export(\n    output_dir=\"./huggingface_model\",\n    model_name=\"my-llm-model\",\n    push_to_hub=False,          # Set True to upload to Hub\n    private=False\n)\n</code></pre>"},{"location":"user-guide/export/#export-configuration","title":"\u2699\ufe0f Export Configuration","text":""},{"location":"user-guide/export/#advanced-export-settings","title":"Advanced Export Settings","text":"<pre><code>from llmbuilder.export import ExportConfig\n\nconfig = ExportConfig(\n    # Model settings\n    max_seq_length=2048,        # Maximum sequence length\n    vocab_size=32000,           # Vocabulary size\n\n    # Quantization settings\n    quantization_method=\"dynamic\",\n    calibration_samples=1000,\n    quantization_bits=8,\n\n    # Optimization settings\n    optimize_for_inference=True,\n    remove_unused_weights=True,\n    fuse_operations=True,\n\n    # Memory settings\n    memory_efficient=True,\n    low_cpu_mem_usage=True,\n\n    # Metadata\n    model_name=\"MyLLM\",\n    model_description=\"Custom language model\",\n    author=\"Your Name\",\n    license=\"MIT\"\n)\n</code></pre>"},{"location":"user-guide/export/#deployment-scenarios","title":"\ud83d\ude80 Deployment Scenarios","text":""},{"location":"user-guide/export/#1-cpu-inference-with-llamacpp","title":"1. CPU Inference with llama.cpp","text":"<pre><code># Export to GGUF\nllmbuilder export gguf ./model/model.pt --output model.gguf --quantization q4_0\n\n# Use with llama.cpp\n./llama.cpp/main -m model.gguf -p \"Hello, world!\" -n 100\n</code></pre>"},{"location":"user-guide/export/#2-mobile-deployment-with-onnx","title":"2. Mobile Deployment with ONNX","text":"<pre><code># Export optimized for mobile\nfrom llmbuilder.export import export_onnx\n\nexport_onnx(\n    model_path=\"./model/model.pt\",\n    output_path=\"mobile_model.onnx\",\n    target=\"mobile\",\n    optimize=True,\n    fp16=True,\n    max_seq_length=512          # Shorter for mobile\n)\n</code></pre>"},{"location":"user-guide/export/#3-cloud-deployment","title":"3. Cloud Deployment","text":"<pre><code># Export for cloud inference\nfrom llmbuilder.export import CloudExporter\n\nexporter = CloudExporter(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\"\n)\n\n# AWS SageMaker\nexporter.export_sagemaker(\n    output_dir=\"./sagemaker_model\",\n    instance_type=\"ml.g4dn.xlarge\"\n)\n\n# Google Cloud AI Platform\nexporter.export_vertex_ai(\n    output_dir=\"./vertex_model\",\n    machine_type=\"n1-standard-4\"\n)\n\n# Azure ML\nexporter.export_azure_ml(\n    output_dir=\"./azure_model\",\n    compute_target=\"gpu-cluster\"\n)\n</code></pre>"},{"location":"user-guide/export/#4-edge-deployment","title":"4. Edge Deployment","text":"<pre><code># Export for edge devices\nfrom llmbuilder.export import EdgeExporter\n\nexporter = EdgeExporter(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\"\n)\n\n# TensorRT for NVIDIA devices\nexporter.export_tensorrt(\n    output_path=\"model.trt\",\n    precision=\"fp16\",\n    max_batch_size=1\n)\n\n# CoreML for Apple devices\nexporter.export_coreml(\n    output_path=\"model.mlmodel\",\n    target=\"iOS15\"\n)\n\n# TensorFlow Lite for mobile\nexporter.export_tflite(\n    output_path=\"model.tflite\",\n    quantize=True\n)\n</code></pre>"},{"location":"user-guide/export/#export-optimization","title":"\ud83d\udcca Export Optimization","text":""},{"location":"user-guide/export/#model-size-optimization","title":"Model Size Optimization","text":"<pre><code>from llmbuilder.export import optimize_model_size\n\n# Remove unused parameters\noptimized_model = optimize_model_size(\n    model_path=\"./model/model.pt\",\n    remove_unused=True,\n    prune_weights=0.1,          # Prune 10% of smallest weights\n    merge_layers=True           # Merge compatible layers\n)\n\n# Compare sizes\noriginal_size = get_model_size(\"./model/model.pt\")\noptimized_size = get_model_size(optimized_model)\nprint(f\"Size reduction: {(1 - optimized_size/original_size)*100:.1f}%\")\n</code></pre>"},{"location":"user-guide/export/#inference-speed-optimization","title":"Inference Speed Optimization","text":"<pre><code>from llmbuilder.export import optimize_inference_speed\n\n# Optimize for speed\nspeed_optimized = optimize_inference_speed(\n    model_path=\"./model/model.pt\",\n    target_device=\"cpu\",\n    batch_size=1,\n    sequence_length=512,\n    fuse_attention=True,\n    use_flash_attention=False   # CPU doesn't support flash attention\n)\n</code></pre>"},{"location":"user-guide/export/#memory-usage-optimization","title":"Memory Usage Optimization","text":"<pre><code>from llmbuilder.export import optimize_memory_usage\n\n# Optimize for memory\nmemory_optimized = optimize_memory_usage(\n    model_path=\"./model/model.pt\",\n    gradient_checkpointing=True,\n    activation_checkpointing=True,\n    weight_sharing=True,\n    low_memory_mode=True\n)\n</code></pre>"},{"location":"user-guide/export/#export-validation","title":"\ud83d\udd0d Export Validation","text":""},{"location":"user-guide/export/#validate-exported-models","title":"Validate Exported Models","text":"<pre><code>from llmbuilder.export import validate_export\n\n# Validate GGUF export\ngguf_validation = validate_export(\n    original_model=\"./model/model.pt\",\n    exported_model=\"model.gguf\",\n    format=\"gguf\",\n    test_prompts=[\"Hello world\", \"The future of AI\"],\n    tolerance=0.01              # Acceptable difference in outputs\n)\n\nprint(f\"GGUF validation passed: {gguf_validation.passed}\")\nprint(f\"Average difference: {gguf_validation.avg_difference:.4f}\")\n\n# Validate ONNX export\nonnx_validation = validate_export(\n    original_model=\"./model/model.pt\",\n    exported_model=\"model.onnx\",\n    format=\"onnx\",\n    test_prompts=[\"Hello world\", \"The future of AI\"]\n)\n</code></pre>"},{"location":"user-guide/export/#performance-benchmarking","title":"Performance Benchmarking","text":"<pre><code>from llmbuilder.export import benchmark_models\n\n# Compare performance across formats\nresults = benchmark_models([\n    (\"Original PyTorch\", \"./model/model.pt\"),\n    (\"GGUF Q4\", \"model_q4.gguf\"),\n    (\"ONNX\", \"model.onnx\"),\n    (\"Quantized PyTorch\", \"model_int8.pt\")\n], test_prompts=[\"Benchmark prompt\"] * 100)\n\nfor name, metrics in results.items():\n    print(f\"{name}:\")\n    print(f\"  Tokens/sec: {metrics.tokens_per_second:.1f}\")\n    print(f\"  Memory usage: {metrics.memory_mb:.1f} MB\")\n    print(f\"  Model size: {metrics.model_size_mb:.1f} MB\")\n    print(f\"  Latency: {metrics.avg_latency_ms:.1f} ms\")\n</code></pre>"},{"location":"user-guide/export/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"user-guide/export/#common-export-issues","title":"Common Export Issues","text":""},{"location":"user-guide/export/#gguf-export-fails","title":"GGUF Export Fails","text":"<pre><code># Solution: Check model compatibility\nfrom llmbuilder.export import check_gguf_compatibility\n\ncompatibility = check_gguf_compatibility(\"./model/model.pt\")\nif not compatibility.compatible:\n    print(f\"Issues: {compatibility.issues}\")\n    # Fix issues or use alternative export\n</code></pre>"},{"location":"user-guide/export/#onnx-export-errors","title":"ONNX Export Errors","text":"<pre><code># Solution: Use compatible operations\nfrom llmbuilder.export import fix_onnx_compatibility\n\nfixed_model = fix_onnx_compatibility(\n    model_path=\"./model/model.pt\",\n    target_opset=11,\n    replace_unsupported=True\n)\n</code></pre>"},{"location":"user-guide/export/#quantization-quality-loss","title":"Quantization Quality Loss","text":"<pre><code># Solution: Use higher precision or calibration\nconfig = ExportConfig(\n    quantization_method=\"static\",  # Better than dynamic\n    calibration_samples=5000,      # More calibration data\n    quantization_bits=8,           # Higher precision\n    preserve_accuracy=True\n)\n</code></pre>"},{"location":"user-guide/export/#performance-issues","title":"Performance Issues","text":""},{"location":"user-guide/export/#slow-inference","title":"Slow Inference","text":"<pre><code># Solution: Optimize for target hardware\nexport_config = ExportConfig(\n    optimize_for_inference=True,\n    target_device=\"cpu\",           # or \"gpu\"\n    batch_size=1,                  # Optimize for single inference\n    fuse_operations=True,\n    use_optimized_kernels=True\n)\n</code></pre>"},{"location":"user-guide/export/#high-memory-usage","title":"High Memory Usage","text":"<pre><code># Solution: Use memory-efficient formats\nexport_config = ExportConfig(\n    quantization_method=\"dynamic\",\n    quantization_bits=8,\n    memory_efficient=True,\n    streaming_weights=True         # Load weights on demand\n)\n</code></pre>"},{"location":"user-guide/export/#best-practices","title":"\ud83d\udcda Best Practices","text":""},{"location":"user-guide/export/#1-choose-the-right-format","title":"1. Choose the Right Format","text":"<ul> <li>GGUF: CPU inference, llama.cpp compatibility</li> <li>ONNX: Cross-platform, mobile deployment</li> <li>Quantized PyTorch: PyTorch ecosystem, balanced performance</li> <li>HuggingFace: Easy sharing, transformers compatibility</li> </ul>"},{"location":"user-guide/export/#2-quantization-strategy","title":"2. Quantization Strategy","text":"<ul> <li>Start with dynamic quantization for quick wins</li> <li>Use static quantization for better quality</li> <li>Consider QAT for maximum quality retention</li> <li>Test different bit widths (4, 8, 16)</li> </ul>"},{"location":"user-guide/export/#3-validation-and-testing","title":"3. Validation and Testing","text":"<ul> <li>Always validate exported models</li> <li>Test on representative data</li> <li>Benchmark performance vs. quality trade-offs</li> <li>Verify compatibility with target deployment</li> </ul>"},{"location":"user-guide/export/#4-deployment-considerations","title":"4. Deployment Considerations","text":"<ul> <li>Consider target hardware capabilities</li> <li>Plan for model updates and versioning</li> <li>Monitor inference performance in production</li> <li>Have fallback options for compatibility issues</li> </ul> <p>Export Tips</p> <ul> <li>Always validate exported models before deployment</li> <li>Start with higher precision and reduce if needed</li> <li>Consider the trade-off between model size and quality</li> <li>Test on your target hardware before production deployment</li> <li>Keep the original model for future re-exports with different settings</li> </ul>"},{"location":"user-guide/fine-tuning/","title":"Fine-tuning","text":"<p>Fine-tuning allows you to adapt pre-trained models to specific domains or tasks. LLMBuilder provides comprehensive fine-tuning capabilities including LoRA, full parameter fine-tuning, and domain adaptation.</p>"},{"location":"user-guide/fine-tuning/#fine-tuning-overview","title":"\ud83c\udfaf Fine-tuning Overview","text":"<p>Fine-tuning is the process of taking a pre-trained model and adapting it to your specific use case:</p> <pre><code>graph LR\n    A[Pre-trained Model] --&gt; B[Fine-tuning Data]\n    B --&gt; C[Fine-tuning Process]\n    C --&gt; D[Adapted Model]\n\n    C --&gt; C1[LoRA]\n    C --&gt; C2[Full Fine-tuning]\n    C --&gt; C3[Domain Adaptation]\n\n    style A fill:#e1f5fe\n    style D fill:#e8f5e8</code></pre>"},{"location":"user-guide/fine-tuning/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"user-guide/fine-tuning/#cli-fine-tuning","title":"CLI Fine-tuning","text":"<pre><code>llmbuilder finetune model \\\n  --model ./pretrained_model/model.pt \\\n  --dataset domain_data.txt \\\n  --output ./finetuned_model \\\n  --epochs 5 \\\n  --lr 5e-5 \\\n  --use-lora\n</code></pre>"},{"location":"user-guide/fine-tuning/#python-api-fine-tuning","title":"Python API Fine-tuning","text":"<pre><code>import llmbuilder as lb\nfrom llmbuilder.finetune import FineTuningConfig\n\n# Load pre-trained model\nmodel = lb.load_model(\"./pretrained_model/model.pt\")\n\n# Prepare fine-tuning dataset\nfrom llmbuilder.data import TextDataset\ndataset = TextDataset(\"domain_data.txt\", block_size=1024)\n\n# Configure fine-tuning\nconfig = FineTuningConfig(\n    num_epochs=5,\n    learning_rate=5e-5,\n    use_lora=True,\n    lora_rank=16\n)\n\n# Fine-tune model\nresults = lb.finetune_model(model, dataset, config)\n</code></pre>"},{"location":"user-guide/fine-tuning/#fine-tuning-methods","title":"\ud83d\udd27 Fine-tuning Methods","text":""},{"location":"user-guide/fine-tuning/#1-lora-low-rank-adaptation","title":"1. LoRA (Low-Rank Adaptation)","text":"<p>LoRA is efficient and requires less memory:</p> <pre><code>from llmbuilder.finetune import LoRAConfig\n\nlora_config = LoRAConfig(\n    rank=16,                    # LoRA rank (4, 8, 16, 32)\n    alpha=32,                   # LoRA alpha (usually 2x rank)\n    dropout=0.1,                # LoRA dropout\n    target_modules=[            # Which modules to adapt\n        \"attention.query\",\n        \"attention.key\",\n        \"attention.value\",\n        \"mlp.dense\"\n    ]\n)\n\nconfig = FineTuningConfig(\n    use_lora=True,\n    lora_config=lora_config,\n    learning_rate=1e-4,\n    num_epochs=10\n)\n</code></pre> <p>Advantages:</p> <ul> <li>Memory efficient (only ~1% of parameters)</li> <li>Fast training</li> <li>Easy to merge back to base model</li> <li>Multiple adapters can be trained</li> </ul>"},{"location":"user-guide/fine-tuning/#2-full-parameter-fine-tuning","title":"2. Full Parameter Fine-tuning","text":"<p>Fine-tune all model parameters:</p> <pre><code>config = FineTuningConfig(\n    use_lora=False,             # Full fine-tuning\n    learning_rate=1e-5,         # Lower LR for stability\n    num_epochs=3,               # Fewer epochs needed\n    weight_decay=0.01,          # Regularization\n    warmup_steps=100\n)\n</code></pre> <p>Advantages:</p> <ul> <li>Maximum adaptation capability</li> <li>Better performance on very different domains</li> <li>Full model customization</li> </ul> <p>Disadvantages:</p> <ul> <li>Requires more memory</li> <li>Slower training</li> <li>Risk of catastrophic forgetting</li> </ul>"},{"location":"user-guide/fine-tuning/#3-domain-adaptation","title":"3. Domain Adaptation","text":"<p>Adapt to specific domains while preserving general capabilities:</p> <pre><code>config = FineTuningConfig(\n    adaptation_method=\"domain\",\n    domain_weight=0.7,          # Balance between domain and general\n    regularization_strength=0.1, # Prevent forgetting\n    learning_rate=3e-5,\n    num_epochs=8\n)\n</code></pre>"},{"location":"user-guide/fine-tuning/#fine-tuning-configuration","title":"\ud83d\udcca Fine-tuning Configuration","text":""},{"location":"user-guide/fine-tuning/#basic-configuration","title":"Basic Configuration","text":"<pre><code>from llmbuilder.finetune import FineTuningConfig\n\nconfig = FineTuningConfig(\n    # Training parameters\n    num_epochs=5,               # Usually fewer than pre-training\n    learning_rate=5e-5,         # Lower than pre-training\n    batch_size=8,               # Often smaller due to memory\n\n    # Fine-tuning specific\n    use_lora=True,              # Use LoRA for efficiency\n    freeze_embeddings=False,    # Whether to freeze embeddings\n    freeze_layers=0,            # Number of layers to freeze\n\n    # Regularization\n    weight_decay=0.01,\n    dropout_rate=0.1,\n\n    # Evaluation\n    eval_every=100,\n    save_every=500,\n    early_stopping_patience=3\n)\n</code></pre>"},{"location":"user-guide/fine-tuning/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>config = FineTuningConfig(\n    # LoRA settings\n    lora_rank=16,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    lora_target_modules=[\"attention\", \"mlp\"],\n\n    # Learning rate scheduling\n    scheduler=\"cosine\",\n    warmup_ratio=0.1,\n    min_lr_ratio=0.1,\n\n    # Data settings\n    max_seq_length=1024,\n    data_collator=\"default\",\n\n    # Optimization\n    optimizer=\"adamw\",\n    gradient_accumulation_steps=4,\n    max_grad_norm=1.0,\n\n    # Memory optimization\n    gradient_checkpointing=True,\n    dataloader_pin_memory=True,\n    dataloader_num_workers=4\n)\n</code></pre>"},{"location":"user-guide/fine-tuning/#fine-tuning-strategies","title":"\ud83c\udfaf Fine-tuning Strategies","text":""},{"location":"user-guide/fine-tuning/#1-task-specific-fine-tuning","title":"1. Task-Specific Fine-tuning","text":"<p>For specific tasks like code generation, question answering, etc.:</p> <pre><code># Prepare task-specific data\ntask_data = \"\"\"\nQuestion: What is machine learning?\nAnswer: Machine learning is a subset of artificial intelligence...\n\nQuestion: How do neural networks work?\nAnswer: Neural networks are computational models inspired by...\n\"\"\"\n\n# Configure for task adaptation\nconfig = FineTuningConfig(\n    use_lora=True,\n    lora_rank=32,               # Higher rank for complex tasks\n    learning_rate=1e-4,\n    num_epochs=10,\n    task_type=\"question_answering\"\n)\n</code></pre>"},{"location":"user-guide/fine-tuning/#2-domain-specific-fine-tuning","title":"2. Domain-Specific Fine-tuning","text":"<p>For specific domains like medical, legal, scientific:</p> <pre><code># Medical domain example\nconfig = FineTuningConfig(\n    use_lora=True,\n    lora_rank=16,\n    learning_rate=5e-5,\n    num_epochs=15,\n    domain=\"medical\",\n    preserve_general_knowledge=True,  # Prevent catastrophic forgetting\n    domain_weight=0.8\n)\n</code></pre>"},{"location":"user-guide/fine-tuning/#3-style-fine-tuning","title":"3. Style Fine-tuning","text":"<p>For specific writing styles or formats:</p> <pre><code># Creative writing style\nconfig = FineTuningConfig(\n    use_lora=True,\n    lora_rank=8,                # Lower rank for style changes\n    learning_rate=3e-5,\n    num_epochs=5,\n    style_adaptation=True,\n    preserve_factual_knowledge=True\n)\n</code></pre>"},{"location":"user-guide/fine-tuning/#monitoring-fine-tuning","title":"\ud83d\udcc8 Monitoring Fine-tuning","text":""},{"location":"user-guide/fine-tuning/#training-metrics","title":"Training Metrics","text":"<pre><code>from llmbuilder.finetune import FineTuner\n\ntrainer = FineTuner(\n    model=model,\n    train_dataset=train_dataset,\n    val_dataset=val_dataset,\n    config=config\n)\n\n# Train with monitoring\nresults = trainer.train()\n\n# Check results\nprint(f\"Base model perplexity: {results.base_perplexity:.2f}\")\nprint(f\"Fine-tuned perplexity: {results.final_perplexity:.2f}\")\nprint(f\"Improvement: {results.improvement_percentage:.1f}%\")\nprint(f\"Training time: {results.training_time}\")\n</code></pre>"},{"location":"user-guide/fine-tuning/#evaluation-metrics","title":"Evaluation Metrics","text":"<pre><code># Evaluate on validation set\neval_results = trainer.evaluate(val_dataset)\n\nprint(f\"Validation loss: {eval_results.loss:.4f}\")\nprint(f\"Perplexity: {eval_results.perplexity:.2f}\")\nprint(f\"BLEU score: {eval_results.bleu_score:.3f}\")\nprint(f\"Task accuracy: {eval_results.task_accuracy:.3f}\")\n</code></pre>"},{"location":"user-guide/fine-tuning/#advanced-techniques","title":"\ud83d\udd04 Advanced Techniques","text":""},{"location":"user-guide/fine-tuning/#progressive-fine-tuning","title":"Progressive Fine-tuning","text":"<p>Gradually unfreeze layers during training:</p> <pre><code>config = FineTuningConfig(\n    progressive_unfreezing=True,\n    unfreeze_schedule=[\n        (0, 2),     # Epochs 0-2: freeze all but last 2 layers\n        (3, 4),     # Epochs 3-4: freeze all but last 4 layers\n        (5, -1),    # Epochs 5+: unfreeze all layers\n    ],\n    learning_rate_schedule=[\n        (0, 1e-5),  # Lower LR for frozen layers\n        (3, 3e-5),  # Medium LR for partial unfreezing\n        (5, 5e-5),  # Higher LR for full unfreezing\n    ]\n)\n</code></pre>"},{"location":"user-guide/fine-tuning/#multi-task-fine-tuning","title":"Multi-task Fine-tuning","text":"<p>Fine-tune on multiple tasks simultaneously:</p> <pre><code>from llmbuilder.data import MultiTaskDataset\n\n# Prepare multi-task dataset\ndataset = MultiTaskDataset([\n    (\"qa\", qa_dataset, 0.4),        # 40% question answering\n    (\"summarization\", sum_dataset, 0.3),  # 30% summarization\n    (\"generation\", gen_dataset, 0.3),     # 30% text generation\n])\n\nconfig = FineTuningConfig(\n    multi_task=True,\n    task_weights={\"qa\": 1.0, \"summarization\": 0.8, \"generation\": 1.2},\n    shared_encoder=True,\n    task_specific_heads=True\n)\n</code></pre>"},{"location":"user-guide/fine-tuning/#continual-learning","title":"Continual Learning","text":"<p>Fine-tune while preventing catastrophic forgetting:</p> <pre><code>config = FineTuningConfig(\n    continual_learning=True,\n    regularization_method=\"ewc\",    # Elastic Weight Consolidation\n    regularization_strength=1000,\n    memory_buffer_size=1000,        # Store important examples\n    replay_frequency=0.1            # 10% replay during training\n)\n</code></pre>"},{"location":"user-guide/fine-tuning/#specialized-fine-tuning","title":"\ud83c\udfa8 Specialized Fine-tuning","text":""},{"location":"user-guide/fine-tuning/#code-generation-fine-tuning","title":"Code Generation Fine-tuning","text":"<pre><code># Configure for code generation\nconfig = FineTuningConfig(\n    use_lora=True,\n    lora_rank=32,\n    target_modules=[\"attention\", \"mlp\"],\n    learning_rate=1e-4,\n    num_epochs=8,\n\n    # Code-specific settings\n    max_seq_length=2048,        # Longer sequences for code\n    special_tokens=[\"&lt;code&gt;\", \"&lt;/code&gt;\", \"&lt;comment&gt;\"],\n    code_specific_loss=True,\n    syntax_aware_training=True\n)\n</code></pre>"},{"location":"user-guide/fine-tuning/#conversational-fine-tuning","title":"Conversational Fine-tuning","text":"<pre><code># Configure for chat/dialogue\nconfig = FineTuningConfig(\n    use_lora=True,\n    lora_rank=16,\n    learning_rate=5e-5,\n    num_epochs=6,\n\n    # Conversation-specific\n    dialogue_format=True,\n    turn_separator=\"&lt;turn&gt;\",\n    response_loss_only=True,    # Only compute loss on responses\n    max_turns=10\n)\n</code></pre>"},{"location":"user-guide/fine-tuning/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"user-guide/fine-tuning/#common-issues","title":"Common Issues","text":""},{"location":"user-guide/fine-tuning/#catastrophic-forgetting","title":"Catastrophic Forgetting","text":"<pre><code># Solution: Use regularization\nconfig = FineTuningConfig(\n    regularization_method=\"l2\",\n    regularization_strength=0.01,\n    preserve_base_capabilities=True,\n    validation_on_base_tasks=True\n)\n</code></pre>"},{"location":"user-guide/fine-tuning/#overfitting","title":"Overfitting","text":"<pre><code># Solution: Reduce learning rate and add regularization\nconfig = FineTuningConfig(\n    learning_rate=1e-5,         # Lower LR\n    weight_decay=0.1,           # Higher weight decay\n    dropout_rate=0.2,           # Higher dropout\n    early_stopping_patience=2,  # Early stopping\n    validation_split=0.2        # More validation data\n)\n</code></pre>"},{"location":"user-guide/fine-tuning/#poor-task-performance","title":"Poor Task Performance","text":"<pre><code># Solution: Increase adaptation capacity\nconfig = FineTuningConfig(\n    lora_rank=64,               # Higher LoRA rank\n    lora_alpha=128,             # Higher alpha\n    learning_rate=1e-4,         # Higher LR\n    num_epochs=15,              # More epochs\n    target_modules=\"all\"        # Adapt more modules\n)\n</code></pre>"},{"location":"user-guide/fine-tuning/#best-practices","title":"\ud83d\udcda Best Practices","text":""},{"location":"user-guide/fine-tuning/#1-data-preparation","title":"1. Data Preparation","text":"<ul> <li>Use high-quality, task-relevant data</li> <li>Balance dataset sizes across tasks</li> <li>Include diverse examples</li> <li>Validate data format and quality</li> </ul>"},{"location":"user-guide/fine-tuning/#2-hyperparameter-selection","title":"2. Hyperparameter Selection","text":"<ul> <li>Start with lower learning rates (1e-5 to 1e-4)</li> <li>Use fewer epochs than pre-training (3-10)</li> <li>Choose appropriate LoRA rank (8-32)</li> <li>Monitor validation metrics closely</li> </ul>"},{"location":"user-guide/fine-tuning/#3-evaluation-strategy","title":"3. Evaluation Strategy","text":"<ul> <li>Evaluate on both task-specific and general metrics</li> <li>Test for catastrophic forgetting</li> <li>Use held-out test sets</li> <li>Compare against base model performance</li> </ul>"},{"location":"user-guide/fine-tuning/#4-resource-management","title":"4. Resource Management","text":"<ul> <li>Use LoRA for memory efficiency</li> <li>Enable gradient checkpointing if needed</li> <li>Monitor GPU memory usage</li> <li>Save checkpoints frequently</li> </ul> <p>Fine-tuning Tips</p> <ul> <li>Always start with a smaller learning rate than pre-training</li> <li>Use LoRA for most fine-tuning tasks unless you need maximum adaptation</li> <li>Monitor both task performance and general capabilities</li> <li>Save multiple checkpoints to find the best stopping point</li> <li>Test your fine-tuned model thoroughly before deployment</li> </ul>"},{"location":"user-guide/generation/","title":"Text Generation","text":"<p>Text generation is where your trained language model comes to life. LLMBuilder provides powerful and flexible text generation capabilities with various sampling strategies, interactive modes, and customization options.</p>"},{"location":"user-guide/generation/#generation-overview","title":"\ud83c\udfaf Generation Overview","text":"<p>Text generation transforms your trained model into a creative writing assistant:</p> <pre><code>graph LR\n    A[Prompt] --&gt; B[Tokenizer]\n    B --&gt; C[Model]\n    C --&gt; D[Sampling Strategy]\n    D --&gt; E[Generated Tokens]\n    E --&gt; F[Detokenizer]\n    F --&gt; G[Generated Text]\n\n    D --&gt; D1[Greedy]\n    D --&gt; D2[Top-k]\n    D --&gt; D3[Top-p]\n    D --&gt; D4[Temperature]\n\n    style A fill:#e1f5fe\n    style G fill:#e8f5e8</code></pre>"},{"location":"user-guide/generation/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"user-guide/generation/#cli-generation","title":"CLI Generation","text":"<pre><code># Interactive generation setup\nllmbuilder generate text --setup\n\n# Direct generation\nllmbuilder generate text \\\n  --model ./model/model.pt \\\n  --tokenizer ./tokenizer \\\n  --prompt \"The future of AI is\" \\\n  --max-tokens 100 \\\n  --temperature 0.8\n\n# Interactive chat mode\nllmbuilder generate text \\\n  --model ./model/model.pt \\\n  --tokenizer ./tokenizer \\\n  --interactive\n</code></pre>"},{"location":"user-guide/generation/#python-api-generation","title":"Python API Generation","text":"<pre><code>import llmbuilder as lb\n\n# Simple generation\ntext = lb.generate_text(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    prompt=\"The future of AI is\",\n    max_new_tokens=100,\n    temperature=0.8\n)\nprint(text)\n\n# Interactive generation\nlb.interactive_cli(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    temperature=0.8\n)\n</code></pre>"},{"location":"user-guide/generation/#generation-parameters","title":"\u2699\ufe0f Generation Parameters","text":""},{"location":"user-guide/generation/#core-parameters","title":"Core Parameters","text":"<pre><code>from llmbuilder.inference import GenerationConfig\n\nconfig = GenerationConfig(\n    # Length control\n    max_new_tokens=100,         # Maximum tokens to generate\n    min_new_tokens=10,          # Minimum tokens to generate\n    max_length=1024,            # Total sequence length limit\n\n    # Sampling parameters\n    temperature=0.8,            # Creativity (0.1-2.0)\n    top_k=50,                   # Top-k sampling\n    top_p=0.9,                  # Nucleus sampling\n    repetition_penalty=1.1,     # Prevent repetition\n\n    # Special tokens\n    pad_token_id=0,\n    eos_token_id=2,\n    bos_token_id=1,\n\n    # Generation strategy\n    do_sample=True,             # Use sampling vs greedy\n    num_beams=1,                # Beam search width\n    early_stopping=True         # Stop at EOS token\n)\n</code></pre>"},{"location":"user-guide/generation/#advanced-parameters","title":"Advanced Parameters","text":"<pre><code>config = GenerationConfig(\n    # Advanced sampling\n    typical_p=0.95,             # Typical sampling\n    eta_cutoff=1e-4,            # Eta sampling cutoff\n    epsilon_cutoff=1e-4,        # Epsilon sampling cutoff\n\n    # Repetition control\n    repetition_penalty=1.1,\n    no_repeat_ngram_size=3,     # Prevent n-gram repetition\n    encoder_repetition_penalty=1.0,\n\n    # Length penalties\n    length_penalty=1.0,         # Beam search length penalty\n    exponential_decay_length_penalty=None,\n\n    # Diversity\n    num_beam_groups=1,          # Diverse beam search\n    diversity_penalty=0.0,\n\n    # Stopping criteria\n    max_time=None,              # Maximum generation time\n    stop_strings=[\"&lt;/s&gt;\", \"\\n\\n\"],  # Custom stop strings\n)\n</code></pre>"},{"location":"user-guide/generation/#sampling-strategies","title":"\ud83c\udfa8 Sampling Strategies","text":""},{"location":"user-guide/generation/#1-greedy-decoding","title":"1. Greedy Decoding","text":"<p>Always choose the most likely token:</p> <pre><code>config = GenerationConfig(\n    do_sample=False,            # Disable sampling\n    temperature=1.0,            # Not used in greedy\n    top_k=None,                 # Not used in greedy\n    top_p=None                  # Not used in greedy\n)\n\ntext = lb.generate_text(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    prompt=\"Machine learning is\",\n    config=config\n)\n</code></pre> <p>Use cases:</p> <ul> <li>Deterministic output needed</li> <li>Factual question answering</li> <li>Code generation</li> <li>Translation tasks</li> </ul>"},{"location":"user-guide/generation/#2-temperature-sampling","title":"2. Temperature Sampling","text":"<p>Control randomness with temperature:</p> <pre><code># Conservative (more predictable)\nconservative_config = GenerationConfig(\n    temperature=0.3,            # Low temperature\n    do_sample=True\n)\n\n# Balanced\nbalanced_config = GenerationConfig(\n    temperature=0.8,            # Medium temperature\n    do_sample=True\n)\n\n# Creative (more diverse)\ncreative_config = GenerationConfig(\n    temperature=1.5,            # High temperature\n    do_sample=True\n)\n</code></pre> <p>Temperature effects:</p> <ul> <li>0.1-0.3: Very focused, predictable</li> <li>0.5-0.8: Balanced creativity</li> <li>1.0-1.5: More creative, diverse</li> <li>1.5+: Very creative, potentially incoherent</li> </ul>"},{"location":"user-guide/generation/#3-top-k-sampling","title":"3. Top-k Sampling","text":"<p>Sample from top k most likely tokens:</p> <pre><code>config = GenerationConfig(\n    do_sample=True,\n    temperature=0.8,\n    top_k=40,                   # Consider top 40 tokens\n    top_p=None                  # Disable nucleus sampling\n)\n</code></pre> <p>Top-k values:</p> <ul> <li>1: Greedy decoding</li> <li>10-20: Conservative sampling</li> <li>40-100: Balanced sampling</li> <li>200+: Very diverse sampling</li> </ul>"},{"location":"user-guide/generation/#4-top-p-nucleus-sampling","title":"4. Top-p (Nucleus) Sampling","text":"<p>Sample from tokens that make up top p probability mass:</p> <pre><code>config = GenerationConfig(\n    do_sample=True,\n    temperature=0.8,\n    top_k=None,                 # Disable top-k\n    top_p=0.9                   # Use top 90% probability mass\n)\n</code></pre> <p>Top-p values:</p> <ul> <li>0.1-0.3: Very focused</li> <li>0.5-0.7: Balanced</li> <li>0.8-0.95: Diverse</li> <li>0.95+: Very diverse</li> </ul>"},{"location":"user-guide/generation/#5-combined-sampling","title":"5. Combined Sampling","text":"<p>Combine multiple strategies:</p> <pre><code>config = GenerationConfig(\n    do_sample=True,\n    temperature=0.8,            # Add randomness\n    top_k=50,                   # Limit to top 50 tokens\n    top_p=0.9,                  # Within 90% probability mass\n    repetition_penalty=1.1      # Reduce repetition\n)\n</code></pre>"},{"location":"user-guide/generation/#generation-modes","title":"\ud83c\udfaf Generation Modes","text":""},{"location":"user-guide/generation/#1-single-generation","title":"1. Single Generation","text":"<p>Generate one response to a prompt:</p> <pre><code>response = lb.generate_text(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    prompt=\"Explain quantum computing in simple terms:\",\n    max_new_tokens=200,\n    temperature=0.7\n)\n</code></pre>"},{"location":"user-guide/generation/#2-batch-generation","title":"2. Batch Generation","text":"<p>Generate multiple responses:</p> <pre><code>from llmbuilder.inference import batch_generate\n\nprompts = [\n    \"The benefits of renewable energy are\",\n    \"Artificial intelligence will help us\",\n    \"The future of space exploration includes\"\n]\n\nresponses = batch_generate(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    prompts=prompts,\n    max_new_tokens=100,\n    temperature=0.8,\n    batch_size=8\n)\n\nfor prompt, response in zip(prompts, responses):\n    print(f\"Prompt: {prompt}\")\n    print(f\"Response: {response}\\n\")\n</code></pre>"},{"location":"user-guide/generation/#3-interactive-generation","title":"3. Interactive Generation","text":"<p>Real-time conversation mode:</p> <pre><code>from llmbuilder.inference import InteractiveGenerator\n\ngenerator = InteractiveGenerator(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    config=GenerationConfig(\n        temperature=0.8,\n        top_k=50,\n        max_new_tokens=150\n    )\n)\n\n# Start interactive session\ngenerator.start_session()\n\n# Or use in code\nwhile True:\n    prompt = input(\"You: \")\n    if prompt.lower() == 'quit':\n        break\n\n    response = generator.generate(prompt)\n    print(f\"AI: {response}\")\n</code></pre>"},{"location":"user-guide/generation/#4-streaming-generation","title":"4. Streaming Generation","text":"<p>Generate text token by token:</p> <pre><code>from llmbuilder.inference import stream_generate\n\nfor token in stream_generate(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    prompt=\"The history of artificial intelligence\",\n    max_new_tokens=200,\n    temperature=0.8\n):\n    print(token, end='', flush=True)\n</code></pre>"},{"location":"user-guide/generation/#advanced-generation-features","title":"\ud83d\udd27 Advanced Generation Features","text":""},{"location":"user-guide/generation/#1-prompt-engineering","title":"1. Prompt Engineering","text":"<p>Optimize prompts for better results:</p> <pre><code># System prompt + user prompt\nsystem_prompt = \"You are a helpful AI assistant that provides accurate and concise answers.\"\nuser_prompt = \"Explain machine learning in simple terms.\"\n\nfull_prompt = f\"System: {system_prompt}\\nUser: {user_prompt}\\nAssistant:\"\n\nresponse = lb.generate_text(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    prompt=full_prompt,\n    max_new_tokens=200\n)\n</code></pre>"},{"location":"user-guide/generation/#2-few-shot-learning","title":"2. Few-shot Learning","text":"<p>Provide examples in the prompt:</p> <pre><code>few_shot_prompt = \"\"\"\nTranslate English to French:\n\nEnglish: Hello, how are you?\nFrench: Bonjour, comment allez-vous?\n\nEnglish: What is your name?\nFrench: Comment vous appelez-vous?\n\nEnglish: I love programming.\nFrench:\"\"\"\n\nresponse = lb.generate_text(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    prompt=few_shot_prompt,\n    max_new_tokens=50,\n    temperature=0.3  # Lower temperature for translation\n)\n</code></pre>"},{"location":"user-guide/generation/#3-constrained-generation","title":"3. Constrained Generation","text":"<p>Generate text with constraints:</p> <pre><code>from llmbuilder.inference import ConstrainedGenerator\n\n# Generate text that must contain certain words\ngenerator = ConstrainedGenerator(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    required_words=[\"machine learning\", \"neural networks\", \"data\"],\n    forbidden_words=[\"impossible\", \"never\"],\n    max_length=200\n)\n\nresponse = generator.generate(\"Explain AI technology:\")\n</code></pre>"},{"location":"user-guide/generation/#4-format-specific-generation","title":"4. Format-Specific Generation","text":"<p>Generate structured output:</p> <pre><code># JSON generation\njson_prompt = \"\"\"Generate a JSON object describing a person:\n{\n  \"name\": \"John Smith\",\n  \"age\": 30,\n  \"occupation\": \"Software Engineer\",\n  \"skills\": [\"Python\", \"JavaScript\", \"Machine Learning\"]\n}\n\nGenerate a similar JSON for a data scientist:\n{\"\"\"\n\nresponse = lb.generate_text(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    prompt=json_prompt,\n    max_new_tokens=150,\n    temperature=0.3,\n    stop_strings=[\"}\"]\n)\n\n# Add closing brace\ncomplete_json = response + \"}\"\n</code></pre>"},{"location":"user-guide/generation/#generation-quality-control","title":"\ud83d\udcca Generation Quality Control","text":""},{"location":"user-guide/generation/#1-output-filtering","title":"1. Output Filtering","text":"<p>Filter generated content:</p> <pre><code>from llmbuilder.inference import OutputFilter\n\nfilter_config = {\n    \"min_length\": 20,           # Minimum response length\n    \"max_repetition\": 0.3,      # Maximum repetition ratio\n    \"profanity_filter\": True,   # Filter inappropriate content\n    \"coherence_threshold\": 0.7, # Minimum coherence score\n    \"factuality_check\": True    # Basic fact checking\n}\n\nfiltered_response = OutputFilter.filter(response, filter_config)\n</code></pre>"},{"location":"user-guide/generation/#2-quality-metrics","title":"2. Quality Metrics","text":"<p>Evaluate generation quality:</p> <pre><code>from llmbuilder.inference import evaluate_generation\n\nmetrics = evaluate_generation(\n    generated_text=response,\n    reference_text=None,        # Optional reference\n    prompt=prompt\n)\n\nprint(f\"Coherence: {metrics.coherence:.3f}\")\nprint(f\"Fluency: {metrics.fluency:.3f}\")\nprint(f\"Relevance: {metrics.relevance:.3f}\")\nprint(f\"Diversity: {metrics.diversity:.3f}\")\nprint(f\"Repetition: {metrics.repetition:.3f}\")\n</code></pre>"},{"location":"user-guide/generation/#3-ab-testing","title":"3. A/B Testing","text":"<p>Compare different generation settings:</p> <pre><code>from llmbuilder.inference import compare_generations\n\nconfigs = [\n    GenerationConfig(temperature=0.7, top_k=40),\n    GenerationConfig(temperature=0.8, top_p=0.9),\n    GenerationConfig(temperature=0.9, top_k=100, top_p=0.95)\n]\n\nresults = compare_generations(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    prompt=\"Explain the benefits of renewable energy:\",\n    configs=configs,\n    num_samples=10\n)\n\nfor i, result in enumerate(results):\n    print(f\"Config {i+1}: Quality={result.avg_quality:.3f}, Diversity={result.avg_diversity:.3f}\")\n</code></pre>"},{"location":"user-guide/generation/#interactive-features","title":"\ud83c\udfae Interactive Features","text":""},{"location":"user-guide/generation/#1-chat-interface","title":"1. Chat Interface","text":"<p>Create a chat-like experience:</p> <pre><code>from llmbuilder.inference import ChatInterface\n\nchat = ChatInterface(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    system_prompt=\"You are a helpful AI assistant.\",\n    config=GenerationConfig(temperature=0.8, max_new_tokens=200)\n)\n\n# Start chat session\nchat.start()\n\n# Or use programmatically\nconversation = []\nwhile True:\n    user_input = input(\"You: \")\n    if user_input.lower() == 'quit':\n        break\n\n    response = chat.respond(user_input, conversation)\n    conversation.append({\"user\": user_input, \"assistant\": response})\n    print(f\"AI: {response}\")\n</code></pre>"},{"location":"user-guide/generation/#2-creative-writing-assistant","title":"2. Creative Writing Assistant","text":"<p>Specialized interface for creative writing:</p> <pre><code>from llmbuilder.inference import CreativeWriter\n\nwriter = CreativeWriter(\n    model_path=\"./model/model.pt\",\n    tokenizer_path=\"./tokenizer\",\n    style=\"creative\",\n    config=GenerationConfig(temperature=1.0, top_p=0.95)\n)\n\n# Story continuation\nstory_start = \"It was a dark and stormy night when Sarah discovered the mysterious letter...\"\ncontinuation = writer.continue_story(story_start, length=300)\n\n# Character development\ncharacter = writer.develop_character(\"a brilliant but eccentric scientist\")\n\n# Dialogue generation\ndialogue = writer.generate_dialogue(\"two friends discussing their dreams\", turns=6)\n</code></pre>"},{"location":"user-guide/generation/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"user-guide/generation/#common-issues","title":"Common Issues","text":""},{"location":"user-guide/generation/#repetitive-output","title":"Repetitive Output","text":"<pre><code># Solution: Adjust repetition penalty and sampling\nconfig = GenerationConfig(\n    repetition_penalty=1.2,     # Higher penalty\n    no_repeat_ngram_size=3,     # Prevent 3-gram repetition\n    temperature=0.9,            # Higher temperature\n    top_p=0.9                   # Use nucleus sampling\n)\n</code></pre>"},{"location":"user-guide/generation/#incoherent-output","title":"Incoherent Output","text":"<pre><code># Solution: Lower temperature and use top-k\nconfig = GenerationConfig(\n    temperature=0.6,            # Lower temperature\n    top_k=40,                   # Limit choices\n    top_p=0.8,                  # Conservative nucleus\n    max_new_tokens=100          # Shorter responses\n)\n</code></pre>"},{"location":"user-guide/generation/#too-conservative-output","title":"Too Conservative Output","text":"<pre><code># Solution: Increase temperature and sampling diversity\nconfig = GenerationConfig(\n    temperature=1.0,            # Higher temperature\n    top_k=100,                  # More choices\n    top_p=0.95,                 # Broader nucleus\n    repetition_penalty=1.1      # Slight repetition penalty\n)\n</code></pre>"},{"location":"user-guide/generation/#slow-generation","title":"Slow Generation","text":"<pre><code># Solution: Optimize for speed\nconfig = GenerationConfig(\n    max_new_tokens=50,          # Shorter responses\n    do_sample=False,            # Use greedy decoding\n    use_cache=True,             # Enable KV cache\n    batch_size=1                # Single sample\n)\n\n# Use GPU if available\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n</code></pre>"},{"location":"user-guide/generation/#best-practices","title":"\ud83d\udcda Best Practices","text":""},{"location":"user-guide/generation/#1-parameter-selection","title":"1. Parameter Selection","text":"<ul> <li>Start with temperature=0.8, top_k=50, top_p=0.9</li> <li>Adjust based on your specific use case</li> <li>Lower temperature for factual content</li> <li>Higher temperature for creative content</li> </ul>"},{"location":"user-guide/generation/#2-prompt-engineering","title":"2. Prompt Engineering","text":"<ul> <li>Be specific and clear in your prompts</li> <li>Use examples for complex tasks</li> <li>Include context and constraints</li> <li>Test different prompt formats</li> </ul>"},{"location":"user-guide/generation/#3-quality-control","title":"3. Quality Control","text":"<ul> <li>Always validate generated content</li> <li>Use appropriate filtering for your use case</li> <li>Monitor for bias and inappropriate content</li> <li>Test with diverse inputs</li> </ul>"},{"location":"user-guide/generation/#4-performance-optimization","title":"4. Performance Optimization","text":"<ul> <li>Use appropriate batch sizes</li> <li>Enable GPU acceleration when available</li> <li>Cache models for repeated use</li> <li>Consider quantization for deployment</li> </ul> <p>Generation Tips</p> <ul> <li>Experiment with different parameter combinations to find what works best</li> <li>Use lower temperatures for factual tasks and higher for creative tasks</li> <li>Always validate generated content before using in production</li> <li>Consider the trade-off between quality and speed for your use case</li> <li>Keep prompts clear and specific for better results</li> </ul>"},{"location":"user-guide/tokenization/","title":"Tokenization","text":"<p>Tokenization is the process of converting text into numerical tokens that language models can understand. LLMBuilder provides comprehensive tokenization tools with support for multiple algorithms and customization options.</p>"},{"location":"user-guide/tokenization/#overview","title":"\ud83c\udfaf Overview","text":"<p>Tokenization bridges the gap between human text and machine learning:</p> <pre><code>graph LR\n    A[\"Hello world!\"] --&gt; B[Tokenizer]\n    B --&gt; C[\"[15496, 995, 0]\"]\n    C --&gt; D[Model]\n    D --&gt; E[Predictions]\n    E --&gt; F[Detokenizer]\n    F --&gt; G[\"Generated text\"]\n\n    style A fill:#e1f5fe\n    style C fill:#fff3e0\n    style G fill:#e8f5e8</code></pre>"},{"location":"user-guide/tokenization/#tokenization-algorithms","title":"\ud83d\udd24 Tokenization Algorithms","text":"<p>LLMBuilder supports several tokenization algorithms:</p>"},{"location":"user-guide/tokenization/#byte-pair-encoding-bpe-recommended","title":"Byte-Pair Encoding (BPE) - Recommended","text":"<p>BPE is the most popular algorithm for language models:</p> <pre><code>from llmbuilder.tokenizer import TokenizerTrainer\nfrom llmbuilder.config import TokenizerConfig\n\n# Configure BPE tokenizer\nconfig = TokenizerConfig(\n    vocab_size=16000,\n    model_type=\"bpe\",\n    character_coverage=1.0,\n    max_sentence_length=4096\n)\n\n# Train tokenizer\ntrainer = TokenizerTrainer(config=config)\ntrainer.train(\n    input_file=\"training_data.txt\",\n    output_dir=\"./tokenizer\",\n    model_prefix=\"tokenizer\"\n)\n</code></pre> <p>Advantages:</p> <ul> <li>Balances vocabulary size and coverage</li> <li>Handles out-of-vocabulary words well</li> <li>Works across multiple languages</li> <li>Standard for most modern LLMs</li> </ul>"},{"location":"user-guide/tokenization/#unigram-language-model","title":"Unigram Language Model","text":"<p>Statistical approach that optimizes vocabulary:</p> <pre><code>config = TokenizerConfig(\n    vocab_size=16000,\n    model_type=\"unigram\",\n    character_coverage=0.9995,\n    unk_token=\"&lt;unk&gt;\",\n    bos_token=\"&lt;s&gt;\",\n    eos_token=\"&lt;/s&gt;\"\n)\n</code></pre> <p>Advantages:</p> <ul> <li>Probabilistically optimal vocabulary</li> <li>Better handling of morphologically rich languages</li> <li>Flexible subword boundaries</li> </ul>"},{"location":"user-guide/tokenization/#word-level-tokenization","title":"Word-Level Tokenization","text":"<p>Simple word-based tokenization:</p> <pre><code>config = TokenizerConfig(\n    vocab_size=50000,\n    model_type=\"word\",\n    lowercase=True,\n    remove_accents=True\n)\n</code></pre> <p>Use cases:</p> <ul> <li>Small, domain-specific datasets</li> <li>Languages with clear word boundaries</li> <li>When interpretability is important</li> </ul>"},{"location":"user-guide/tokenization/#character-level-tokenization","title":"Character-Level Tokenization","text":"<p>Character-by-character tokenization:</p> <pre><code>config = TokenizerConfig(\n    model_type=\"char\",\n    vocab_size=256,  # Usually small for character-level\n    normalize=True\n)\n</code></pre> <p>Use cases:</p> <ul> <li>Very small datasets</li> <li>Morphologically complex languages</li> <li>When dealing with noisy text</li> </ul>"},{"location":"user-guide/tokenization/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"user-guide/tokenization/#cli-training","title":"CLI Training","text":"<pre><code># Train a BPE tokenizer\nllmbuilder data tokenizer \\\n  --input training_data.txt \\\n  --output ./tokenizer \\\n  --vocab-size 16000 \\\n  --model-type bpe\n</code></pre>"},{"location":"user-guide/tokenization/#python-api","title":"Python API","text":"<pre><code>from llmbuilder.tokenizer import train_tokenizer\n\n# Train tokenizer with default settings\ntokenizer = train_tokenizer(\n    input_file=\"training_data.txt\",\n    output_dir=\"./tokenizer\",\n    vocab_size=16000,\n    special_tokens=[\"&lt;pad&gt;\", \"&lt;unk&gt;\", \"&lt;s&gt;\", \"&lt;/s&gt;\"]\n)\n\n# Test the tokenizer\ntext = \"Hello, world! This is a test.\"\ntokens = tokenizer.encode(text)\ndecoded = tokenizer.decode(tokens)\n\nprint(f\"Original: {text}\")\nprint(f\"Tokens: {tokens}\")\nprint(f\"Decoded: {decoded}\")\n</code></pre>"},{"location":"user-guide/tokenization/#configuration-options","title":"\u2699\ufe0f Configuration Options","text":""},{"location":"user-guide/tokenization/#basic-configuration","title":"Basic Configuration","text":"<pre><code>from llmbuilder.config import TokenizerConfig\n\nconfig = TokenizerConfig(\n    # Core settings\n    vocab_size=16000,           # Vocabulary size\n    model_type=\"bpe\",           # Algorithm: bpe, unigram, word, char\n\n    # Text preprocessing\n    lowercase=False,            # Convert to lowercase\n    remove_accents=False,       # Remove accent marks\n    normalize=True,             # Unicode normalization\n\n    # Coverage and quality\n    character_coverage=1.0,     # Character coverage (0.0-1.0)\n    max_sentence_length=4096,   # Maximum sentence length\n    min_frequency=2,            # Minimum token frequency\n\n    # Special tokens\n    unk_token=\"&lt;unk&gt;\",         # Unknown token\n    bos_token=\"&lt;s&gt;\",           # Beginning of sequence\n    eos_token=\"&lt;/s&gt;\",          # End of sequence\n    pad_token=\"&lt;pad&gt;\",         # Padding token\n    mask_token=\"&lt;mask&gt;\",       # Masking token (for MLM)\n)\n</code></pre>"},{"location":"user-guide/tokenization/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>config = TokenizerConfig(\n    # Algorithm-specific settings\n    bpe_dropout=0.1,           # BPE dropout for regularization\n    split_digits=True,         # Split numbers into digits\n    split_by_whitespace=True,  # Pre-tokenize by whitespace\n    split_by_punctuation=True, # Pre-tokenize by punctuation\n\n    # Vocabulary control\n    max_token_length=16,       # Maximum token length\n    vocab_threshold=1e-6,      # Vocabulary pruning threshold\n    shrinking_factor=0.75,     # Unigram shrinking factor\n\n    # Training control\n    num_threads=8,             # Number of training threads\n    seed=42,                   # Random seed\n    verbose=True               # Verbose training output\n)\n</code></pre>"},{"location":"user-guide/tokenization/#special-tokens","title":"\ud83c\udf9b\ufe0f Special Tokens","text":"<p>Special tokens serve specific purposes in language models:</p>"},{"location":"user-guide/tokenization/#standard-special-tokens","title":"Standard Special Tokens","text":"<pre><code>special_tokens = {\n    \"&lt;pad&gt;\": \"Padding token for batch processing\",\n    \"&lt;unk&gt;\": \"Unknown/out-of-vocabulary token\",\n    \"&lt;s&gt;\": \"Beginning of sequence token\",\n    \"&lt;/s&gt;\": \"End of sequence token\",\n    \"&lt;mask&gt;\": \"Masking token for masked language modeling\"\n}\n\nconfig = TokenizerConfig(\n    vocab_size=16000,\n    special_tokens=list(special_tokens.keys())\n)\n</code></pre>"},{"location":"user-guide/tokenization/#custom-special-tokens","title":"Custom Special Tokens","text":"<pre><code># Add domain-specific special tokens\ncustom_tokens = [\n    \"&lt;code&gt;\", \"&lt;/code&gt;\",       # Code blocks\n    \"&lt;math&gt;\", \"&lt;/math&gt;\",       # Mathematical expressions\n    \"&lt;user&gt;\", \"&lt;assistant&gt;\",   # Chat/dialogue markers\n    \"&lt;system&gt;\",                # System messages\n    \"&lt;tool_call&gt;\", \"&lt;/tool_call&gt;\"  # Function calls\n]\n\nconfig = TokenizerConfig(\n    vocab_size=16000,\n    special_tokens=[\"&lt;pad&gt;\", \"&lt;unk&gt;\", \"&lt;s&gt;\", \"&lt;/s&gt;\"] + custom_tokens\n)\n</code></pre>"},{"location":"user-guide/tokenization/#token-id-management","title":"Token ID Management","text":"<pre><code># Access special token IDs\ntokenizer = train_tokenizer(\"data.txt\", \"./tokenizer\", config=config)\n\nprint(f\"PAD token ID: {tokenizer.pad_token_id}\")\nprint(f\"UNK token ID: {tokenizer.unk_token_id}\")\nprint(f\"BOS token ID: {tokenizer.bos_token_id}\")\nprint(f\"EOS token ID: {tokenizer.eos_token_id}\")\n\n# Custom token IDs\ncode_start_id = tokenizer.token_to_id(\"&lt;code&gt;\")\ncode_end_id = tokenizer.token_to_id(\"&lt;/code&gt;\")\n</code></pre>"},{"location":"user-guide/tokenization/#tokenizer-analysis","title":"\ud83d\udd0d Tokenizer Analysis","text":""},{"location":"user-guide/tokenization/#vocabulary-analysis","title":"Vocabulary Analysis","text":"<pre><code>from llmbuilder.tokenizer import analyze_tokenizer\n\n# Analyze trained tokenizer\nanalysis = analyze_tokenizer(\"./tokenizer\")\n\nprint(f\"\ud83d\udcca Tokenizer Analysis:\")\nprint(f\"  Vocabulary size: {analysis.vocab_size:,}\")\nprint(f\"  Average token length: {analysis.avg_token_length:.2f}\")\nprint(f\"  Character coverage: {analysis.character_coverage:.4f}\")\nprint(f\"  Compression ratio: {analysis.compression_ratio:.2f}\")\n\n# Most common tokens\nprint(f\"\\n\ud83d\udd24 Most common tokens:\")\nfor token, freq in analysis.top_tokens[:10]:\n    print(f\"  '{token}': {freq:,}\")\n\n# Token length distribution\nprint(f\"\\n\ud83d\udccf Token length distribution:\")\nfor length, count in analysis.length_distribution.items():\n    print(f\"  {length} chars: {count:,} tokens\")\n</code></pre>"},{"location":"user-guide/tokenization/#text-coverage-analysis","title":"Text Coverage Analysis","text":"<pre><code># Test tokenizer on sample texts\ntest_texts = [\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"Artificial intelligence and machine learning.\",\n    \"Code: print('Hello, world!')\",\n    \"Mathematical equation: E = mc\u00b2\"\n]\n\nfor text in test_texts:\n    tokens = tokenizer.encode(text)\n    decoded = tokenizer.decode(tokens)\n\n    print(f\"\\nText: {text}\")\n    print(f\"Tokens ({len(tokens)}): {tokens}\")\n    print(f\"Decoded: {decoded}\")\n    print(f\"Perfect reconstruction: {text == decoded}\")\n</code></pre>"},{"location":"user-guide/tokenization/#compression-analysis","title":"Compression Analysis","text":"<pre><code>def analyze_compression(tokenizer, text_file):\n    \"\"\"Analyze tokenization compression.\"\"\"\n    with open(text_file, 'r', encoding='utf-8') as f:\n        text = f.read()\n\n    # Character count\n    char_count = len(text)\n\n    # Token count\n    tokens = tokenizer.encode(text)\n    token_count = len(tokens)\n\n    # Compression ratio\n    compression_ratio = char_count / token_count\n\n    print(f\"\ud83d\udcca Compression Analysis:\")\n    print(f\"  Characters: {char_count:,}\")\n    print(f\"  Tokens: {token_count:,}\")\n    print(f\"  Compression ratio: {compression_ratio:.2f}\")\n    print(f\"  Tokens per 1000 chars: {1000 / compression_ratio:.1f}\")\n\n    return compression_ratio\n\n# Analyze compression\nratio = analyze_compression(tokenizer, \"training_data.txt\")\n</code></pre>"},{"location":"user-guide/tokenization/#tokenizer-usage","title":"\ud83d\udd04 Tokenizer Usage","text":""},{"location":"user-guide/tokenization/#basic-encodingdecoding","title":"Basic Encoding/Decoding","text":"<pre><code>from llmbuilder.tokenizer import Tokenizer\n\n# Load trained tokenizer\ntokenizer = Tokenizer.from_pretrained(\"./tokenizer\")\n\n# Encode text to tokens\ntext = \"Hello, how are you today?\"\ntokens = tokenizer.encode(text)\nprint(f\"Tokens: {tokens}\")\n\n# Decode tokens back to text\ndecoded_text = tokenizer.decode(tokens)\nprint(f\"Decoded: {decoded_text}\")\n\n# Encode with special tokens\ntokens_with_special = tokenizer.encode(\n    text,\n    add_bos_token=True,  # Add beginning-of-sequence token\n    add_eos_token=True   # Add end-of-sequence token\n)\nprint(f\"With special tokens: {tokens_with_special}\")\n</code></pre>"},{"location":"user-guide/tokenization/#batch-processing","title":"Batch Processing","text":"<pre><code># Encode multiple texts\ntexts = [\n    \"First example text.\",\n    \"Second example text.\",\n    \"Third example text.\"\n]\n\n# Batch encode\nbatch_tokens = tokenizer.encode_batch(texts)\nfor i, tokens in enumerate(batch_tokens):\n    print(f\"Text {i+1}: {tokens}\")\n\n# Batch decode\nbatch_decoded = tokenizer.decode_batch(batch_tokens)\nfor i, text in enumerate(batch_decoded):\n    print(f\"Decoded {i+1}: {text}\")\n</code></pre>"},{"location":"user-guide/tokenization/#padding-and-truncation","title":"Padding and Truncation","text":"<pre><code># Encode with padding and truncation\ntokens = tokenizer.encode(\n    text,\n    max_length=512,      # Maximum sequence length\n    padding=\"max_length\", # Pad to max_length\n    truncation=True,     # Truncate if longer\n    return_attention_mask=True  # Return attention mask\n)\n\nprint(f\"Tokens: {tokens['input_ids']}\")\nprint(f\"Attention mask: {tokens['attention_mask']}\")\n</code></pre>"},{"location":"user-guide/tokenization/#domain-specific-tokenization","title":"\ud83c\udfaf Domain-Specific Tokenization","text":""},{"location":"user-guide/tokenization/#code-tokenization","title":"Code Tokenization","text":"<pre><code># Configure tokenizer for code\ncode_config = TokenizerConfig(\n    vocab_size=32000,\n    model_type=\"bpe\",\n    split_by_whitespace=True,\n    split_by_punctuation=False,  # Keep operators together\n    special_tokens=[\n        \"&lt;pad&gt;\", \"&lt;unk&gt;\", \"&lt;s&gt;\", \"&lt;/s&gt;\",\n        \"&lt;code&gt;\", \"&lt;/code&gt;\",\n        \"&lt;comment&gt;\", \"&lt;/comment&gt;\",\n        \"&lt;string&gt;\", \"&lt;/string&gt;\",\n        \"&lt;number&gt;\", \"&lt;/number&gt;\"\n    ]\n)\n\n# Train on code data\ncode_tokenizer = train_tokenizer(\n    input_file=\"code_dataset.txt\",\n    output_dir=\"./code_tokenizer\",\n    config=code_config\n)\n</code></pre>"},{"location":"user-guide/tokenization/#multilingual-tokenization","title":"Multilingual Tokenization","text":"<pre><code># Configure for multiple languages\nmultilingual_config = TokenizerConfig(\n    vocab_size=64000,  # Larger vocab for multiple languages\n    model_type=\"unigram\",  # Better for morphologically rich languages\n    character_coverage=0.9995,  # High coverage for diverse scripts\n    normalize=True,\n    special_tokens=[\n        \"&lt;pad&gt;\", \"&lt;unk&gt;\", \"&lt;s&gt;\", \"&lt;/s&gt;\",\n        \"&lt;en&gt;\", \"&lt;es&gt;\", \"&lt;fr&gt;\", \"&lt;de&gt;\",  # Language tags\n        \"&lt;zh&gt;\", \"&lt;ja&gt;\", \"&lt;ar&gt;\", \"&lt;hi&gt;\"\n    ]\n)\n</code></pre>"},{"location":"user-guide/tokenization/#scientific-text-tokenization","title":"Scientific Text Tokenization","text":"<pre><code># Configure for scientific text\nscientific_config = TokenizerConfig(\n    vocab_size=24000,\n    model_type=\"bpe\",\n    split_digits=False,  # Keep numbers together\n    special_tokens=[\n        \"&lt;pad&gt;\", \"&lt;unk&gt;\", \"&lt;s&gt;\", \"&lt;/s&gt;\",\n        \"&lt;math&gt;\", \"&lt;/math&gt;\",\n        \"&lt;formula&gt;\", \"&lt;/formula&gt;\",\n        \"&lt;citation&gt;\", \"&lt;/citation&gt;\",\n        \"&lt;table&gt;\", \"&lt;/table&gt;\",\n        \"&lt;figure&gt;\", \"&lt;/figure&gt;\"\n    ]\n)\n</code></pre>"},{"location":"user-guide/tokenization/#advanced-features","title":"\ud83d\udd27 Advanced Features","text":""},{"location":"user-guide/tokenization/#tokenizer-merging","title":"Tokenizer Merging","text":"<pre><code># Merge multiple tokenizers\nfrom llmbuilder.tokenizer import merge_tokenizers\n\nbase_tokenizer = Tokenizer.from_pretrained(\"./base_tokenizer\")\ndomain_tokenizer = Tokenizer.from_pretrained(\"./domain_tokenizer\")\n\nmerged_tokenizer = merge_tokenizers(\n    [base_tokenizer, domain_tokenizer],\n    output_dir=\"./merged_tokenizer\",\n    vocab_size=32000\n)\n</code></pre>"},{"location":"user-guide/tokenization/#vocabulary-extension","title":"Vocabulary Extension","text":"<pre><code># Extend existing tokenizer vocabulary\nnew_tokens = [\"&lt;new_token_1&gt;\", \"&lt;new_token_2&gt;\", \"domain_term\"]\n\nextended_tokenizer = tokenizer.extend_vocabulary(\n    new_tokens=new_tokens,\n    output_dir=\"./extended_tokenizer\"\n)\n\nprint(f\"Original vocab size: {len(tokenizer)}\")\nprint(f\"Extended vocab size: {len(extended_tokenizer)}\")\n</code></pre>"},{"location":"user-guide/tokenization/#tokenizer-adaptation","title":"Tokenizer Adaptation","text":"<pre><code># Adapt tokenizer to new domain\nadapted_tokenizer = tokenizer.adapt_to_domain(\n    domain_data=\"new_domain_data.txt\",\n    adaptation_ratio=0.1,  # 10% of vocab from new domain\n    output_dir=\"./adapted_tokenizer\"\n)\n</code></pre>"},{"location":"user-guide/tokenization/#performance-optimization","title":"\ud83d\udcca Performance Optimization","text":""},{"location":"user-guide/tokenization/#fast-tokenization","title":"Fast Tokenization","text":"<pre><code># Enable fast tokenization\ntokenizer = Tokenizer.from_pretrained(\n    \"./tokenizer\",\n    use_fast=True,      # Use fast Rust implementation\n    num_threads=8       # Parallel processing\n)\n\n# Benchmark tokenization speed\nimport time\n\ntext = \"Your text here\" * 1000  # Large text\nstart_time = time.time()\ntokens = tokenizer.encode(text)\nend_time = time.time()\n\nprint(f\"Tokenization speed: {len(tokens) / (end_time - start_time):.0f} tokens/sec\")\n</code></pre>"},{"location":"user-guide/tokenization/#memory-optimization","title":"Memory Optimization","text":"<pre><code># Optimize for memory usage\ntokenizer = Tokenizer.from_pretrained(\n    \"./tokenizer\",\n    low_memory=True,    # Reduce memory usage\n    mmap=True          # Memory-map vocabulary files\n)\n</code></pre>"},{"location":"user-guide/tokenization/#caching","title":"Caching","text":"<pre><code># Enable tokenization caching\ntokenizer.enable_cache(\n    cache_dir=\"./tokenizer_cache\",\n    max_cache_size=\"1GB\"\n)\n\n# Tokenization results will be cached for repeated texts\n</code></pre>"},{"location":"user-guide/tokenization/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"user-guide/tokenization/#common-issues","title":"Common Issues","text":""},{"location":"user-guide/tokenization/#1-poor-tokenization-quality","title":"1. Poor Tokenization Quality","text":"<pre><code># Increase vocabulary size\nconfig.vocab_size = 32000\n\n# Improve character coverage\nconfig.character_coverage = 0.9999\n\n# Adjust minimum frequency\nconfig.min_frequency = 1\n</code></pre>"},{"location":"user-guide/tokenization/#2-out-of-memory-during-training","title":"2. Out-of-Memory During Training","text":"<pre><code># Reduce training data size\nconfig.max_sentence_length = 1024\n\n# Use fewer threads\nconfig.num_threads = 4\n\n# Process in chunks\ntrainer.train_chunked(\n    input_file=\"large_data.txt\",\n    chunk_size=1000000  # 1M characters per chunk\n)\n</code></pre>"},{"location":"user-guide/tokenization/#3-slow-tokenization","title":"3. Slow Tokenization","text":"<pre><code># Use fast tokenizer\ntokenizer = Tokenizer.from_pretrained(\"./tokenizer\", use_fast=True)\n\n# Enable parallel processing\ntokenizer.enable_parallelism(True)\n\n# Batch process texts\ntokens = tokenizer.encode_batch(texts, batch_size=1000)\n</code></pre>"},{"location":"user-guide/tokenization/#validation-and-testing","title":"Validation and Testing","text":"<pre><code># Validate tokenizer quality\ndef validate_tokenizer(tokenizer, test_texts):\n    \"\"\"Validate tokenizer on test texts.\"\"\"\n    issues = []\n\n    for text in test_texts:\n        tokens = tokenizer.encode(text)\n        decoded = tokenizer.decode(tokens)\n\n        if text != decoded:\n            issues.append({\n                'original': text,\n                'decoded': decoded,\n                'tokens': tokens\n            })\n\n    if issues:\n        print(f\"\u26a0\ufe0f  Found {len(issues)} reconstruction issues:\")\n        for issue in issues[:5]:  # Show first 5\n            print(f\"  Original: {issue['original']}\")\n            print(f\"  Decoded:  {issue['decoded']}\")\n            print()\n    else:\n        print(\"\u2705 All texts reconstructed perfectly!\")\n\n    return len(issues) == 0\n\n# Test tokenizer\ntest_texts = [\n    \"Hello, world!\",\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"Special characters: @#$%^&amp;*()\",\n    \"Numbers: 123 456.789\",\n    \"Unicode: caf\u00e9 na\u00efve r\u00e9sum\u00e9\"\n]\n\nis_valid = validate_tokenizer(tokenizer, test_texts)\n</code></pre>"},{"location":"user-guide/tokenization/#best-practices","title":"\ud83d\udcda Best Practices","text":""},{"location":"user-guide/tokenization/#1-vocabulary-size-selection","title":"1. Vocabulary Size Selection","text":"<ul> <li>Small datasets (&lt; 1M tokens): 8K - 16K vocabulary</li> <li>Medium datasets (1M - 100M tokens): 16K - 32K vocabulary</li> <li>Large datasets (&gt; 100M tokens): 32K - 64K vocabulary</li> <li>Multilingual: 64K - 128K vocabulary</li> </ul>"},{"location":"user-guide/tokenization/#2-algorithm-selection","title":"2. Algorithm Selection","text":"<ul> <li>BPE: General purpose, most compatible</li> <li>Unigram: Better for morphologically rich languages</li> <li>Word: Simple datasets, interpretability needed</li> <li>Character: Very small datasets, noisy text</li> </ul>"},{"location":"user-guide/tokenization/#3-special-token-strategy","title":"3. Special Token Strategy","text":"<ul> <li>Always include <code>&lt;pad&gt;</code>, <code>&lt;unk&gt;</code>, <code>&lt;s&gt;</code>, <code>&lt;/s&gt;</code></li> <li>Add domain-specific tokens for better performance</li> <li>Reserve 5-10% of vocabulary for special tokens</li> <li>Use consistent special token formats</li> </ul>"},{"location":"user-guide/tokenization/#4-training-data-quality","title":"4. Training Data Quality","text":"<ul> <li>Use the same data distribution as your target task</li> <li>Include diverse text types and styles</li> <li>Clean data but preserve important patterns</li> <li>Ensure sufficient data size (&gt; 1M characters recommended)</li> </ul>"},{"location":"user-guide/tokenization/#5-validation-and-testing","title":"5. Validation and Testing","text":"<ul> <li>Test on held-out data</li> <li>Verify perfect reconstruction for important text types</li> <li>Monitor compression ratios</li> <li>Check coverage of domain-specific terms</li> </ul> <p>Tokenization Tips</p> <ul> <li>Train your tokenizer on the same type of data you'll use for model training</li> <li>Always validate tokenizer quality before training your model</li> <li>Consider the trade-off between vocabulary size and model size</li> <li>Save tokenizer training logs and configurations for reproducibility</li> <li>Test tokenization on edge cases and special characters</li> </ul>"},{"location":"user-guide/training/","title":"Model Training","text":"<p>This comprehensive guide covers everything you need to know about training language models with LLMBuilder, from basic concepts to advanced techniques.</p>"},{"location":"user-guide/training/#training-overview","title":"\ud83c\udfaf Training Overview","text":"<p>LLMBuilder provides a complete training pipeline that handles:</p> <pre><code>graph LR\n    A[Dataset] --&gt; B[DataLoader]\n    B --&gt; C[Model]\n    C --&gt; D[Loss Function]\n    D --&gt; E[Optimizer]\n    E --&gt; F[Training Loop]\n    F --&gt; G[Checkpoints]\n    G --&gt; H[Evaluation]\n\n    style A fill:#e1f5fe\n    style G fill:#e8f5e8\n    style H fill:#fff3e0</code></pre>"},{"location":"user-guide/training/#quick-start-training","title":"\ud83d\ude80 Quick Start Training","text":""},{"location":"user-guide/training/#basic-training-command","title":"Basic Training Command","text":"<pre><code>llmbuilder train model \\\n  --data training_data.txt \\\n  --tokenizer ./tokenizer \\\n  --output ./model \\\n  --epochs 10 \\\n  --batch-size 16\n</code></pre>"},{"location":"user-guide/training/#python-api-training","title":"Python API Training","text":"<pre><code>import llmbuilder as lb\n\n# Load configuration\nconfig = lb.load_config(preset=\"cpu_small\")\n\n# Build model\nmodel = lb.build_model(config.model)\n\n# Prepare dataset\nfrom llmbuilder.data import TextDataset\ndataset = TextDataset(\"training_data.txt\", block_size=config.model.max_seq_length)\n\n# Train model\nresults = lb.train_model(model, dataset, config.training)\n</code></pre>"},{"location":"user-guide/training/#small-dataset-workflow-model_test-example","title":"Small-Dataset Workflow (Model_Test example)","text":"<p>For tiny datasets (hundreds of tokens), use the bundled example for stable training and easy generation. It uses small-friendly settings (e.g., block_size=64, batch_size=1).</p> <pre><code># Train on the included cybersecurity texts in Model_Test/\npython docs/train_model.py --data_dir ./Model_Test --output_dir ./Model_Test/output \\\n  --epochs 5 --batch_size 1 --block_size 64 --embed_dim 256 --layers 4 --heads 8 \\\n  --prompt \"Cybersecurity is important because\"\n\n# Generate again later without retraining\npython -c \"import llmbuilder; print(llmbuilder.generate_text(\\\n  model_path=r'.\\\\Model_Test\\\\output\\\\checkpoints\\\\latest_checkpoint.pt', \\\n  tokenizer_path=r'.\\\\Model_Test\\\\output\\\\tokenizer', \\\n  prompt='what is Cybersecurity', max_new_tokens=80, temperature=0.8, top_p=0.9))\"\n</code></pre> <p>Outputs are saved to:</p> <ul> <li><code>Model_Test/output/tokenizer/</code></li> <li><code>Model_Test/output/checkpoints/</code> (contains latest and epoch checkpoints)</li> </ul>"},{"location":"user-guide/training/#full-example-script-docstrain_modelpy","title":"Full Example Script: <code>docs/train_model.py</code>","text":"<pre><code>\"\"\"\nExample: Train a small GPT model on cybersecurity text files using LLMBuilder.\n\nUsage:\n  python docs/train_model.py --data_dir ./Model_Test --output_dir ./Model_Test/output \\\n      --prompt \"Cybersecurity is important because\" --epochs 5\n\nIf --data_dir is omitted, it defaults to the directory containing this script.\nIf --output_dir is omitted, it defaults to &lt;data_dir&gt;/output.\n\nThis script uses small-friendly settings (block_size=64, batch_size=1) so it\nworks on tiny datasets. It trains, saves checkpoints, and performs a sample\ntext generation from the latest/best checkpoint.\n\"\"\"\nfrom __future__ import annotations\nimport argparse\nfrom pathlib import Path\nimport llmbuilder\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Train and generate with LLMBuilder on small text datasets.\")\n    parser.add_argument(\"--data_dir\", type=str, default=None, help=\"Directory with .txt files (default: folder of this script)\")\n    parser.add_argument(\"--output_dir\", type=str, default=None, help=\"Where to save outputs (default: &lt;data_dir&gt;/output)\")\n    parser.add_argument(\"--epochs\", type=int, default=5, help=\"Number of training epochs\")\n    parser.add_argument(\"--batch_size\", type=int, default=1, help=\"Training batch size (small data friendly)\")\n    parser.add_argument(\"--block_size\", type=int, default=64, help=\"Context window size for training\")\n    parser.add_argument(\"--embed_dim\", type=int, default=256, help=\"Model embedding dimension\")\n    parser.add_argument(\"--layers\", type=int, default=4, help=\"Number of transformer layers\")\n    parser.add_argument(\"--heads\", type=int, default=8, help=\"Number of attention heads\")\n    parser.add_argument(\"--lr\", type=float, default=6e-4, help=\"Learning rate\")\n    parser.add_argument(\"--prompt\", type=str, default=\"Cybersecurity is important because\", help=\"Prompt for sample generation\")\n    parser.add_argument(\"--max_new_tokens\", type=int, default=80, help=\"Tokens to generate\")\n    parser.add_argument(\"--temperature\", type=float, default=0.8, help=\"Sampling temperature\")\n    parser.add_argument(\"--top_p\", type=float, default=0.9, help=\"Nucleus sampling top_p\")\n    args = parser.parse_args()\n\n    # Resolve paths\n    if args.data_dir is None:\n        data_dir = Path(__file__).parent\n    else:\n        data_dir = Path(args.data_dir)\n    output_dir = Path(args.output_dir) if args.output_dir else (data_dir / \"output\")\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    print(f\"Data directory: {data_dir}\")\n    print(f\"Output directory: {output_dir}\")\n\n    # Configs mapped to llmbuilder expected keys\n    config = {\n        # tokenizer/dataset convenience\n        \"vocab_size\": 8000,\n        \"block_size\": int(args.block_size),\n        # training config -&gt; llmbuilder.config.TrainingConfig\n        \"training\": {\n            \"batch_size\": int(args.batch_size),\n            \"learning_rate\": float(args.lr),\n            \"num_epochs\": int(args.epochs),\n            \"max_grad_norm\": 1.0,\n            \"save_every\": 1,\n            \"log_every\": 10,\n        },\n        # model config -&gt; llmbuilder.config.ModelConfig\n        \"model\": {\n            \"embedding_dim\": int(args.embed_dim),\n            \"num_layers\": int(args.layers),\n            \"num_heads\": int(args.heads),\n            \"max_seq_length\": int(args.block_size),\n            \"dropout\": 0.1,\n        },\n    }\n\n    print(\"Starting LLMBuilder training pipeline...\")\n    _pipeline = llmbuilder.train(\n        data_path=str(data_dir),\n        output_dir=str(output_dir),\n        config=config,\n        clean=False,\n    )\n\n    # Generation\n    best_ckpt = output_dir / \"checkpoints\" / \"best_checkpoint.pt\"\n    latest_ckpt = output_dir / \"checkpoints\" / \"latest_checkpoint.pt\"\n    model_ckpt = best_ckpt if best_ckpt.exists() else latest_ckpt\n    tokenizer_dir = output_dir / \"tokenizer\"\n\n    if model_ckpt.exists() and tokenizer_dir.exists():\n        print(\"\\nGenerating sample text with trained model...\")\n        text = llmbuilder.generate_text(\n            model_path=str(model_ckpt),\n            tokenizer_path=str(tokenizer_dir),\n            prompt=args.prompt,\n            max_new_tokens=int(args.max_new_tokens),\n            temperature=float(args.temperature),\n            top_p=float(args.top_p),\n        )\n        print(\"\\nSample generation:\\n\" + text)\n    else:\n        print(\"\\nSkipping generation because artifacts were not found.\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"user-guide/training/#training-configuration","title":"\u2699\ufe0f Training Configuration","text":""},{"location":"user-guide/training/#core-training-parameters","title":"Core Training Parameters","text":"<pre><code>from llmbuilder.config import TrainingConfig\n\nconfig = TrainingConfig(\n    # Basic settings\n    batch_size=16,              # Samples per training step\n    num_epochs=10,              # Number of training epochs\n    learning_rate=3e-4,         # Learning rate\n\n    # Optimization\n    optimizer=\"adamw\",          # Optimizer type\n    weight_decay=0.01,          # L2 regularization\n    max_grad_norm=1.0,          # Gradient clipping\n\n    # Learning rate scheduling\n    warmup_steps=1000,          # Warmup steps\n    scheduler=\"cosine\",         # LR scheduler type\n\n    # Checkpointing\n    save_every=1000,            # Save checkpoint every N steps\n    eval_every=500,             # Evaluate every N steps\n    max_checkpoints=5,          # Maximum checkpoints to keep\n\n    # Logging\n    log_every=100,              # Log every N steps\n    wandb_project=None,         # Weights &amp; Biases project\n)\n</code></pre>"},{"location":"user-guide/training/#training-process","title":"\ud83c\udfcb\ufe0f Training Process","text":""},{"location":"user-guide/training/#training-loop","title":"Training Loop","text":"<p>The training process follows these steps:</p> <ol> <li>Data Loading: Load and batch training data</li> <li>Forward Pass: Compute model predictions</li> <li>Loss Calculation: Calculate training loss</li> <li>Backward Pass: Compute gradients</li> <li>Optimization: Update model parameters</li> <li>Evaluation: Periodic validation</li> <li>Checkpointing: Save model state</li> </ol>"},{"location":"user-guide/training/#monitoring-training","title":"Monitoring Training","text":"<pre><code>from llmbuilder.training import Trainer\n\ntrainer = Trainer(\n    model=model,\n    train_dataset=train_dataset,\n    val_dataset=val_dataset,\n    config=training_config\n)\n\n# Train with progress monitoring\nresults = trainer.train()\n\nprint(f\"Final training loss: {results.final_train_loss:.4f}\")\nprint(f\"Final validation loss: {results.final_val_loss:.4f}\")\nprint(f\"Best validation loss: {results.best_val_loss:.4f}\")\nprint(f\"Training time: {results.training_time}\")\n</code></pre>"},{"location":"user-guide/training/#advanced-training-techniques","title":"\ud83d\udcca Advanced Training Techniques","text":""},{"location":"user-guide/training/#mixed-precision-training","title":"Mixed Precision Training","text":"<pre><code>config = TrainingConfig(\n    mixed_precision=\"fp16\",     # Use 16-bit precision\n    gradient_accumulation_steps=4,  # Accumulate gradients\n)\n</code></pre>"},{"location":"user-guide/training/#gradient-checkpointing","title":"Gradient Checkpointing","text":"<pre><code>from llmbuilder.config import ModelConfig\n\nmodel_config = ModelConfig(\n    gradient_checkpointing=True,  # Save memory at cost of compute\n    # ... other config\n)\n</code></pre>"},{"location":"user-guide/training/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"<pre><code>config = TrainingConfig(\n    scheduler=\"cosine\",         # Cosine annealing\n    warmup_steps=2000,         # Linear warmup\n    min_lr_ratio=0.1,          # Minimum LR as ratio of max LR\n)\n</code></pre>"},{"location":"user-guide/training/#training-best-practices","title":"\ud83c\udfaf Training Best Practices","text":""},{"location":"user-guide/training/#1-data-quality","title":"1. Data Quality","text":"<ul> <li>Use high-quality, diverse training data</li> <li>Remove duplicates and low-quality samples</li> <li>Ensure proper text preprocessing</li> </ul>"},{"location":"user-guide/training/#2-hyperparameter-tuning","title":"2. Hyperparameter Tuning","text":"<ul> <li>Start with proven configurations</li> <li>Adjust learning rate based on model size</li> <li>Use appropriate batch sizes for your hardware</li> </ul>"},{"location":"user-guide/training/#3-monitoring-and-evaluation","title":"3. Monitoring and Evaluation","text":"<ul> <li>Monitor both training and validation loss</li> <li>Use early stopping to prevent overfitting</li> <li>Regular checkpointing for recovery</li> </ul>"},{"location":"user-guide/training/#4-hardware-optimization","title":"4. Hardware Optimization","text":"<ul> <li>Use GPU when available</li> <li>Enable mixed precision for faster training</li> <li>Optimize batch size for your hardware</li> </ul>"},{"location":"user-guide/training/#troubleshooting","title":"\ud83d\udea8 Troubleshooting","text":""},{"location":"user-guide/training/#common-training-issues","title":"Common Training Issues","text":""},{"location":"user-guide/training/#out-of-memory","title":"Out of Memory","text":"<pre><code># Reduce batch size\nconfig.batch_size = 8\n\n# Enable gradient checkpointing\nmodel_config.gradient_checkpointing = True\n\n# Use gradient accumulation\nconfig.gradient_accumulation_steps = 4\n</code></pre>"},{"location":"user-guide/training/#slow-convergence","title":"Slow Convergence","text":"<pre><code># Increase learning rate\nconfig.learning_rate = 5e-4\n\n# Longer warmup\nconfig.warmup_steps = 2000\n\n# Different optimizer\nconfig.optimizer = \"adam\"\n</code></pre>"},{"location":"user-guide/training/#unstable-training","title":"Unstable Training","text":"<pre><code># Lower learning rate\nconfig.learning_rate = 1e-4\n\n# Stronger gradient clipping\nconfig.max_grad_norm = 0.5\n\n# Add weight decay\nconfig.weight_decay = 0.1\n</code></pre> <p>Training Tips</p> <ul> <li>Start with small models and scale up gradually</li> <li>Monitor GPU memory usage and adjust batch size accordingly</li> <li>Use validation loss to detect overfitting</li> <li>Save checkpoints frequently during long training runs</li> </ul>"}]}